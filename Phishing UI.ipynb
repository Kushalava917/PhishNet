{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141155,"status":"ok","timestamp":1747802763290,"user":{"displayName":"228W1A5430_Sec-A Kavuri Kushalava","userId":"01804207717655710695"},"user_tz":-330},"id":"OXl5Vl9z6SZr","outputId":"be958801-3dcc-4873-8aa6-6aa2234ee14a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-5.30.0-py3-none-any.whl.metadata (16 kB)\n","Collecting easyocr\n","  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Collecting aiofiles<25.0,>=22.0 (from gradio)\n","  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio)\n","  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n","Collecting gradio-client==1.10.1 (from gradio)\n","  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting groovy~=0.1 (from gradio)\n","  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n","Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.18 (from gradio)\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.9.3 (from gradio)\n","  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n","  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio)\n","  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n","Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n","  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.15.3)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.1.0)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->easyocr)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->easyocr)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->easyocr)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->easyocr)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->easyocr)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->easyocr)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->easyocr)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->easyocr)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->easyocr)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->easyocr)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.5.10)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-5.30.0-py3-none-any.whl (54.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n","Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pydub, pyclipper, uvicorn, tomlkit, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, groovy, ffmpy, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio, easyocr\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed aiofiles-24.1.0 easyocr-1.7.2 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.30.0 gradio-client-1.10.1 groovy-0.1.2 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyclipper-1.3.0.post6 pydub-0.25.1 python-bidi-0.6.6 python-multipart-0.0.20 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n","Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Installing collected packages: pytesseract\n","Successfully installed pytesseract-0.3.13\n"]}],"source":["!pip install gradio easyocr transformers\n","!pip install pytesseract"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9355,"status":"ok","timestamp":1747802772650,"user":{"displayName":"228W1A5430_Sec-A Kavuri Kushalava","userId":"01804207717655710695"},"user_tz":-330},"id":"cJAgLXOdAbXv","outputId":"fb3c88ce-c30b-4982-8af2-d26c0a921622"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-21 04:46:10--  https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4.weights\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/4b8a4e00-b2d7-11eb-900f-678196af5945?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250521T044610Z&X-Amz-Expires=300&X-Amz-Signature=80a46ec6253e90176d5b163ce79af01e1ee940b8ffbf463daa74f757988d4f57&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov4.weights&response-content-type=application%2Foctet-stream [following]\n","--2025-05-21 04:46:10--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/4b8a4e00-b2d7-11eb-900f-678196af5945?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250521T044610Z&X-Amz-Expires=300&X-Amz-Signature=80a46ec6253e90176d5b163ce79af01e1ee940b8ffbf463daa74f757988d4f57&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dyolov4.weights&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 257717640 (246M) [application/octet-stream]\n","Saving to: ‘yolov4.weights’\n","\n","yolov4.weights      100%[===================>] 245.78M  33.5MB/s    in 7.3s    \n","\n","2025-05-21 04:46:19 (33.7 MB/s) - ‘yolov4.weights’ saved [257717640/257717640]\n","\n","--2025-05-21 04:46:19--  https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12231 (12K) [text/plain]\n","Saving to: ‘yolov4.cfg’\n","\n","yolov4.cfg          100%[===================>]  11.94K  --.-KB/s    in 0s      \n","\n","2025-05-21 04:46:19 (105 MB/s) - ‘yolov4.cfg’ saved [12231/12231]\n","\n"]}],"source":["!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4.weights\n","!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":43597,"status":"ok","timestamp":1747802816251,"user":{"displayName":"228W1A5430_Sec-A Kavuri Kushalava","userId":"01804207717655710695"},"user_tz":-330},"id":"ILGqq6RTAQna","colab":{"base_uri":"https://localhost:8080/","height":339,"referenced_widgets":["9b840de26f5e464fa69b597255b86e59","7f58549b29cc44dea100c4294026bd94","29c2a1a01bce4f408c9c0c88eb764e4d","99683a517e9a41418b25b90819c25dbc","ec6aab008bdb4bd09000d08559a78032","691e60d3dafa4dff9ba71daa79134cb1","4b7284d37ed24bc6bc23fc214ba41cda","5ff044207a7c4beebf5af0f77332b631","20090a040e7a4c96ae9d87141c45a894","b79e9ad2051f4b339525c19b7238d459","1baaf89a71164d4b96ce8b89dfeabb46","546a7dfbc7e64cfda02e858fb0865e4a","0517e07ef51f48e4a9ac0184aab2dafc","69056b79986e42feb507c94c8a55f5b4","57639ff084a6410fa1790832dbc1401b","20a5fe78f14c4f3cab11aa8cee454243","7d18764f74194161a499ddec686172ee","1017e21334104e06aa40a9566aea54f3","3a72f4caf842403598d4f44e5dcd7f50","e04e8d1fc62c43ddb618e373e15f35c4","596caa65e0c54f68bdd59baccb4ad569","9cd5c0038eb64e1db4686c09c62392b1","0e2800c21bc643f5b0ce463b588f5070","e6e85c3af69e47c88b31acd0caf335f4","09e847be8b584d3e86ccaa6ede0d4d98","ffc37ee799bd4960910fbad27b9efe70","22b6871d64974f4a82b7e33150fbb8c8","3fc09e6b16364524a4c677b5d447c952","36ef8b2d12d949b78f78b7347feba687","a93eaf869bf34bccbedaefecbbc3eeb0","0168e04412934ec48ea8b09585595217","eb62124bf77748c2b83f12a072297eb4","35c20836861d43d18398c36d19d90395","233529b35e074309b20609573db52d52","c694e93e317f46579073554bccc68731","727866fd570b45149dc55ac49ca24b1a","104c8a3075934673b9bdb896ea08106c","8ae860562e59485bac16778360bbba43","35a3d0c27af94a07a3b0d5602e0a5e0f","e3b0f48d374145ddadb3d10eff0650bf","5b8de581ad7346e89a7a49897e051971","a6fe309560d74cdca49abd5e4d37b7bb","d176ab33f1c648929f40162a32b053c1","d447543f1120445ebf67904c52ad5858"]},"outputId":"b06eb89c-4e6d-482f-d03a-5c5460b8704c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b840de26f5e464fa69b597255b86e59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546a7dfbc7e64cfda02e858fb0865e4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e2800c21bc643f5b0ce463b588f5070"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"]},{"output_type":"stream","name":"stdout","text":["Progress: |█████████████████████████████████████████---------| 82.5% Complete"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233529b35e074309b20609573db52d52"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Progress: |██████████████████████████████████████████████████| 100.0% Complete"]},{"output_type":"stream","name":"stderr","text":["WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"]},{"output_type":"stream","name":"stdout","text":["\rProgress: |--------------------------------------------------| 0.0% Complete\rProgress: |--------------------------------------------------| 0.1% Complete\rProgress: |--------------------------------------------------| 0.1% Complete\rProgress: |--------------------------------------------------| 0.2% Complete\rProgress: |--------------------------------------------------| 0.2% Complete\rProgress: |--------------------------------------------------| 0.3% Complete\rProgress: |--------------------------------------------------| 0.4% Complete\rProgress: |--------------------------------------------------| 0.4% Complete\rProgress: |--------------------------------------------------| 0.5% Complete\rProgress: |--------------------------------------------------| 0.5% Complete\rProgress: |--------------------------------------------------| 0.6% Complete\rProgress: |--------------------------------------------------| 0.6% Complete\rProgress: |--------------------------------------------------| 0.7% Complete\rProgress: |--------------------------------------------------| 0.8% Complete\rProgress: |--------------------------------------------------| 0.8% Complete\rProgress: |--------------------------------------------------| 0.9% Complete\rProgress: |--------------------------------------------------| 0.9% Complete\rProgress: |--------------------------------------------------| 1.0% Complete\rProgress: |--------------------------------------------------| 1.1% Complete\rProgress: |--------------------------------------------------| 1.1% Complete\rProgress: |--------------------------------------------------| 1.2% Complete\rProgress: |--------------------------------------------------| 1.2% Complete\rProgress: |--------------------------------------------------| 1.3% Complete\rProgress: |--------------------------------------------------| 1.3% Complete\rProgress: |--------------------------------------------------| 1.4% Complete\rProgress: |--------------------------------------------------| 1.5% Complete\rProgress: |--------------------------------------------------| 1.5% Complete\rProgress: |--------------------------------------------------| 1.6% Complete\rProgress: |--------------------------------------------------| 1.6% Complete\rProgress: |--------------------------------------------------| 1.7% Complete\rProgress: |--------------------------------------------------| 1.8% Complete\rProgress: |--------------------------------------------------| 1.8% Complete\rProgress: |--------------------------------------------------| 1.9% Complete\rProgress: |--------------------------------------------------| 1.9% Complete\rProgress: |--------------------------------------------------| 2.0% Complete\rProgress: |█-------------------------------------------------| 2.0% Complete\rProgress: |█-------------------------------------------------| 2.1% Complete\rProgress: |█-------------------------------------------------| 2.2% Complete\rProgress: |█-------------------------------------------------| 2.2% Complete\rProgress: |█-------------------------------------------------| 2.3% Complete\rProgress: |█-------------------------------------------------| 2.3% Complete\rProgress: |█-------------------------------------------------| 2.4% Complete\rProgress: |█-------------------------------------------------| 2.5% Complete\rProgress: |█-------------------------------------------------| 2.5% Complete\rProgress: |█-------------------------------------------------| 2.6% Complete\rProgress: |█-------------------------------------------------| 2.6% Complete\rProgress: |█-------------------------------------------------| 2.7% Complete\rProgress: |█-------------------------------------------------| 2.7% Complete\rProgress: |█-------------------------------------------------| 2.8% Complete\rProgress: |█-------------------------------------------------| 2.9% Complete\rProgress: |█-------------------------------------------------| 2.9% Complete\rProgress: |█-------------------------------------------------| 3.0% Complete\rProgress: |█-------------------------------------------------| 3.0% Complete\rProgress: |█-------------------------------------------------| 3.1% Complete\rProgress: |█-------------------------------------------------| 3.2% Complete\rProgress: |█-------------------------------------------------| 3.2% Complete\rProgress: |█-------------------------------------------------| 3.3% Complete\rProgress: |█-------------------------------------------------| 3.3% Complete\rProgress: |█-------------------------------------------------| 3.4% Complete\rProgress: |█-------------------------------------------------| 3.4% Complete\rProgress: |█-------------------------------------------------| 3.5% Complete\rProgress: |█-------------------------------------------------| 3.6% Complete\rProgress: |█-------------------------------------------------| 3.6% Complete\rProgress: |█-------------------------------------------------| 3.7% Complete\rProgress: |█-------------------------------------------------| 3.7% Complete\rProgress: |█-------------------------------------------------| 3.8% Complete\rProgress: |█-------------------------------------------------| 3.9% Complete\rProgress: |█-------------------------------------------------| 3.9% Complete\rProgress: |█-------------------------------------------------| 4.0% Complete\rProgress: |██------------------------------------------------| 4.0% Complete\rProgress: |██------------------------------------------------| 4.1% Complete\rProgress: |██------------------------------------------------| 4.1% Complete\rProgress: |██------------------------------------------------| 4.2% Complete\rProgress: |██------------------------------------------------| 4.3% Complete\rProgress: |██------------------------------------------------| 4.3% Complete\rProgress: |██------------------------------------------------| 4.4% Complete\rProgress: |██------------------------------------------------| 4.4% Complete\rProgress: |██------------------------------------------------| 4.5% Complete\rProgress: |██------------------------------------------------| 4.6% Complete\rProgress: |██------------------------------------------------| 4.6% Complete\rProgress: |██------------------------------------------------| 4.7% Complete\rProgress: |██------------------------------------------------| 4.7% Complete\rProgress: |██------------------------------------------------| 4.8% Complete\rProgress: |██------------------------------------------------| 4.8% Complete\rProgress: |██------------------------------------------------| 4.9% Complete\rProgress: |██------------------------------------------------| 5.0% Complete\rProgress: |██------------------------------------------------| 5.0% Complete\rProgress: |██------------------------------------------------| 5.1% Complete\rProgress: |██------------------------------------------------| 5.1% Complete\rProgress: |██------------------------------------------------| 5.2% Complete\rProgress: |██------------------------------------------------| 5.3% Complete\rProgress: |██------------------------------------------------| 5.3% Complete\rProgress: |██------------------------------------------------| 5.4% Complete\rProgress: |██------------------------------------------------| 5.4% Complete\rProgress: |██------------------------------------------------| 5.5% Complete\rProgress: |██------------------------------------------------| 5.5% Complete\rProgress: |██------------------------------------------------| 5.6% Complete\rProgress: |██------------------------------------------------| 5.7% Complete\rProgress: |██------------------------------------------------| 5.7% Complete\rProgress: |██------------------------------------------------| 5.8% Complete\rProgress: |██------------------------------------------------| 5.8% Complete\rProgress: |██------------------------------------------------| 5.9% Complete\rProgress: |██------------------------------------------------| 6.0% Complete\rProgress: |███-----------------------------------------------| 6.0% Complete\rProgress: |███-----------------------------------------------| 6.1% Complete\rProgress: |███-----------------------------------------------| 6.1% Complete\rProgress: |███-----------------------------------------------| 6.2% Complete\rProgress: |███-----------------------------------------------| 6.2% Complete\rProgress: |███-----------------------------------------------| 6.3% Complete\rProgress: |███-----------------------------------------------| 6.4% Complete\rProgress: |███-----------------------------------------------| 6.4% Complete\rProgress: |███-----------------------------------------------| 6.5% Complete\rProgress: |███-----------------------------------------------| 6.5% Complete\rProgress: |███-----------------------------------------------| 6.6% Complete\rProgress: |███-----------------------------------------------| 6.7% Complete\rProgress: |███-----------------------------------------------| 6.7% Complete\rProgress: |███-----------------------------------------------| 6.8% Complete\rProgress: |███-----------------------------------------------| 6.8% Complete\rProgress: |███-----------------------------------------------| 6.9% Complete\rProgress: |███-----------------------------------------------| 6.9% Complete\rProgress: |███-----------------------------------------------| 7.0% Complete\rProgress: |███-----------------------------------------------| 7.1% Complete\rProgress: |███-----------------------------------------------| 7.1% Complete\rProgress: |███-----------------------------------------------| 7.2% Complete\rProgress: |███-----------------------------------------------| 7.2% Complete\rProgress: |███-----------------------------------------------| 7.3% Complete\rProgress: |███-----------------------------------------------| 7.4% Complete\rProgress: |███-----------------------------------------------| 7.4% Complete\rProgress: |███-----------------------------------------------| 7.5% Complete\rProgress: |███-----------------------------------------------| 7.5% Complete\rProgress: |███-----------------------------------------------| 7.6% Complete\rProgress: |███-----------------------------------------------| 7.6% Complete\rProgress: |███-----------------------------------------------| 7.7% Complete\rProgress: |███-----------------------------------------------| 7.8% Complete\rProgress: |███-----------------------------------------------| 7.8% Complete\rProgress: |███-----------------------------------------------| 7.9% Complete\rProgress: |███-----------------------------------------------| 7.9% Complete\rProgress: |███-----------------------------------------------| 8.0% Complete\rProgress: |████----------------------------------------------| 8.1% Complete\rProgress: |████----------------------------------------------| 8.1% Complete\rProgress: |████----------------------------------------------| 8.2% Complete\rProgress: |████----------------------------------------------| 8.2% Complete\rProgress: |████----------------------------------------------| 8.3% Complete\rProgress: |████----------------------------------------------| 8.3% Complete\rProgress: |████----------------------------------------------| 8.4% Complete\rProgress: |████----------------------------------------------| 8.5% Complete\rProgress: |████----------------------------------------------| 8.5% Complete\rProgress: |████----------------------------------------------| 8.6% Complete\rProgress: |████----------------------------------------------| 8.6% Complete\rProgress: |████----------------------------------------------| 8.7% Complete\rProgress: |████----------------------------------------------| 8.8% Complete\rProgress: |████----------------------------------------------| 8.8% Complete\rProgress: |████----------------------------------------------| 8.9% Complete\rProgress: |████----------------------------------------------| 8.9% Complete\rProgress: |████----------------------------------------------| 9.0% Complete\rProgress: |████----------------------------------------------| 9.0% Complete\rProgress: |████----------------------------------------------| 9.1% Complete\rProgress: |████----------------------------------------------| 9.2% Complete\rProgress: |████----------------------------------------------| 9.2% Complete\rProgress: |████----------------------------------------------| 9.3% Complete\rProgress: |████----------------------------------------------| 9.3% Complete\rProgress: |████----------------------------------------------| 9.4% Complete\rProgress: |████----------------------------------------------| 9.5% Complete\rProgress: |████----------------------------------------------| 9.5% Complete\rProgress: |████----------------------------------------------| 9.6% Complete\rProgress: |████----------------------------------------------| 9.6% Complete\rProgress: |████----------------------------------------------| 9.7% Complete\rProgress: |████----------------------------------------------| 9.7% Complete\rProgress: |████----------------------------------------------| 9.8% Complete\rProgress: |████----------------------------------------------| 9.9% Complete\rProgress: |████----------------------------------------------| 9.9% Complete\rProgress: |████----------------------------------------------| 10.0% Complete\rProgress: |█████---------------------------------------------| 10.0% Complete\rProgress: |█████---------------------------------------------| 10.1% Complete\rProgress: |█████---------------------------------------------| 10.2% Complete\rProgress: |█████---------------------------------------------| 10.2% Complete\rProgress: |█████---------------------------------------------| 10.3% Complete\rProgress: |█████---------------------------------------------| 10.3% Complete\rProgress: |█████---------------------------------------------| 10.4% Complete\rProgress: |█████---------------------------------------------| 10.4% Complete\rProgress: |█████---------------------------------------------| 10.5% Complete\rProgress: |█████---------------------------------------------| 10.6% Complete\rProgress: |█████---------------------------------------------| 10.6% Complete\rProgress: |█████---------------------------------------------| 10.7% Complete\rProgress: |█████---------------------------------------------| 10.7% Complete\rProgress: |█████---------------------------------------------| 10.8% Complete\rProgress: |█████---------------------------------------------| 10.9% Complete\rProgress: |█████---------------------------------------------| 10.9% Complete\rProgress: |█████---------------------------------------------| 11.0% Complete\rProgress: |█████---------------------------------------------| 11.0% Complete\rProgress: |█████---------------------------------------------| 11.1% Complete\rProgress: |█████---------------------------------------------| 11.1% Complete\rProgress: |█████---------------------------------------------| 11.2% Complete\rProgress: |█████---------------------------------------------| 11.3% Complete\rProgress: |█████---------------------------------------------| 11.3% Complete\rProgress: |█████---------------------------------------------| 11.4% Complete\rProgress: |█████---------------------------------------------| 11.4% Complete\rProgress: |█████---------------------------------------------| 11.5% Complete\rProgress: |█████---------------------------------------------| 11.6% Complete\rProgress: |█████---------------------------------------------| 11.6% Complete\rProgress: |█████---------------------------------------------| 11.7% Complete\rProgress: |█████---------------------------------------------| 11.7% Complete\rProgress: |█████---------------------------------------------| 11.8% Complete\rProgress: |█████---------------------------------------------| 11.8% Complete\rProgress: |█████---------------------------------------------| 11.9% Complete\rProgress: |█████---------------------------------------------| 12.0% Complete\rProgress: |██████--------------------------------------------| 12.0% Complete\rProgress: |██████--------------------------------------------| 12.1% Complete\rProgress: |██████--------------------------------------------| 12.1% Complete\rProgress: |██████--------------------------------------------| 12.2% Complete\rProgress: |██████--------------------------------------------| 12.3% Complete\rProgress: |██████--------------------------------------------| 12.3% Complete\rProgress: |██████--------------------------------------------| 12.4% Complete\rProgress: |██████--------------------------------------------| 12.4% Complete\rProgress: |██████--------------------------------------------| 12.5% Complete\rProgress: |██████--------------------------------------------| 12.5% Complete\rProgress: |██████--------------------------------------------| 12.6% Complete\rProgress: |██████--------------------------------------------| 12.7% Complete\rProgress: |██████--------------------------------------------| 12.7% Complete\rProgress: |██████--------------------------------------------| 12.8% Complete\rProgress: |██████--------------------------------------------| 12.8% Complete\rProgress: |██████--------------------------------------------| 12.9% Complete\rProgress: |██████--------------------------------------------| 13.0% Complete\rProgress: |██████--------------------------------------------| 13.0% Complete\rProgress: |██████--------------------------------------------| 13.1% Complete\rProgress: |██████--------------------------------------------| 13.1% Complete\rProgress: |██████--------------------------------------------| 13.2% Complete\rProgress: |██████--------------------------------------------| 13.2% Complete\rProgress: |██████--------------------------------------------| 13.3% Complete\rProgress: |██████--------------------------------------------| 13.4% Complete\rProgress: |██████--------------------------------------------| 13.4% Complete\rProgress: |██████--------------------------------------------| 13.5% Complete\rProgress: |██████--------------------------------------------| 13.5% Complete\rProgress: |██████--------------------------------------------| 13.6% Complete\rProgress: |██████--------------------------------------------| 13.7% Complete\rProgress: |██████--------------------------------------------| 13.7% Complete\rProgress: |██████--------------------------------------------| 13.8% Complete\rProgress: |██████--------------------------------------------| 13.8% Complete\rProgress: |██████--------------------------------------------| 13.9% Complete\rProgress: |██████--------------------------------------------| 13.9% Complete\rProgress: |███████-------------------------------------------| 14.0% Complete\rProgress: |███████-------------------------------------------| 14.1% Complete\rProgress: |███████-------------------------------------------| 14.1% Complete\rProgress: |███████-------------------------------------------| 14.2% Complete\rProgress: |███████-------------------------------------------| 14.2% Complete\rProgress: |███████-------------------------------------------| 14.3% Complete\rProgress: |███████-------------------------------------------| 14.4% Complete\rProgress: |███████-------------------------------------------| 14.4% Complete\rProgress: |███████-------------------------------------------| 14.5% Complete\rProgress: |███████-------------------------------------------| 14.5% Complete\rProgress: |███████-------------------------------------------| 14.6% Complete\rProgress: |███████-------------------------------------------| 14.6% Complete\rProgress: |███████-------------------------------------------| 14.7% Complete\rProgress: |███████-------------------------------------------| 14.8% Complete\rProgress: |███████-------------------------------------------| 14.8% Complete\rProgress: |███████-------------------------------------------| 14.9% Complete\rProgress: |███████-------------------------------------------| 14.9% Complete\rProgress: |███████-------------------------------------------| 15.0% Complete\rProgress: |███████-------------------------------------------| 15.1% Complete\rProgress: |███████-------------------------------------------| 15.1% Complete\rProgress: |███████-------------------------------------------| 15.2% Complete\rProgress: |███████-------------------------------------------| 15.2% Complete\rProgress: |███████-------------------------------------------| 15.3% Complete\rProgress: |███████-------------------------------------------| 15.3% Complete\rProgress: |███████-------------------------------------------| 15.4% Complete\rProgress: |███████-------------------------------------------| 15.5% Complete\rProgress: |███████-------------------------------------------| 15.5% Complete\rProgress: |███████-------------------------------------------| 15.6% Complete\rProgress: |███████-------------------------------------------| 15.6% Complete\rProgress: |███████-------------------------------------------| 15.7% Complete\rProgress: |███████-------------------------------------------| 15.8% Complete\rProgress: |███████-------------------------------------------| 15.8% Complete\rProgress: |███████-------------------------------------------| 15.9% Complete\rProgress: |███████-------------------------------------------| 15.9% Complete\rProgress: |███████-------------------------------------------| 16.0% Complete\rProgress: |████████------------------------------------------| 16.0% Complete\rProgress: |████████------------------------------------------| 16.1% Complete\rProgress: |████████------------------------------------------| 16.2% Complete\rProgress: |████████------------------------------------------| 16.2% Complete\rProgress: |████████------------------------------------------| 16.3% Complete\rProgress: |████████------------------------------------------| 16.3% Complete\rProgress: |████████------------------------------------------| 16.4% Complete\rProgress: |████████------------------------------------------| 16.5% Complete\rProgress: |████████------------------------------------------| 16.5% Complete\rProgress: |████████------------------------------------------| 16.6% Complete\rProgress: |████████------------------------------------------| 16.6% Complete\rProgress: |████████------------------------------------------| 16.7% Complete\rProgress: |████████------------------------------------------| 16.7% Complete\rProgress: |████████------------------------------------------| 16.8% Complete\rProgress: |████████------------------------------------------| 16.9% Complete\rProgress: |████████------------------------------------------| 16.9% Complete\rProgress: |████████------------------------------------------| 17.0% Complete\rProgress: |████████------------------------------------------| 17.0% Complete\rProgress: |████████------------------------------------------| 17.1% Complete\rProgress: |████████------------------------------------------| 17.2% Complete\rProgress: |████████------------------------------------------| 17.2% Complete\rProgress: |████████------------------------------------------| 17.3% Complete\rProgress: |████████------------------------------------------| 17.3% Complete\rProgress: |████████------------------------------------------| 17.4% Complete\rProgress: |████████------------------------------------------| 17.4% Complete\rProgress: |████████------------------------------------------| 17.5% Complete\rProgress: |████████------------------------------------------| 17.6% Complete\rProgress: |████████------------------------------------------| 17.6% Complete\rProgress: |████████------------------------------------------| 17.7% Complete\rProgress: |████████------------------------------------------| 17.7% Complete\rProgress: |████████------------------------------------------| 17.8% Complete\rProgress: |████████------------------------------------------| 17.9% Complete\rProgress: |████████------------------------------------------| 17.9% Complete\rProgress: |████████------------------------------------------| 18.0% Complete\rProgress: |█████████-----------------------------------------| 18.0% Complete\rProgress: |█████████-----------------------------------------| 18.1% Complete\rProgress: |█████████-----------------------------------------| 18.1% Complete\rProgress: |█████████-----------------------------------------| 18.2% Complete\rProgress: |█████████-----------------------------------------| 18.3% Complete\rProgress: |█████████-----------------------------------------| 18.3% Complete\rProgress: |█████████-----------------------------------------| 18.4% Complete\rProgress: |█████████-----------------------------------------| 18.4% Complete\rProgress: |█████████-----------------------------------------| 18.5% Complete\rProgress: |█████████-----------------------------------------| 18.6% Complete\rProgress: |█████████-----------------------------------------| 18.6% Complete\rProgress: |█████████-----------------------------------------| 18.7% Complete\rProgress: |█████████-----------------------------------------| 18.7% Complete\rProgress: |█████████-----------------------------------------| 18.8% Complete\rProgress: |█████████-----------------------------------------| 18.8% Complete\rProgress: |█████████-----------------------------------------| 18.9% Complete\rProgress: |█████████-----------------------------------------| 19.0% Complete\rProgress: |█████████-----------------------------------------| 19.0% Complete\rProgress: |█████████-----------------------------------------| 19.1% Complete\rProgress: |█████████-----------------------------------------| 19.1% Complete\rProgress: |█████████-----------------------------------------| 19.2% Complete\rProgress: |█████████-----------------------------------------| 19.3% Complete\rProgress: |█████████-----------------------------------------| 19.3% Complete\rProgress: |█████████-----------------------------------------| 19.4% Complete\rProgress: |█████████-----------------------------------------| 19.4% Complete\rProgress: |█████████-----------------------------------------| 19.5% Complete\rProgress: |█████████-----------------------------------------| 19.5% Complete\rProgress: |█████████-----------------------------------------| 19.6% Complete\rProgress: |█████████-----------------------------------------| 19.7% Complete\rProgress: |█████████-----------------------------------------| 19.7% Complete\rProgress: |█████████-----------------------------------------| 19.8% Complete\rProgress: |█████████-----------------------------------------| 19.8% Complete\rProgress: |█████████-----------------------------------------| 19.9% Complete\rProgress: |█████████-----------------------------------------| 20.0% Complete\rProgress: |██████████----------------------------------------| 20.0% Complete\rProgress: |██████████----------------------------------------| 20.1% Complete\rProgress: |██████████----------------------------------------| 20.1% Complete\rProgress: |██████████----------------------------------------| 20.2% Complete\rProgress: |██████████----------------------------------------| 20.2% Complete\rProgress: |██████████----------------------------------------| 20.3% Complete\rProgress: |██████████----------------------------------------| 20.4% Complete\rProgress: |██████████----------------------------------------| 20.4% Complete\rProgress: |██████████----------------------------------------| 20.5% Complete\rProgress: |██████████----------------------------------------| 20.5% Complete\rProgress: |██████████----------------------------------------| 20.6% Complete\rProgress: |██████████----------------------------------------| 20.7% Complete\rProgress: |██████████----------------------------------------| 20.7% Complete\rProgress: |██████████----------------------------------------| 20.8% Complete\rProgress: |██████████----------------------------------------| 20.8% Complete\rProgress: |██████████----------------------------------------| 20.9% Complete\rProgress: |██████████----------------------------------------| 20.9% Complete\rProgress: |██████████----------------------------------------| 21.0% Complete\rProgress: |██████████----------------------------------------| 21.1% Complete\rProgress: |██████████----------------------------------------| 21.1% Complete\rProgress: |██████████----------------------------------------| 21.2% Complete\rProgress: |██████████----------------------------------------| 21.2% Complete\rProgress: |██████████----------------------------------------| 21.3% Complete\rProgress: |██████████----------------------------------------| 21.4% Complete\rProgress: |██████████----------------------------------------| 21.4% Complete\rProgress: |██████████----------------------------------------| 21.5% Complete\rProgress: |██████████----------------------------------------| 21.5% Complete\rProgress: |██████████----------------------------------------| 21.6% Complete\rProgress: |██████████----------------------------------------| 21.6% Complete\rProgress: |██████████----------------------------------------| 21.7% Complete\rProgress: |██████████----------------------------------------| 21.8% Complete\rProgress: |██████████----------------------------------------| 21.8% Complete\rProgress: |██████████----------------------------------------| 21.9% Complete\rProgress: |██████████----------------------------------------| 21.9% Complete\rProgress: |██████████----------------------------------------| 22.0% Complete\rProgress: |███████████---------------------------------------| 22.1% Complete\rProgress: |███████████---------------------------------------| 22.1% Complete\rProgress: |███████████---------------------------------------| 22.2% Complete\rProgress: |███████████---------------------------------------| 22.2% Complete\rProgress: |███████████---------------------------------------| 22.3% Complete\rProgress: |███████████---------------------------------------| 22.3% Complete\rProgress: |███████████---------------------------------------| 22.4% Complete\rProgress: |███████████---------------------------------------| 22.5% Complete\rProgress: |███████████---------------------------------------| 22.5% Complete\rProgress: |███████████---------------------------------------| 22.6% Complete\rProgress: |███████████---------------------------------------| 22.6% Complete\rProgress: |███████████---------------------------------------| 22.7% Complete\rProgress: |███████████---------------------------------------| 22.8% Complete\rProgress: |███████████---------------------------------------| 22.8% Complete\rProgress: |███████████---------------------------------------| 22.9% Complete\rProgress: |███████████---------------------------------------| 22.9% Complete\rProgress: |███████████---------------------------------------| 23.0% Complete\rProgress: |███████████---------------------------------------| 23.0% Complete\rProgress: |███████████---------------------------------------| 23.1% Complete\rProgress: |███████████---------------------------------------| 23.2% Complete\rProgress: |███████████---------------------------------------| 23.2% Complete\rProgress: |███████████---------------------------------------| 23.3% Complete\rProgress: |███████████---------------------------------------| 23.3% Complete\rProgress: |███████████---------------------------------------| 23.4% Complete\rProgress: |███████████---------------------------------------| 23.5% Complete\rProgress: |███████████---------------------------------------| 23.5% Complete\rProgress: |███████████---------------------------------------| 23.6% Complete\rProgress: |███████████---------------------------------------| 23.6% Complete\rProgress: |███████████---------------------------------------| 23.7% Complete\rProgress: |███████████---------------------------------------| 23.7% Complete\rProgress: |███████████---------------------------------------| 23.8% Complete\rProgress: |███████████---------------------------------------| 23.9% Complete\rProgress: |███████████---------------------------------------| 23.9% Complete\rProgress: |███████████---------------------------------------| 24.0% Complete\rProgress: |████████████--------------------------------------| 24.0% Complete\rProgress: |████████████--------------------------------------| 24.1% Complete\rProgress: |████████████--------------------------------------| 24.2% Complete\rProgress: |████████████--------------------------------------| 24.2% Complete\rProgress: |████████████--------------------------------------| 24.3% Complete\rProgress: |████████████--------------------------------------| 24.3% Complete\rProgress: |████████████--------------------------------------| 24.4% Complete\rProgress: |████████████--------------------------------------| 24.4% Complete\rProgress: |████████████--------------------------------------| 24.5% Complete\rProgress: |████████████--------------------------------------| 24.6% Complete\rProgress: |████████████--------------------------------------| 24.6% Complete\rProgress: |████████████--------------------------------------| 24.7% Complete\rProgress: |████████████--------------------------------------| 24.7% Complete\rProgress: |████████████--------------------------------------| 24.8% Complete\rProgress: |████████████--------------------------------------| 24.9% Complete\rProgress: |████████████--------------------------------------| 24.9% Complete\rProgress: |████████████--------------------------------------| 25.0% Complete\rProgress: |████████████--------------------------------------| 25.0% Complete\rProgress: |████████████--------------------------------------| 25.1% Complete\rProgress: |████████████--------------------------------------| 25.1% Complete\rProgress: |████████████--------------------------------------| 25.2% Complete\rProgress: |████████████--------------------------------------| 25.3% Complete\rProgress: |████████████--------------------------------------| 25.3% Complete\rProgress: |████████████--------------------------------------| 25.4% Complete\rProgress: |████████████--------------------------------------| 25.4% Complete\rProgress: |████████████--------------------------------------| 25.5% Complete\rProgress: |████████████--------------------------------------| 25.6% Complete\rProgress: |████████████--------------------------------------| 25.6% Complete\rProgress: |████████████--------------------------------------| 25.7% Complete\rProgress: |████████████--------------------------------------| 25.7% Complete\rProgress: |████████████--------------------------------------| 25.8% Complete\rProgress: |████████████--------------------------------------| 25.8% Complete\rProgress: |████████████--------------------------------------| 25.9% Complete\rProgress: |████████████--------------------------------------| 26.0% Complete\rProgress: |█████████████-------------------------------------| 26.0% Complete\rProgress: |█████████████-------------------------------------| 26.1% Complete\rProgress: |█████████████-------------------------------------| 26.1% Complete\rProgress: |█████████████-------------------------------------| 26.2% Complete\rProgress: |█████████████-------------------------------------| 26.3% Complete\rProgress: |█████████████-------------------------------------| 26.3% Complete\rProgress: |█████████████-------------------------------------| 26.4% Complete\rProgress: |█████████████-------------------------------------| 26.4% Complete\rProgress: |█████████████-------------------------------------| 26.5% Complete\rProgress: |█████████████-------------------------------------| 26.5% Complete\rProgress: |█████████████-------------------------------------| 26.6% Complete\rProgress: |█████████████-------------------------------------| 26.7% Complete\rProgress: |█████████████-------------------------------------| 26.7% Complete\rProgress: |█████████████-------------------------------------| 26.8% Complete\rProgress: |█████████████-------------------------------------| 26.8% Complete\rProgress: |█████████████-------------------------------------| 26.9% Complete\rProgress: |█████████████-------------------------------------| 27.0% Complete\rProgress: |█████████████-------------------------------------| 27.0% Complete\rProgress: |█████████████-------------------------------------| 27.1% Complete\rProgress: |█████████████-------------------------------------| 27.1% Complete\rProgress: |█████████████-------------------------------------| 27.2% Complete\rProgress: |█████████████-------------------------------------| 27.2% Complete\rProgress: |█████████████-------------------------------------| 27.3% Complete\rProgress: |█████████████-------------------------------------| 27.4% Complete\rProgress: |█████████████-------------------------------------| 27.4% Complete\rProgress: |█████████████-------------------------------------| 27.5% Complete\rProgress: |█████████████-------------------------------------| 27.5% Complete\rProgress: |█████████████-------------------------------------| 27.6% Complete\rProgress: |█████████████-------------------------------------| 27.7% Complete\rProgress: |█████████████-------------------------------------| 27.7% Complete\rProgress: |█████████████-------------------------------------| 27.8% Complete\rProgress: |█████████████-------------------------------------| 27.8% Complete\rProgress: |█████████████-------------------------------------| 27.9% Complete\rProgress: |█████████████-------------------------------------| 27.9% Complete\rProgress: |██████████████------------------------------------| 28.0% Complete\rProgress: |██████████████------------------------------------| 28.1% Complete\rProgress: |██████████████------------------------------------| 28.1% Complete\rProgress: |██████████████------------------------------------| 28.2% Complete\rProgress: |██████████████------------------------------------| 28.2% Complete\rProgress: |██████████████------------------------------------| 28.3% Complete\rProgress: |██████████████------------------------------------| 28.4% Complete\rProgress: |██████████████------------------------------------| 28.4% Complete\rProgress: |██████████████------------------------------------| 28.5% Complete\rProgress: |██████████████------------------------------------| 28.5% Complete\rProgress: |██████████████------------------------------------| 28.6% Complete\rProgress: |██████████████------------------------------------| 28.6% Complete\rProgress: |██████████████------------------------------------| 28.7% Complete\rProgress: |██████████████------------------------------------| 28.8% Complete\rProgress: |██████████████------------------------------------| 28.8% Complete\rProgress: |██████████████------------------------------------| 28.9% Complete\rProgress: |██████████████------------------------------------| 28.9% Complete\rProgress: |██████████████------------------------------------| 29.0% Complete\rProgress: |██████████████------------------------------------| 29.1% Complete\rProgress: |██████████████------------------------------------| 29.1% Complete\rProgress: |██████████████------------------------------------| 29.2% Complete\rProgress: |██████████████------------------------------------| 29.2% Complete\rProgress: |██████████████------------------------------------| 29.3% Complete\rProgress: |██████████████------------------------------------| 29.3% Complete\rProgress: |██████████████------------------------------------| 29.4% Complete\rProgress: |██████████████------------------------------------| 29.5% Complete\rProgress: |██████████████------------------------------------| 29.5% Complete\rProgress: |██████████████------------------------------------| 29.6% Complete\rProgress: |██████████████------------------------------------| 29.6% Complete\rProgress: |██████████████------------------------------------| 29.7% Complete\rProgress: |██████████████------------------------------------| 29.8% Complete\rProgress: |██████████████------------------------------------| 29.8% Complete\rProgress: |██████████████------------------------------------| 29.9% Complete\rProgress: |██████████████------------------------------------| 29.9% Complete\rProgress: |██████████████------------------------------------| 30.0% Complete\rProgress: |███████████████-----------------------------------| 30.0% Complete\rProgress: |███████████████-----------------------------------| 30.1% Complete\rProgress: |███████████████-----------------------------------| 30.2% Complete\rProgress: |███████████████-----------------------------------| 30.2% Complete\rProgress: |███████████████-----------------------------------| 30.3% Complete\rProgress: |███████████████-----------------------------------| 30.3% Complete\rProgress: |███████████████-----------------------------------| 30.4% Complete\rProgress: |███████████████-----------------------------------| 30.5% Complete\rProgress: |███████████████-----------------------------------| 30.5% Complete\rProgress: |███████████████-----------------------------------| 30.6% Complete\rProgress: |███████████████-----------------------------------| 30.6% Complete\rProgress: |███████████████-----------------------------------| 30.7% Complete\rProgress: |███████████████-----------------------------------| 30.7% Complete\rProgress: |███████████████-----------------------------------| 30.8% Complete\rProgress: |███████████████-----------------------------------| 30.9% Complete\rProgress: |███████████████-----------------------------------| 30.9% Complete\rProgress: |███████████████-----------------------------------| 31.0% Complete\rProgress: |███████████████-----------------------------------| 31.0% Complete\rProgress: |███████████████-----------------------------------| 31.1% Complete\rProgress: |███████████████-----------------------------------| 31.2% Complete\rProgress: |███████████████-----------------------------------| 31.2% Complete\rProgress: |███████████████-----------------------------------| 31.3% Complete\rProgress: |███████████████-----------------------------------| 31.3% Complete\rProgress: |███████████████-----------------------------------| 31.4% Complete\rProgress: |███████████████-----------------------------------| 31.4% Complete\rProgress: |███████████████-----------------------------------| 31.5% Complete\rProgress: |███████████████-----------------------------------| 31.6% Complete\rProgress: |███████████████-----------------------------------| 31.6% Complete\rProgress: |███████████████-----------------------------------| 31.7% Complete\rProgress: |███████████████-----------------------------------| 31.7% Complete\rProgress: |███████████████-----------------------------------| 31.8% Complete\rProgress: |███████████████-----------------------------------| 31.9% Complete\rProgress: |███████████████-----------------------------------| 31.9% Complete\rProgress: |███████████████-----------------------------------| 32.0% Complete\rProgress: |████████████████----------------------------------| 32.0% Complete\rProgress: |████████████████----------------------------------| 32.1% Complete\rProgress: |████████████████----------------------------------| 32.1% Complete\rProgress: |████████████████----------------------------------| 32.2% Complete\rProgress: |████████████████----------------------------------| 32.3% Complete\rProgress: |████████████████----------------------------------| 32.3% Complete\rProgress: |████████████████----------------------------------| 32.4% Complete\rProgress: |████████████████----------------------------------| 32.4% Complete\rProgress: |████████████████----------------------------------| 32.5% Complete\rProgress: |████████████████----------------------------------| 32.6% Complete\rProgress: |████████████████----------------------------------| 32.6% Complete\rProgress: |████████████████----------------------------------| 32.7% Complete\rProgress: |████████████████----------------------------------| 32.7% Complete\rProgress: |████████████████----------------------------------| 32.8% Complete\rProgress: |████████████████----------------------------------| 32.8% Complete\rProgress: |████████████████----------------------------------| 32.9% Complete\rProgress: |████████████████----------------------------------| 33.0% Complete\rProgress: |████████████████----------------------------------| 33.0% Complete\rProgress: |████████████████----------------------------------| 33.1% Complete\rProgress: |████████████████----------------------------------| 33.1% Complete\rProgress: |████████████████----------------------------------| 33.2% Complete\rProgress: |████████████████----------------------------------| 33.3% Complete\rProgress: |████████████████----------------------------------| 33.3% Complete\rProgress: |████████████████----------------------------------| 33.4% Complete\rProgress: |████████████████----------------------------------| 33.4% Complete\rProgress: |████████████████----------------------------------| 33.5% Complete\rProgress: |████████████████----------------------------------| 33.5% Complete\rProgress: |████████████████----------------------------------| 33.6% Complete\rProgress: |████████████████----------------------------------| 33.7% Complete\rProgress: |████████████████----------------------------------| 33.7% Complete\rProgress: |████████████████----------------------------------| 33.8% Complete\rProgress: |████████████████----------------------------------| 33.8% Complete\rProgress: |████████████████----------------------------------| 33.9% Complete\rProgress: |████████████████----------------------------------| 34.0% Complete\rProgress: |█████████████████---------------------------------| 34.0% Complete\rProgress: |█████████████████---------------------------------| 34.1% Complete\rProgress: |█████████████████---------------------------------| 34.1% Complete\rProgress: |█████████████████---------------------------------| 34.2% Complete\rProgress: |█████████████████---------------------------------| 34.2% Complete\rProgress: |█████████████████---------------------------------| 34.3% Complete\rProgress: |█████████████████---------------------------------| 34.4% Complete\rProgress: |█████████████████---------------------------------| 34.4% Complete\rProgress: |█████████████████---------------------------------| 34.5% Complete\rProgress: |█████████████████---------------------------------| 34.5% Complete\rProgress: |█████████████████---------------------------------| 34.6% Complete\rProgress: |█████████████████---------------------------------| 34.7% Complete\rProgress: |█████████████████---------------------------------| 34.7% Complete\rProgress: |█████████████████---------------------------------| 34.8% Complete\rProgress: |█████████████████---------------------------------| 34.8% Complete\rProgress: |█████████████████---------------------------------| 34.9% Complete\rProgress: |█████████████████---------------------------------| 34.9% Complete\rProgress: |█████████████████---------------------------------| 35.0% Complete\rProgress: |█████████████████---------------------------------| 35.1% Complete\rProgress: |█████████████████---------------------------------| 35.1% Complete\rProgress: |█████████████████---------------------------------| 35.2% Complete\rProgress: |█████████████████---------------------------------| 35.2% Complete\rProgress: |█████████████████---------------------------------| 35.3% Complete\rProgress: |█████████████████---------------------------------| 35.4% Complete\rProgress: |█████████████████---------------------------------| 35.4% Complete\rProgress: |█████████████████---------------------------------| 35.5% Complete\rProgress: |█████████████████---------------------------------| 35.5% Complete\rProgress: |█████████████████---------------------------------| 35.6% Complete\rProgress: |█████████████████---------------------------------| 35.6% Complete\rProgress: |█████████████████---------------------------------| 35.7% Complete\rProgress: |█████████████████---------------------------------| 35.8% Complete\rProgress: |█████████████████---------------------------------| 35.8% Complete\rProgress: |█████████████████---------------------------------| 35.9% Complete\rProgress: |█████████████████---------------------------------| 35.9% Complete\rProgress: |█████████████████---------------------------------| 36.0% Complete\rProgress: |██████████████████--------------------------------| 36.1% Complete\rProgress: |██████████████████--------------------------------| 36.1% Complete\rProgress: |██████████████████--------------------------------| 36.2% Complete\rProgress: |██████████████████--------------------------------| 36.2% Complete\rProgress: |██████████████████--------------------------------| 36.3% Complete\rProgress: |██████████████████--------------------------------| 36.3% Complete\rProgress: |██████████████████--------------------------------| 36.4% Complete\rProgress: |██████████████████--------------------------------| 36.5% Complete\rProgress: |██████████████████--------------------------------| 36.5% Complete\rProgress: |██████████████████--------------------------------| 36.6% Complete\rProgress: |██████████████████--------------------------------| 36.6% Complete\rProgress: |██████████████████--------------------------------| 36.7% Complete\rProgress: |██████████████████--------------------------------| 36.8% Complete\rProgress: |██████████████████--------------------------------| 36.8% Complete\rProgress: |██████████████████--------------------------------| 36.9% Complete\rProgress: |██████████████████--------------------------------| 36.9% Complete\rProgress: |██████████████████--------------------------------| 37.0% Complete\rProgress: |██████████████████--------------------------------| 37.0% Complete\rProgress: |██████████████████--------------------------------| 37.1% Complete\rProgress: |██████████████████--------------------------------| 37.2% Complete\rProgress: |██████████████████--------------------------------| 37.2% Complete\rProgress: |██████████████████--------------------------------| 37.3% Complete\rProgress: |██████████████████--------------------------------| 37.3% Complete\rProgress: |██████████████████--------------------------------| 37.4% Complete\rProgress: |██████████████████--------------------------------| 37.5% Complete\rProgress: |██████████████████--------------------------------| 37.5% Complete\rProgress: |██████████████████--------------------------------| 37.6% Complete\rProgress: |██████████████████--------------------------------| 37.6% Complete\rProgress: |██████████████████--------------------------------| 37.7% Complete\rProgress: |██████████████████--------------------------------| 37.7% Complete\rProgress: |██████████████████--------------------------------| 37.8% Complete\rProgress: |██████████████████--------------------------------| 37.9% Complete\rProgress: |██████████████████--------------------------------| 37.9% Complete\rProgress: |██████████████████--------------------------------| 38.0% Complete\rProgress: |███████████████████-------------------------------| 38.0% Complete\rProgress: |███████████████████-------------------------------| 38.1% Complete\rProgress: |███████████████████-------------------------------| 38.2% Complete\rProgress: |███████████████████-------------------------------| 38.2% Complete\rProgress: |███████████████████-------------------------------| 38.3% Complete\rProgress: |███████████████████-------------------------------| 38.3% Complete\rProgress: |███████████████████-------------------------------| 38.4% Complete\rProgress: |███████████████████-------------------------------| 38.4% Complete\rProgress: |███████████████████-------------------------------| 38.5% Complete\rProgress: |███████████████████-------------------------------| 38.6% Complete\rProgress: |███████████████████-------------------------------| 38.6% Complete\rProgress: |███████████████████-------------------------------| 38.7% Complete\rProgress: |███████████████████-------------------------------| 38.7% Complete\rProgress: |███████████████████-------------------------------| 38.8% Complete\rProgress: |███████████████████-------------------------------| 38.9% Complete\rProgress: |███████████████████-------------------------------| 38.9% Complete\rProgress: |███████████████████-------------------------------| 39.0% Complete\rProgress: |███████████████████-------------------------------| 39.0% Complete\rProgress: |███████████████████-------------------------------| 39.1% Complete\rProgress: |███████████████████-------------------------------| 39.1% Complete\rProgress: |███████████████████-------------------------------| 39.2% Complete\rProgress: |███████████████████-------------------------------| 39.3% Complete\rProgress: |███████████████████-------------------------------| 39.3% Complete\rProgress: |███████████████████-------------------------------| 39.4% Complete\rProgress: |███████████████████-------------------------------| 39.4% Complete\rProgress: |███████████████████-------------------------------| 39.5% Complete\rProgress: |███████████████████-------------------------------| 39.6% Complete\rProgress: |███████████████████-------------------------------| 39.6% Complete\rProgress: |███████████████████-------------------------------| 39.7% Complete\rProgress: |███████████████████-------------------------------| 39.7% Complete\rProgress: |███████████████████-------------------------------| 39.8% Complete\rProgress: |███████████████████-------------------------------| 39.8% Complete\rProgress: |███████████████████-------------------------------| 39.9% Complete\rProgress: |███████████████████-------------------------------| 40.0% Complete\rProgress: |████████████████████------------------------------| 40.0% Complete\rProgress: |████████████████████------------------------------| 40.1% Complete\rProgress: |████████████████████------------------------------| 40.1% Complete\rProgress: |████████████████████------------------------------| 40.2% Complete\rProgress: |████████████████████------------------------------| 40.3% Complete\rProgress: |████████████████████------------------------------| 40.3% Complete\rProgress: |████████████████████------------------------------| 40.4% Complete\rProgress: |████████████████████------------------------------| 40.4% Complete\rProgress: |████████████████████------------------------------| 40.5% Complete\rProgress: |████████████████████------------------------------| 40.5% Complete\rProgress: |████████████████████------------------------------| 40.6% Complete\rProgress: |████████████████████------------------------------| 40.7% Complete\rProgress: |████████████████████------------------------------| 40.7% Complete\rProgress: |████████████████████------------------------------| 40.8% Complete\rProgress: |████████████████████------------------------------| 40.8% Complete\rProgress: |████████████████████------------------------------| 40.9% Complete\rProgress: |████████████████████------------------------------| 41.0% Complete\rProgress: |████████████████████------------------------------| 41.0% Complete\rProgress: |████████████████████------------------------------| 41.1% Complete\rProgress: |████████████████████------------------------------| 41.1% Complete\rProgress: |████████████████████------------------------------| 41.2% Complete\rProgress: |████████████████████------------------------------| 41.2% Complete\rProgress: |████████████████████------------------------------| 41.3% Complete\rProgress: |████████████████████------------------------------| 41.4% Complete\rProgress: |████████████████████------------------------------| 41.4% Complete\rProgress: |████████████████████------------------------------| 41.5% Complete\rProgress: |████████████████████------------------------------| 41.5% Complete\rProgress: |████████████████████------------------------------| 41.6% Complete\rProgress: |████████████████████------------------------------| 41.7% Complete\rProgress: |████████████████████------------------------------| 41.7% Complete\rProgress: |████████████████████------------------------------| 41.8% Complete\rProgress: |████████████████████------------------------------| 41.8% Complete\rProgress: |████████████████████------------------------------| 41.9% Complete\rProgress: |████████████████████------------------------------| 41.9% Complete\rProgress: |█████████████████████-----------------------------| 42.0% Complete\rProgress: |█████████████████████-----------------------------| 42.1% Complete\rProgress: |█████████████████████-----------------------------| 42.1% Complete\rProgress: |█████████████████████-----------------------------| 42.2% Complete\rProgress: |█████████████████████-----------------------------| 42.2% Complete\rProgress: |█████████████████████-----------------------------| 42.3% Complete\rProgress: |█████████████████████-----------------------------| 42.4% Complete\rProgress: |█████████████████████-----------------------------| 42.4% Complete\rProgress: |█████████████████████-----------------------------| 42.5% Complete\rProgress: |█████████████████████-----------------------------| 42.5% Complete\rProgress: |█████████████████████-----------------------------| 42.6% Complete\rProgress: |█████████████████████-----------------------------| 42.6% Complete\rProgress: |█████████████████████-----------------------------| 42.7% Complete\rProgress: |█████████████████████-----------------------------| 42.8% Complete\rProgress: |█████████████████████-----------------------------| 42.8% Complete\rProgress: |█████████████████████-----------------------------| 42.9% Complete\rProgress: |█████████████████████-----------------------------| 42.9% Complete\rProgress: |█████████████████████-----------------------------| 43.0% Complete\rProgress: |█████████████████████-----------------------------| 43.1% Complete\rProgress: |█████████████████████-----------------------------| 43.1% Complete\rProgress: |█████████████████████-----------------------------| 43.2% Complete\rProgress: |█████████████████████-----------------------------| 43.2% Complete\rProgress: |█████████████████████-----------------------------| 43.3% Complete\rProgress: |█████████████████████-----------------------------| 43.3% Complete\rProgress: |█████████████████████-----------------------------| 43.4% Complete\rProgress: |█████████████████████-----------------------------| 43.5% Complete\rProgress: |█████████████████████-----------------------------| 43.5% Complete\rProgress: |█████████████████████-----------------------------| 43.6% Complete\rProgress: |█████████████████████-----------------------------| 43.6% Complete\rProgress: |█████████████████████-----------------------------| 43.7% Complete\rProgress: |█████████████████████-----------------------------| 43.8% Complete\rProgress: |█████████████████████-----------------------------| 43.8% Complete\rProgress: |█████████████████████-----------------------------| 43.9% Complete\rProgress: |█████████████████████-----------------------------| 43.9% Complete\rProgress: |█████████████████████-----------------------------| 44.0% Complete\rProgress: |██████████████████████----------------------------| 44.0% Complete\rProgress: |██████████████████████----------------------------| 44.1% Complete\rProgress: |██████████████████████----------------------------| 44.2% Complete\rProgress: |██████████████████████----------------------------| 44.2% Complete\rProgress: |██████████████████████----------------------------| 44.3% Complete\rProgress: |██████████████████████----------------------------| 44.3% Complete\rProgress: |██████████████████████----------------------------| 44.4% Complete\rProgress: |██████████████████████----------------------------| 44.5% Complete\rProgress: |██████████████████████----------------------------| 44.5% Complete\rProgress: |██████████████████████----------------------------| 44.6% Complete\rProgress: |██████████████████████----------------------------| 44.6% Complete\rProgress: |██████████████████████----------------------------| 44.7% Complete\rProgress: |██████████████████████----------------------------| 44.7% Complete\rProgress: |██████████████████████----------------------------| 44.8% Complete\rProgress: |██████████████████████----------------------------| 44.9% Complete\rProgress: |██████████████████████----------------------------| 44.9% Complete\rProgress: |██████████████████████----------------------------| 45.0% Complete\rProgress: |██████████████████████----------------------------| 45.0% Complete\rProgress: |██████████████████████----------------------------| 45.1% Complete\rProgress: |██████████████████████----------------------------| 45.2% Complete\rProgress: |██████████████████████----------------------------| 45.2% Complete\rProgress: |██████████████████████----------------------------| 45.3% Complete\rProgress: |██████████████████████----------------------------| 45.3% Complete\rProgress: |██████████████████████----------------------------| 45.4% Complete\rProgress: |██████████████████████----------------------------| 45.4% Complete\rProgress: |██████████████████████----------------------------| 45.5% Complete\rProgress: |██████████████████████----------------------------| 45.6% Complete\rProgress: |██████████████████████----------------------------| 45.6% Complete\rProgress: |██████████████████████----------------------------| 45.7% Complete\rProgress: |██████████████████████----------------------------| 45.7% Complete\rProgress: |██████████████████████----------------------------| 45.8% Complete\rProgress: |██████████████████████----------------------------| 45.9% Complete\rProgress: |██████████████████████----------------------------| 45.9% Complete\rProgress: |██████████████████████----------------------------| 46.0% Complete\rProgress: |███████████████████████---------------------------| 46.0% Complete\rProgress: |███████████████████████---------------------------| 46.1% Complete\rProgress: |███████████████████████---------------------------| 46.1% Complete\rProgress: |███████████████████████---------------------------| 46.2% Complete\rProgress: |███████████████████████---------------------------| 46.3% Complete\rProgress: |███████████████████████---------------------------| 46.3% Complete\rProgress: |███████████████████████---------------------------| 46.4% Complete\rProgress: |███████████████████████---------------------------| 46.4% Complete\rProgress: |███████████████████████---------------------------| 46.5% Complete\rProgress: |███████████████████████---------------------------| 46.6% Complete\rProgress: |███████████████████████---------------------------| 46.6% Complete\rProgress: |███████████████████████---------------------------| 46.7% Complete\rProgress: |███████████████████████---------------------------| 46.7% Complete\rProgress: |███████████████████████---------------------------| 46.8% Complete\rProgress: |███████████████████████---------------------------| 46.8% Complete\rProgress: |███████████████████████---------------------------| 46.9% Complete\rProgress: |███████████████████████---------------------------| 47.0% Complete\rProgress: |███████████████████████---------------------------| 47.0% Complete\rProgress: |███████████████████████---------------------------| 47.1% Complete\rProgress: |███████████████████████---------------------------| 47.1% Complete\rProgress: |███████████████████████---------------------------| 47.2% Complete\rProgress: |███████████████████████---------------------------| 47.3% Complete\rProgress: |███████████████████████---------------------------| 47.3% Complete\rProgress: |███████████████████████---------------------------| 47.4% Complete\rProgress: |███████████████████████---------------------------| 47.4% Complete\rProgress: |███████████████████████---------------------------| 47.5% Complete\rProgress: |███████████████████████---------------------------| 47.6% Complete\rProgress: |███████████████████████---------------------------| 47.6% Complete\rProgress: |███████████████████████---------------------------| 47.7% Complete\rProgress: |███████████████████████---------------------------| 47.7% Complete\rProgress: |███████████████████████---------------------------| 47.8% Complete\rProgress: |███████████████████████---------------------------| 47.8% Complete\rProgress: |███████████████████████---------------------------| 47.9% Complete\rProgress: |███████████████████████---------------------------| 48.0% Complete\rProgress: |████████████████████████--------------------------| 48.0% Complete\rProgress: |████████████████████████--------------------------| 48.1% Complete\rProgress: |████████████████████████--------------------------| 48.1% Complete\rProgress: |████████████████████████--------------------------| 48.2% Complete\rProgress: |████████████████████████--------------------------| 48.3% Complete\rProgress: |████████████████████████--------------------------| 48.3% Complete\rProgress: |████████████████████████--------------------------| 48.4% Complete\rProgress: |████████████████████████--------------------------| 48.4% Complete\rProgress: |████████████████████████--------------------------| 48.5% Complete\rProgress: |████████████████████████--------------------------| 48.5% Complete\rProgress: |████████████████████████--------------------------| 48.6% Complete\rProgress: |████████████████████████--------------------------| 48.7% Complete\rProgress: |████████████████████████--------------------------| 48.7% Complete\rProgress: |████████████████████████--------------------------| 48.8% Complete\rProgress: |████████████████████████--------------------------| 48.8% Complete\rProgress: |████████████████████████--------------------------| 48.9% Complete\rProgress: |████████████████████████--------------------------| 49.0% Complete\rProgress: |████████████████████████--------------------------| 49.0% Complete\rProgress: |████████████████████████--------------------------| 49.1% Complete\rProgress: |████████████████████████--------------------------| 49.1% Complete\rProgress: |████████████████████████--------------------------| 49.2% Complete\rProgress: |████████████████████████--------------------------| 49.2% Complete\rProgress: |████████████████████████--------------------------| 49.3% Complete\rProgress: |████████████████████████--------------------------| 49.4% Complete\rProgress: |████████████████████████--------------------------| 49.4% Complete\rProgress: |████████████████████████--------------------------| 49.5% Complete\rProgress: |████████████████████████--------------------------| 49.5% Complete\rProgress: |████████████████████████--------------------------| 49.6% Complete\rProgress: |████████████████████████--------------------------| 49.7% Complete\rProgress: |████████████████████████--------------------------| 49.7% Complete\rProgress: |████████████████████████--------------------------| 49.8% Complete\rProgress: |████████████████████████--------------------------| 49.8% Complete\rProgress: |████████████████████████--------------------------| 49.9% Complete\rProgress: |████████████████████████--------------------------| 49.9% Complete\rProgress: |█████████████████████████-------------------------| 50.0% Complete\rProgress: |█████████████████████████-------------------------| 50.1% Complete\rProgress: |█████████████████████████-------------------------| 50.1% Complete\rProgress: |█████████████████████████-------------------------| 50.2% Complete\rProgress: |█████████████████████████-------------------------| 50.2% Complete\rProgress: |█████████████████████████-------------------------| 50.3% Complete\rProgress: |█████████████████████████-------------------------| 50.4% Complete\rProgress: |█████████████████████████-------------------------| 50.4% Complete\rProgress: |█████████████████████████-------------------------| 50.5% Complete\rProgress: |█████████████████████████-------------------------| 50.5% Complete\rProgress: |█████████████████████████-------------------------| 50.6% Complete\rProgress: |█████████████████████████-------------------------| 50.6% Complete\rProgress: |█████████████████████████-------------------------| 50.7% Complete\rProgress: |█████████████████████████-------------------------| 50.8% Complete\rProgress: |█████████████████████████-------------------------| 50.8% Complete\rProgress: |█████████████████████████-------------------------| 50.9% Complete\rProgress: |█████████████████████████-------------------------| 50.9% Complete\rProgress: |█████████████████████████-------------------------| 51.0% Complete\rProgress: |█████████████████████████-------------------------| 51.1% Complete\rProgress: |█████████████████████████-------------------------| 51.1% Complete\rProgress: |█████████████████████████-------------------------| 51.2% Complete\rProgress: |█████████████████████████-------------------------| 51.2% Complete\rProgress: |█████████████████████████-------------------------| 51.3% Complete\rProgress: |█████████████████████████-------------------------| 51.3% Complete\rProgress: |█████████████████████████-------------------------| 51.4% Complete\rProgress: |█████████████████████████-------------------------| 51.5% Complete\rProgress: |█████████████████████████-------------------------| 51.5% Complete\rProgress: |█████████████████████████-------------------------| 51.6% Complete\rProgress: |█████████████████████████-------------------------| 51.6% Complete\rProgress: |█████████████████████████-------------------------| 51.7% Complete\rProgress: |█████████████████████████-------------------------| 51.8% Complete\rProgress: |█████████████████████████-------------------------| 51.8% Complete\rProgress: |█████████████████████████-------------------------| 51.9% Complete\rProgress: |█████████████████████████-------------------------| 51.9% Complete\rProgress: |█████████████████████████-------------------------| 52.0% Complete\rProgress: |██████████████████████████------------------------| 52.0% Complete\rProgress: |██████████████████████████------------------------| 52.1% Complete\rProgress: |██████████████████████████------------------------| 52.2% Complete\rProgress: |██████████████████████████------------------------| 52.2% Complete\rProgress: |██████████████████████████------------------------| 52.3% Complete\rProgress: |██████████████████████████------------------------| 52.3% Complete\rProgress: |██████████████████████████------------------------| 52.4% Complete\rProgress: |██████████████████████████------------------------| 52.5% Complete\rProgress: |██████████████████████████------------------------| 52.5% Complete\rProgress: |██████████████████████████------------------------| 52.6% Complete\rProgress: |██████████████████████████------------------------| 52.6% Complete\rProgress: |██████████████████████████------------------------| 52.7% Complete\rProgress: |██████████████████████████------------------------| 52.7% Complete\rProgress: |██████████████████████████------------------------| 52.8% Complete\rProgress: |██████████████████████████------------------------| 52.9% Complete\rProgress: |██████████████████████████------------------------| 52.9% Complete\rProgress: |██████████████████████████------------------------| 53.0% Complete\rProgress: |██████████████████████████------------------------| 53.0% Complete\rProgress: |██████████████████████████------------------------| 53.1% Complete\rProgress: |██████████████████████████------------------------| 53.2% Complete\rProgress: |██████████████████████████------------------------| 53.2% Complete\rProgress: |██████████████████████████------------------------| 53.3% Complete\rProgress: |██████████████████████████------------------------| 53.3% Complete\rProgress: |██████████████████████████------------------------| 53.4% Complete\rProgress: |██████████████████████████------------------------| 53.4% Complete\rProgress: |██████████████████████████------------------------| 53.5% Complete\rProgress: |██████████████████████████------------------------| 53.6% Complete\rProgress: |██████████████████████████------------------------| 53.6% Complete\rProgress: |██████████████████████████------------------------| 53.7% Complete\rProgress: |██████████████████████████------------------------| 53.7% Complete\rProgress: |██████████████████████████------------------------| 53.8% Complete\rProgress: |██████████████████████████------------------------| 53.9% Complete\rProgress: |██████████████████████████------------------------| 53.9% Complete\rProgress: |██████████████████████████------------------------| 54.0% Complete\rProgress: |███████████████████████████-----------------------| 54.0% Complete\rProgress: |███████████████████████████-----------------------| 54.1% Complete\rProgress: |███████████████████████████-----------------------| 54.1% Complete\rProgress: |███████████████████████████-----------------------| 54.2% Complete\rProgress: |███████████████████████████-----------------------| 54.3% Complete\rProgress: |███████████████████████████-----------------------| 54.3% Complete\rProgress: |███████████████████████████-----------------------| 54.4% Complete\rProgress: |███████████████████████████-----------------------| 54.4% Complete\rProgress: |███████████████████████████-----------------------| 54.5% Complete\rProgress: |███████████████████████████-----------------------| 54.6% Complete\rProgress: |███████████████████████████-----------------------| 54.6% Complete\rProgress: |███████████████████████████-----------------------| 54.7% Complete\rProgress: |███████████████████████████-----------------------| 54.7% Complete\rProgress: |███████████████████████████-----------------------| 54.8% Complete\rProgress: |███████████████████████████-----------------------| 54.8% Complete\rProgress: |███████████████████████████-----------------------| 54.9% Complete\rProgress: |███████████████████████████-----------------------| 55.0% Complete\rProgress: |███████████████████████████-----------------------| 55.0% Complete\rProgress: |███████████████████████████-----------------------| 55.1% Complete\rProgress: |███████████████████████████-----------------------| 55.1% Complete\rProgress: |███████████████████████████-----------------------| 55.2% Complete\rProgress: |███████████████████████████-----------------------| 55.3% Complete\rProgress: |███████████████████████████-----------------------| 55.3% Complete\rProgress: |███████████████████████████-----------------------| 55.4% Complete\rProgress: |███████████████████████████-----------------------| 55.4% Complete\rProgress: |███████████████████████████-----------------------| 55.5% Complete\rProgress: |███████████████████████████-----------------------| 55.5% Complete\rProgress: |███████████████████████████-----------------------| 55.6% Complete\rProgress: |███████████████████████████-----------------------| 55.7% Complete\rProgress: |███████████████████████████-----------------------| 55.7% Complete\rProgress: |███████████████████████████-----------------------| 55.8% Complete\rProgress: |███████████████████████████-----------------------| 55.8% Complete\rProgress: |███████████████████████████-----------------------| 55.9% Complete\rProgress: |███████████████████████████-----------------------| 56.0% Complete\rProgress: |████████████████████████████----------------------| 56.0% Complete\rProgress: |████████████████████████████----------------------| 56.1% Complete\rProgress: |████████████████████████████----------------------| 56.1% Complete\rProgress: |████████████████████████████----------------------| 56.2% Complete\rProgress: |████████████████████████████----------------------| 56.2% Complete\rProgress: |████████████████████████████----------------------| 56.3% Complete\rProgress: |████████████████████████████----------------------| 56.4% Complete\rProgress: |████████████████████████████----------------------| 56.4% Complete\rProgress: |████████████████████████████----------------------| 56.5% Complete\rProgress: |████████████████████████████----------------------| 56.5% Complete\rProgress: |████████████████████████████----------------------| 56.6% Complete\rProgress: |████████████████████████████----------------------| 56.7% Complete\rProgress: |████████████████████████████----------------------| 56.7% Complete\rProgress: |████████████████████████████----------------------| 56.8% Complete\rProgress: |████████████████████████████----------------------| 56.8% Complete\rProgress: |████████████████████████████----------------------| 56.9% Complete\rProgress: |████████████████████████████----------------------| 56.9% Complete\rProgress: |████████████████████████████----------------------| 57.0% Complete\rProgress: |████████████████████████████----------------------| 57.1% Complete\rProgress: |████████████████████████████----------------------| 57.1% Complete\rProgress: |████████████████████████████----------------------| 57.2% Complete\rProgress: |████████████████████████████----------------------| 57.2% Complete\rProgress: |████████████████████████████----------------------| 57.3% Complete\rProgress: |████████████████████████████----------------------| 57.4% Complete\rProgress: |████████████████████████████----------------------| 57.4% Complete\rProgress: |████████████████████████████----------------------| 57.5% Complete\rProgress: |████████████████████████████----------------------| 57.5% Complete\rProgress: |████████████████████████████----------------------| 57.6% Complete\rProgress: |████████████████████████████----------------------| 57.6% Complete\rProgress: |████████████████████████████----------------------| 57.7% Complete\rProgress: |████████████████████████████----------------------| 57.8% Complete\rProgress: |████████████████████████████----------------------| 57.8% Complete\rProgress: |████████████████████████████----------------------| 57.9% Complete\rProgress: |████████████████████████████----------------------| 57.9% Complete\rProgress: |████████████████████████████----------------------| 58.0% Complete\rProgress: |█████████████████████████████---------------------| 58.1% Complete\rProgress: |█████████████████████████████---------------------| 58.1% Complete\rProgress: |█████████████████████████████---------------------| 58.2% Complete\rProgress: |█████████████████████████████---------------------| 58.2% Complete\rProgress: |█████████████████████████████---------------------| 58.3% Complete\rProgress: |█████████████████████████████---------------------| 58.3% Complete\rProgress: |█████████████████████████████---------------------| 58.4% Complete\rProgress: |█████████████████████████████---------------------| 58.5% Complete\rProgress: |█████████████████████████████---------------------| 58.5% Complete\rProgress: |█████████████████████████████---------------------| 58.6% Complete\rProgress: |█████████████████████████████---------------------| 58.6% Complete\rProgress: |█████████████████████████████---------------------| 58.7% Complete\rProgress: |█████████████████████████████---------------------| 58.8% Complete\rProgress: |█████████████████████████████---------------------| 58.8% Complete\rProgress: |█████████████████████████████---------------------| 58.9% Complete\rProgress: |█████████████████████████████---------------------| 58.9% Complete\rProgress: |█████████████████████████████---------------------| 59.0% Complete\rProgress: |█████████████████████████████---------------------| 59.0% Complete\rProgress: |█████████████████████████████---------------------| 59.1% Complete\rProgress: |█████████████████████████████---------------------| 59.2% Complete\rProgress: |█████████████████████████████---------------------| 59.2% Complete\rProgress: |█████████████████████████████---------------------| 59.3% Complete\rProgress: |█████████████████████████████---------------------| 59.3% Complete\rProgress: |█████████████████████████████---------------------| 59.4% Complete\rProgress: |█████████████████████████████---------------------| 59.5% Complete\rProgress: |█████████████████████████████---------------------| 59.5% Complete\rProgress: |█████████████████████████████---------------------| 59.6% Complete\rProgress: |█████████████████████████████---------------------| 59.6% Complete\rProgress: |█████████████████████████████---------------------| 59.7% Complete\rProgress: |█████████████████████████████---------------------| 59.7% Complete\rProgress: |█████████████████████████████---------------------| 59.8% Complete\rProgress: |█████████████████████████████---------------------| 59.9% Complete\rProgress: |█████████████████████████████---------------------| 59.9% Complete\rProgress: |█████████████████████████████---------------------| 60.0% Complete\rProgress: |██████████████████████████████--------------------| 60.0% Complete\rProgress: |██████████████████████████████--------------------| 60.1% Complete\rProgress: |██████████████████████████████--------------------| 60.2% Complete\rProgress: |██████████████████████████████--------------------| 60.2% Complete\rProgress: |██████████████████████████████--------------------| 60.3% Complete\rProgress: |██████████████████████████████--------------------| 60.3% Complete\rProgress: |██████████████████████████████--------------------| 60.4% Complete\rProgress: |██████████████████████████████--------------------| 60.4% Complete\rProgress: |██████████████████████████████--------------------| 60.5% Complete\rProgress: |██████████████████████████████--------------------| 60.6% Complete\rProgress: |██████████████████████████████--------------------| 60.6% Complete\rProgress: |██████████████████████████████--------------------| 60.7% Complete\rProgress: |██████████████████████████████--------------------| 60.7% Complete\rProgress: |██████████████████████████████--------------------| 60.8% Complete\rProgress: |██████████████████████████████--------------------| 60.9% Complete\rProgress: |██████████████████████████████--------------------| 60.9% Complete\rProgress: |██████████████████████████████--------------------| 61.0% Complete\rProgress: |██████████████████████████████--------------------| 61.0% Complete\rProgress: |██████████████████████████████--------------------| 61.1% Complete\rProgress: |██████████████████████████████--------------------| 61.1% Complete\rProgress: |██████████████████████████████--------------------| 61.2% Complete\rProgress: |██████████████████████████████--------------------| 61.3% Complete\rProgress: |██████████████████████████████--------------------| 61.3% Complete\rProgress: |██████████████████████████████--------------------| 61.4% Complete\rProgress: |██████████████████████████████--------------------| 61.4% Complete\rProgress: |██████████████████████████████--------------------| 61.5% Complete\rProgress: |██████████████████████████████--------------------| 61.6% Complete\rProgress: |██████████████████████████████--------------------| 61.6% Complete\rProgress: |██████████████████████████████--------------------| 61.7% Complete\rProgress: |██████████████████████████████--------------------| 61.7% Complete\rProgress: |██████████████████████████████--------------------| 61.8% Complete\rProgress: |██████████████████████████████--------------------| 61.8% Complete\rProgress: |██████████████████████████████--------------------| 61.9% Complete\rProgress: |██████████████████████████████--------------------| 62.0% Complete\rProgress: |███████████████████████████████-------------------| 62.0% Complete\rProgress: |███████████████████████████████-------------------| 62.1% Complete\rProgress: |███████████████████████████████-------------------| 62.1% Complete\rProgress: |███████████████████████████████-------------------| 62.2% Complete\rProgress: |███████████████████████████████-------------------| 62.3% Complete\rProgress: |███████████████████████████████-------------------| 62.3% Complete\rProgress: |███████████████████████████████-------------------| 62.4% Complete\rProgress: |███████████████████████████████-------------------| 62.4% Complete\rProgress: |███████████████████████████████-------------------| 62.5% Complete\rProgress: |███████████████████████████████-------------------| 62.5% Complete\rProgress: |███████████████████████████████-------------------| 62.6% Complete\rProgress: |███████████████████████████████-------------------| 62.7% Complete\rProgress: |███████████████████████████████-------------------| 62.7% Complete\rProgress: |███████████████████████████████-------------------| 62.8% Complete\rProgress: |███████████████████████████████-------------------| 62.8% Complete\rProgress: |███████████████████████████████-------------------| 62.9% Complete\rProgress: |███████████████████████████████-------------------| 63.0% Complete\rProgress: |███████████████████████████████-------------------| 63.0% Complete\rProgress: |███████████████████████████████-------------------| 63.1% Complete\rProgress: |███████████████████████████████-------------------| 63.1% Complete\rProgress: |███████████████████████████████-------------------| 63.2% Complete\rProgress: |███████████████████████████████-------------------| 63.2% Complete\rProgress: |███████████████████████████████-------------------| 63.3% Complete\rProgress: |███████████████████████████████-------------------| 63.4% Complete\rProgress: |███████████████████████████████-------------------| 63.4% Complete\rProgress: |███████████████████████████████-------------------| 63.5% Complete\rProgress: |███████████████████████████████-------------------| 63.5% Complete\rProgress: |███████████████████████████████-------------------| 63.6% Complete\rProgress: |███████████████████████████████-------------------| 63.7% Complete\rProgress: |███████████████████████████████-------------------| 63.7% Complete\rProgress: |███████████████████████████████-------------------| 63.8% Complete\rProgress: |███████████████████████████████-------------------| 63.8% Complete\rProgress: |███████████████████████████████-------------------| 63.9% Complete\rProgress: |███████████████████████████████-------------------| 63.9% Complete\rProgress: |████████████████████████████████------------------| 64.0% Complete\rProgress: |████████████████████████████████------------------| 64.1% Complete\rProgress: |████████████████████████████████------------------| 64.1% Complete\rProgress: |████████████████████████████████------------------| 64.2% Complete\rProgress: |████████████████████████████████------------------| 64.2% Complete\rProgress: |████████████████████████████████------------------| 64.3% Complete\rProgress: |████████████████████████████████------------------| 64.4% Complete\rProgress: |████████████████████████████████------------------| 64.4% Complete\rProgress: |████████████████████████████████------------------| 64.5% Complete\rProgress: |████████████████████████████████------------------| 64.5% Complete\rProgress: |████████████████████████████████------------------| 64.6% Complete\rProgress: |████████████████████████████████------------------| 64.6% Complete\rProgress: |████████████████████████████████------------------| 64.7% Complete\rProgress: |████████████████████████████████------------------| 64.8% Complete\rProgress: |████████████████████████████████------------------| 64.8% Complete\rProgress: |████████████████████████████████------------------| 64.9% Complete\rProgress: |████████████████████████████████------------------| 64.9% Complete\rProgress: |████████████████████████████████------------------| 65.0% Complete\rProgress: |████████████████████████████████------------------| 65.1% Complete\rProgress: |████████████████████████████████------------------| 65.1% Complete\rProgress: |████████████████████████████████------------------| 65.2% Complete\rProgress: |████████████████████████████████------------------| 65.2% Complete\rProgress: |████████████████████████████████------------------| 65.3% Complete\rProgress: |████████████████████████████████------------------| 65.3% Complete\rProgress: |████████████████████████████████------------------| 65.4% Complete\rProgress: |████████████████████████████████------------------| 65.5% Complete\rProgress: |████████████████████████████████------------------| 65.5% Complete\rProgress: |████████████████████████████████------------------| 65.6% Complete\rProgress: |████████████████████████████████------------------| 65.6% Complete\rProgress: |████████████████████████████████------------------| 65.7% Complete\rProgress: |████████████████████████████████------------------| 65.8% Complete\rProgress: |████████████████████████████████------------------| 65.8% Complete\rProgress: |████████████████████████████████------------------| 65.9% Complete\rProgress: |████████████████████████████████------------------| 65.9% Complete\rProgress: |████████████████████████████████------------------| 66.0% Complete\rProgress: |█████████████████████████████████-----------------| 66.0% Complete\rProgress: |█████████████████████████████████-----------------| 66.1% Complete\rProgress: |█████████████████████████████████-----------------| 66.2% Complete\rProgress: |█████████████████████████████████-----------------| 66.2% Complete\rProgress: |█████████████████████████████████-----------------| 66.3% Complete\rProgress: |█████████████████████████████████-----------------| 66.3% Complete\rProgress: |█████████████████████████████████-----------------| 66.4% Complete\rProgress: |█████████████████████████████████-----------------| 66.5% Complete\rProgress: |█████████████████████████████████-----------------| 66.5% Complete\rProgress: |█████████████████████████████████-----------------| 66.6% Complete\rProgress: |█████████████████████████████████-----------------| 66.6% Complete\rProgress: |█████████████████████████████████-----------------| 66.7% Complete\rProgress: |█████████████████████████████████-----------------| 66.7% Complete\rProgress: |█████████████████████████████████-----------------| 66.8% Complete\rProgress: |█████████████████████████████████-----------------| 66.9% Complete\rProgress: |█████████████████████████████████-----------------| 66.9% Complete\rProgress: |█████████████████████████████████-----------------| 67.0% Complete\rProgress: |█████████████████████████████████-----------------| 67.0% Complete\rProgress: |█████████████████████████████████-----------------| 67.1% Complete\rProgress: |█████████████████████████████████-----------------| 67.2% Complete\rProgress: |█████████████████████████████████-----------------| 67.2% Complete\rProgress: |█████████████████████████████████-----------------| 67.3% Complete\rProgress: |█████████████████████████████████-----------------| 67.3% Complete\rProgress: |█████████████████████████████████-----------------| 67.4% Complete\rProgress: |█████████████████████████████████-----------------| 67.4% Complete\rProgress: |█████████████████████████████████-----------------| 67.5% Complete\rProgress: |█████████████████████████████████-----------------| 67.6% Complete\rProgress: |█████████████████████████████████-----------------| 67.6% Complete\rProgress: |█████████████████████████████████-----------------| 67.7% Complete\rProgress: |█████████████████████████████████-----------------| 67.7% Complete\rProgress: |█████████████████████████████████-----------------| 67.8% Complete\rProgress: |█████████████████████████████████-----------------| 67.9% Complete\rProgress: |█████████████████████████████████-----------------| 67.9% Complete\rProgress: |█████████████████████████████████-----------------| 68.0% Complete\rProgress: |██████████████████████████████████----------------| 68.0% Complete\rProgress: |██████████████████████████████████----------------| 68.1% Complete\rProgress: |██████████████████████████████████----------------| 68.1% Complete\rProgress: |██████████████████████████████████----------------| 68.2% Complete\rProgress: |██████████████████████████████████----------------| 68.3% Complete\rProgress: |██████████████████████████████████----------------| 68.3% Complete\rProgress: |██████████████████████████████████----------------| 68.4% Complete\rProgress: |██████████████████████████████████----------------| 68.4% Complete\rProgress: |██████████████████████████████████----------------| 68.5% Complete\rProgress: |██████████████████████████████████----------------| 68.6% Complete\rProgress: |██████████████████████████████████----------------| 68.6% Complete\rProgress: |██████████████████████████████████----------------| 68.7% Complete\rProgress: |██████████████████████████████████----------------| 68.7% Complete\rProgress: |██████████████████████████████████----------------| 68.8% Complete\rProgress: |██████████████████████████████████----------------| 68.8% Complete\rProgress: |██████████████████████████████████----------------| 68.9% Complete\rProgress: |██████████████████████████████████----------------| 69.0% Complete\rProgress: |██████████████████████████████████----------------| 69.0% Complete\rProgress: |██████████████████████████████████----------------| 69.1% Complete\rProgress: |██████████████████████████████████----------------| 69.1% Complete\rProgress: |██████████████████████████████████----------------| 69.2% Complete\rProgress: |██████████████████████████████████----------------| 69.3% Complete\rProgress: |██████████████████████████████████----------------| 69.3% Complete\rProgress: |██████████████████████████████████----------------| 69.4% Complete\rProgress: |██████████████████████████████████----------------| 69.4% Complete\rProgress: |██████████████████████████████████----------------| 69.5% Complete\rProgress: |██████████████████████████████████----------------| 69.5% Complete\rProgress: |██████████████████████████████████----------------| 69.6% Complete\rProgress: |██████████████████████████████████----------------| 69.7% Complete\rProgress: |██████████████████████████████████----------------| 69.7% Complete\rProgress: |██████████████████████████████████----------------| 69.8% Complete\rProgress: |██████████████████████████████████----------------| 69.8% Complete\rProgress: |██████████████████████████████████----------------| 69.9% Complete\rProgress: |██████████████████████████████████----------------| 70.0% Complete\rProgress: |███████████████████████████████████---------------| 70.0% Complete\rProgress: |███████████████████████████████████---------------| 70.1% Complete\rProgress: |███████████████████████████████████---------------| 70.1% Complete\rProgress: |███████████████████████████████████---------------| 70.2% Complete\rProgress: |███████████████████████████████████---------------| 70.2% Complete\rProgress: |███████████████████████████████████---------------| 70.3% Complete\rProgress: |███████████████████████████████████---------------| 70.4% Complete\rProgress: |███████████████████████████████████---------------| 70.4% Complete\rProgress: |███████████████████████████████████---------------| 70.5% Complete\rProgress: |███████████████████████████████████---------------| 70.5% Complete\rProgress: |███████████████████████████████████---------------| 70.6% Complete\rProgress: |███████████████████████████████████---------------| 70.7% Complete\rProgress: |███████████████████████████████████---------------| 70.7% Complete\rProgress: |███████████████████████████████████---------------| 70.8% Complete\rProgress: |███████████████████████████████████---------------| 70.8% Complete\rProgress: |███████████████████████████████████---------------| 70.9% Complete\rProgress: |███████████████████████████████████---------------| 70.9% Complete\rProgress: |███████████████████████████████████---------------| 71.0% Complete\rProgress: |███████████████████████████████████---------------| 71.1% Complete\rProgress: |███████████████████████████████████---------------| 71.1% Complete\rProgress: |███████████████████████████████████---------------| 71.2% Complete\rProgress: |███████████████████████████████████---------------| 71.2% Complete\rProgress: |███████████████████████████████████---------------| 71.3% Complete\rProgress: |███████████████████████████████████---------------| 71.4% Complete\rProgress: |███████████████████████████████████---------------| 71.4% Complete\rProgress: |███████████████████████████████████---------------| 71.5% Complete\rProgress: |███████████████████████████████████---------------| 71.5% Complete\rProgress: |███████████████████████████████████---------------| 71.6% Complete\rProgress: |███████████████████████████████████---------------| 71.6% Complete\rProgress: |███████████████████████████████████---------------| 71.7% Complete\rProgress: |███████████████████████████████████---------------| 71.8% Complete\rProgress: |███████████████████████████████████---------------| 71.8% Complete\rProgress: |███████████████████████████████████---------------| 71.9% Complete\rProgress: |███████████████████████████████████---------------| 71.9% Complete\rProgress: |███████████████████████████████████---------------| 72.0% Complete\rProgress: |████████████████████████████████████--------------| 72.1% Complete\rProgress: |████████████████████████████████████--------------| 72.1% Complete\rProgress: |████████████████████████████████████--------------| 72.2% Complete\rProgress: |████████████████████████████████████--------------| 72.2% Complete\rProgress: |████████████████████████████████████--------------| 72.3% Complete\rProgress: |████████████████████████████████████--------------| 72.3% Complete\rProgress: |████████████████████████████████████--------------| 72.4% Complete\rProgress: |████████████████████████████████████--------------| 72.5% Complete\rProgress: |████████████████████████████████████--------------| 72.5% Complete\rProgress: |████████████████████████████████████--------------| 72.6% Complete\rProgress: |████████████████████████████████████--------------| 72.6% Complete\rProgress: |████████████████████████████████████--------------| 72.7% Complete\rProgress: |████████████████████████████████████--------------| 72.8% Complete\rProgress: |████████████████████████████████████--------------| 72.8% Complete\rProgress: |████████████████████████████████████--------------| 72.9% Complete\rProgress: |████████████████████████████████████--------------| 72.9% Complete\rProgress: |████████████████████████████████████--------------| 73.0% Complete\rProgress: |████████████████████████████████████--------------| 73.0% Complete\rProgress: |████████████████████████████████████--------------| 73.1% Complete\rProgress: |████████████████████████████████████--------------| 73.2% Complete\rProgress: |████████████████████████████████████--------------| 73.2% Complete\rProgress: |████████████████████████████████████--------------| 73.3% Complete\rProgress: |████████████████████████████████████--------------| 73.3% Complete\rProgress: |████████████████████████████████████--------------| 73.4% Complete\rProgress: |████████████████████████████████████--------------| 73.5% Complete\rProgress: |████████████████████████████████████--------------| 73.5% Complete\rProgress: |████████████████████████████████████--------------| 73.6% Complete\rProgress: |████████████████████████████████████--------------| 73.6% Complete\rProgress: |████████████████████████████████████--------------| 73.7% Complete\rProgress: |████████████████████████████████████--------------| 73.7% Complete\rProgress: |████████████████████████████████████--------------| 73.8% Complete\rProgress: |████████████████████████████████████--------------| 73.9% Complete\rProgress: |████████████████████████████████████--------------| 73.9% Complete\rProgress: |████████████████████████████████████--------------| 74.0% Complete\rProgress: |█████████████████████████████████████-------------| 74.0% Complete\rProgress: |█████████████████████████████████████-------------| 74.1% Complete\rProgress: |█████████████████████████████████████-------------| 74.2% Complete\rProgress: |█████████████████████████████████████-------------| 74.2% Complete\rProgress: |█████████████████████████████████████-------------| 74.3% Complete\rProgress: |█████████████████████████████████████-------------| 74.3% Complete\rProgress: |█████████████████████████████████████-------------| 74.4% Complete\rProgress: |█████████████████████████████████████-------------| 74.4% Complete\rProgress: |█████████████████████████████████████-------------| 74.5% Complete\rProgress: |█████████████████████████████████████-------------| 74.6% Complete\rProgress: |█████████████████████████████████████-------------| 74.6% Complete\rProgress: |█████████████████████████████████████-------------| 74.7% Complete\rProgress: |█████████████████████████████████████-------------| 74.7% Complete\rProgress: |█████████████████████████████████████-------------| 74.8% Complete\rProgress: |█████████████████████████████████████-------------| 74.9% Complete\rProgress: |█████████████████████████████████████-------------| 74.9% Complete\rProgress: |█████████████████████████████████████-------------| 75.0% Complete\rProgress: |█████████████████████████████████████-------------| 75.0% Complete\rProgress: |█████████████████████████████████████-------------| 75.1% Complete\rProgress: |█████████████████████████████████████-------------| 75.1% Complete\rProgress: |█████████████████████████████████████-------------| 75.2% Complete\rProgress: |█████████████████████████████████████-------------| 75.3% Complete\rProgress: |█████████████████████████████████████-------------| 75.3% Complete\rProgress: |█████████████████████████████████████-------------| 75.4% Complete\rProgress: |█████████████████████████████████████-------------| 75.4% Complete\rProgress: |█████████████████████████████████████-------------| 75.5% Complete\rProgress: |█████████████████████████████████████-------------| 75.6% Complete\rProgress: |█████████████████████████████████████-------------| 75.6% Complete\rProgress: |█████████████████████████████████████-------------| 75.7% Complete\rProgress: |█████████████████████████████████████-------------| 75.7% Complete\rProgress: |█████████████████████████████████████-------------| 75.8% Complete\rProgress: |█████████████████████████████████████-------------| 75.8% Complete\rProgress: |█████████████████████████████████████-------------| 75.9% Complete\rProgress: |█████████████████████████████████████-------------| 76.0% Complete\rProgress: |██████████████████████████████████████------------| 76.0% Complete\rProgress: |██████████████████████████████████████------------| 76.1% Complete\rProgress: |██████████████████████████████████████------------| 76.1% Complete\rProgress: |██████████████████████████████████████------------| 76.2% Complete\rProgress: |██████████████████████████████████████------------| 76.3% Complete\rProgress: |██████████████████████████████████████------------| 76.3% Complete\rProgress: |██████████████████████████████████████------------| 76.4% Complete\rProgress: |██████████████████████████████████████------------| 76.4% Complete\rProgress: |██████████████████████████████████████------------| 76.5% Complete\rProgress: |██████████████████████████████████████------------| 76.5% Complete\rProgress: |██████████████████████████████████████------------| 76.6% Complete\rProgress: |██████████████████████████████████████------------| 76.7% Complete\rProgress: |██████████████████████████████████████------------| 76.7% Complete\rProgress: |██████████████████████████████████████------------| 76.8% Complete\rProgress: |██████████████████████████████████████------------| 76.8% Complete\rProgress: |██████████████████████████████████████------------| 76.9% Complete\rProgress: |██████████████████████████████████████------------| 77.0% Complete\rProgress: |██████████████████████████████████████------------| 77.0% Complete\rProgress: |██████████████████████████████████████------------| 77.1% Complete\rProgress: |██████████████████████████████████████------------| 77.1% Complete\rProgress: |██████████████████████████████████████------------| 77.2% Complete\rProgress: |██████████████████████████████████████------------| 77.2% Complete\rProgress: |██████████████████████████████████████------------| 77.3% Complete\rProgress: |██████████████████████████████████████------------| 77.4% Complete\rProgress: |██████████████████████████████████████------------| 77.4% Complete\rProgress: |██████████████████████████████████████------------| 77.5% Complete\rProgress: |██████████████████████████████████████------------| 77.5% Complete\rProgress: |██████████████████████████████████████------------| 77.6% Complete\rProgress: |██████████████████████████████████████------------| 77.7% Complete\rProgress: |██████████████████████████████████████------------| 77.7% Complete\rProgress: |██████████████████████████████████████------------| 77.8% Complete\rProgress: |██████████████████████████████████████------------| 77.8% Complete\rProgress: |██████████████████████████████████████------------| 77.9% Complete\rProgress: |██████████████████████████████████████------------| 77.9% Complete\rProgress: |███████████████████████████████████████-----------| 78.0% Complete\rProgress: |███████████████████████████████████████-----------| 78.1% Complete\rProgress: |███████████████████████████████████████-----------| 78.1% Complete\rProgress: |███████████████████████████████████████-----------| 78.2% Complete\rProgress: |███████████████████████████████████████-----------| 78.2% Complete\rProgress: |███████████████████████████████████████-----------| 78.3% Complete\rProgress: |███████████████████████████████████████-----------| 78.4% Complete\rProgress: |███████████████████████████████████████-----------| 78.4% Complete\rProgress: |███████████████████████████████████████-----------| 78.5% Complete\rProgress: |███████████████████████████████████████-----------| 78.5% Complete\rProgress: |███████████████████████████████████████-----------| 78.6% Complete\rProgress: |███████████████████████████████████████-----------| 78.6% Complete\rProgress: |███████████████████████████████████████-----------| 78.7% Complete\rProgress: |███████████████████████████████████████-----------| 78.8% Complete\rProgress: |███████████████████████████████████████-----------| 78.8% Complete\rProgress: |███████████████████████████████████████-----------| 78.9% Complete\rProgress: |███████████████████████████████████████-----------| 78.9% Complete\rProgress: |███████████████████████████████████████-----------| 79.0% Complete\rProgress: |███████████████████████████████████████-----------| 79.1% Complete\rProgress: |███████████████████████████████████████-----------| 79.1% Complete\rProgress: |███████████████████████████████████████-----------| 79.2% Complete\rProgress: |███████████████████████████████████████-----------| 79.2% Complete\rProgress: |███████████████████████████████████████-----------| 79.3% Complete\rProgress: |███████████████████████████████████████-----------| 79.3% Complete\rProgress: |███████████████████████████████████████-----------| 79.4% Complete\rProgress: |███████████████████████████████████████-----------| 79.5% Complete\rProgress: |███████████████████████████████████████-----------| 79.5% Complete\rProgress: |███████████████████████████████████████-----------| 79.6% Complete\rProgress: |███████████████████████████████████████-----------| 79.6% Complete\rProgress: |███████████████████████████████████████-----------| 79.7% Complete\rProgress: |███████████████████████████████████████-----------| 79.8% Complete\rProgress: |███████████████████████████████████████-----------| 79.8% Complete\rProgress: |███████████████████████████████████████-----------| 79.9% Complete\rProgress: |███████████████████████████████████████-----------| 79.9% Complete\rProgress: |███████████████████████████████████████-----------| 80.0% Complete\rProgress: |████████████████████████████████████████----------| 80.0% Complete\rProgress: |████████████████████████████████████████----------| 80.1% Complete\rProgress: |████████████████████████████████████████----------| 80.2% Complete\rProgress: |████████████████████████████████████████----------| 80.2% Complete\rProgress: |████████████████████████████████████████----------| 80.3% Complete\rProgress: |████████████████████████████████████████----------| 80.3% Complete\rProgress: |████████████████████████████████████████----------| 80.4% Complete\rProgress: |████████████████████████████████████████----------| 80.5% Complete\rProgress: |████████████████████████████████████████----------| 80.5% Complete\rProgress: |████████████████████████████████████████----------| 80.6% Complete\rProgress: |████████████████████████████████████████----------| 80.6% Complete\rProgress: |████████████████████████████████████████----------| 80.7% Complete\rProgress: |████████████████████████████████████████----------| 80.7% Complete\rProgress: |████████████████████████████████████████----------| 80.8% Complete\rProgress: |████████████████████████████████████████----------| 80.9% Complete\rProgress: |████████████████████████████████████████----------| 80.9% Complete\rProgress: |████████████████████████████████████████----------| 81.0% Complete\rProgress: |████████████████████████████████████████----------| 81.0% Complete\rProgress: |████████████████████████████████████████----------| 81.1% Complete\rProgress: |████████████████████████████████████████----------| 81.2% Complete\rProgress: |████████████████████████████████████████----------| 81.2% Complete\rProgress: |████████████████████████████████████████----------| 81.3% Complete\rProgress: |████████████████████████████████████████----------| 81.3% Complete\rProgress: |████████████████████████████████████████----------| 81.4% Complete\rProgress: |████████████████████████████████████████----------| 81.4% Complete\rProgress: |████████████████████████████████████████----------| 81.5% Complete\rProgress: |████████████████████████████████████████----------| 81.6% Complete\rProgress: |████████████████████████████████████████----------| 81.6% Complete\rProgress: |████████████████████████████████████████----------| 81.7% Complete\rProgress: |████████████████████████████████████████----------| 81.7% Complete\rProgress: |████████████████████████████████████████----------| 81.8% Complete\rProgress: |████████████████████████████████████████----------| 81.9% Complete\rProgress: |████████████████████████████████████████----------| 81.9% Complete\rProgress: |████████████████████████████████████████----------| 82.0% Complete\rProgress: |█████████████████████████████████████████---------| 82.0% Complete\rProgress: |█████████████████████████████████████████---------| 82.1% Complete\rProgress: |█████████████████████████████████████████---------| 82.1% Complete\rProgress: |█████████████████████████████████████████---------| 82.2% Complete\rProgress: |█████████████████████████████████████████---------| 82.3% Complete\rProgress: |█████████████████████████████████████████---------| 82.3% Complete\rProgress: |█████████████████████████████████████████---------| 82.4% Complete\rProgress: |█████████████████████████████████████████---------| 82.4% Complete\rProgress: |█████████████████████████████████████████---------| 82.5% Complete\rProgress: |█████████████████████████████████████████---------| 82.6% Complete\rProgress: |█████████████████████████████████████████---------| 82.6% Complete\rProgress: |█████████████████████████████████████████---------| 82.7% Complete\rProgress: |█████████████████████████████████████████---------| 82.7% Complete\rProgress: |█████████████████████████████████████████---------| 82.8% Complete\rProgress: |█████████████████████████████████████████---------| 82.8% Complete\rProgress: |█████████████████████████████████████████---------| 82.9% Complete\rProgress: |█████████████████████████████████████████---------| 83.0% Complete\rProgress: |█████████████████████████████████████████---------| 83.0% Complete\rProgress: |█████████████████████████████████████████---------| 83.1% Complete\rProgress: |█████████████████████████████████████████---------| 83.1% Complete\rProgress: |█████████████████████████████████████████---------| 83.2% Complete\rProgress: |█████████████████████████████████████████---------| 83.3% Complete\rProgress: |█████████████████████████████████████████---------| 83.3% Complete\rProgress: |█████████████████████████████████████████---------| 83.4% Complete\rProgress: |█████████████████████████████████████████---------| 83.4% Complete\rProgress: |█████████████████████████████████████████---------| 83.5% Complete\rProgress: |█████████████████████████████████████████---------| 83.5% Complete\rProgress: |█████████████████████████████████████████---------| 83.6% Complete\rProgress: |█████████████████████████████████████████---------| 83.7% Complete\rProgress: |█████████████████████████████████████████---------| 83.7% Complete\rProgress: |█████████████████████████████████████████---------| 83.8% Complete\rProgress: |█████████████████████████████████████████---------| 83.8% Complete\rProgress: |█████████████████████████████████████████---------| 83.9% Complete\rProgress: |█████████████████████████████████████████---------| 84.0% Complete\rProgress: |██████████████████████████████████████████--------| 84.0% Complete\rProgress: |██████████████████████████████████████████--------| 84.1% Complete\rProgress: |██████████████████████████████████████████--------| 84.1% Complete\rProgress: |██████████████████████████████████████████--------| 84.2% Complete\rProgress: |██████████████████████████████████████████--------| 84.2% Complete\rProgress: |██████████████████████████████████████████--------| 84.3% Complete\rProgress: |██████████████████████████████████████████--------| 84.4% Complete\rProgress: |██████████████████████████████████████████--------| 84.4% Complete\rProgress: |██████████████████████████████████████████--------| 84.5% Complete\rProgress: |██████████████████████████████████████████--------| 84.5% Complete\rProgress: |██████████████████████████████████████████--------| 84.6% Complete\rProgress: |██████████████████████████████████████████--------| 84.7% Complete\rProgress: |██████████████████████████████████████████--------| 84.7% Complete\rProgress: |██████████████████████████████████████████--------| 84.8% Complete\rProgress: |██████████████████████████████████████████--------| 84.8% Complete\rProgress: |██████████████████████████████████████████--------| 84.9% Complete\rProgress: |██████████████████████████████████████████--------| 84.9% Complete\rProgress: |██████████████████████████████████████████--------| 85.0% Complete\rProgress: |██████████████████████████████████████████--------| 85.1% Complete\rProgress: |██████████████████████████████████████████--------| 85.1% Complete\rProgress: |██████████████████████████████████████████--------| 85.2% Complete\rProgress: |██████████████████████████████████████████--------| 85.2% Complete\rProgress: |██████████████████████████████████████████--------| 85.3% Complete\rProgress: |██████████████████████████████████████████--------| 85.4% Complete\rProgress: |██████████████████████████████████████████--------| 85.4% Complete\rProgress: |██████████████████████████████████████████--------| 85.5% Complete\rProgress: |██████████████████████████████████████████--------| 85.5% Complete\rProgress: |██████████████████████████████████████████--------| 85.6% Complete\rProgress: |██████████████████████████████████████████--------| 85.6% Complete\rProgress: |██████████████████████████████████████████--------| 85.7% Complete\rProgress: |██████████████████████████████████████████--------| 85.8% Complete\rProgress: |██████████████████████████████████████████--------| 85.8% Complete\rProgress: |██████████████████████████████████████████--------| 85.9% Complete\rProgress: |██████████████████████████████████████████--------| 85.9% Complete\rProgress: |██████████████████████████████████████████--------| 86.0% Complete\rProgress: |███████████████████████████████████████████-------| 86.1% Complete\rProgress: |███████████████████████████████████████████-------| 86.1% Complete\rProgress: |███████████████████████████████████████████-------| 86.2% Complete\rProgress: |███████████████████████████████████████████-------| 86.2% Complete\rProgress: |███████████████████████████████████████████-------| 86.3% Complete\rProgress: |███████████████████████████████████████████-------| 86.3% Complete\rProgress: |███████████████████████████████████████████-------| 86.4% Complete\rProgress: |███████████████████████████████████████████-------| 86.5% Complete\rProgress: |███████████████████████████████████████████-------| 86.5% Complete\rProgress: |███████████████████████████████████████████-------| 86.6% Complete\rProgress: |███████████████████████████████████████████-------| 86.6% Complete\rProgress: |███████████████████████████████████████████-------| 86.7% Complete\rProgress: |███████████████████████████████████████████-------| 86.8% Complete\rProgress: |███████████████████████████████████████████-------| 86.8% Complete\rProgress: |███████████████████████████████████████████-------| 86.9% Complete\rProgress: |███████████████████████████████████████████-------| 86.9% Complete\rProgress: |███████████████████████████████████████████-------| 87.0% Complete\rProgress: |███████████████████████████████████████████-------| 87.0% Complete\rProgress: |███████████████████████████████████████████-------| 87.1% Complete\rProgress: |███████████████████████████████████████████-------| 87.2% Complete\rProgress: |███████████████████████████████████████████-------| 87.2% Complete\rProgress: |███████████████████████████████████████████-------| 87.3% Complete\rProgress: |███████████████████████████████████████████-------| 87.3% Complete\rProgress: |███████████████████████████████████████████-------| 87.4% Complete\rProgress: |███████████████████████████████████████████-------| 87.5% Complete\rProgress: |███████████████████████████████████████████-------| 87.5% Complete\rProgress: |███████████████████████████████████████████-------| 87.6% Complete\rProgress: |███████████████████████████████████████████-------| 87.6% Complete\rProgress: |███████████████████████████████████████████-------| 87.7% Complete\rProgress: |███████████████████████████████████████████-------| 87.7% Complete\rProgress: |███████████████████████████████████████████-------| 87.8% Complete\rProgress: |███████████████████████████████████████████-------| 87.9% Complete\rProgress: |███████████████████████████████████████████-------| 87.9% Complete\rProgress: |███████████████████████████████████████████-------| 88.0% Complete\rProgress: |████████████████████████████████████████████------| 88.0% Complete\rProgress: |████████████████████████████████████████████------| 88.1% Complete\rProgress: |████████████████████████████████████████████------| 88.2% Complete\rProgress: |████████████████████████████████████████████------| 88.2% Complete\rProgress: |████████████████████████████████████████████------| 88.3% Complete\rProgress: |████████████████████████████████████████████------| 88.3% Complete\rProgress: |████████████████████████████████████████████------| 88.4% Complete\rProgress: |████████████████████████████████████████████------| 88.4% Complete\rProgress: |████████████████████████████████████████████------| 88.5% Complete\rProgress: |████████████████████████████████████████████------| 88.6% Complete\rProgress: |████████████████████████████████████████████------| 88.6% Complete\rProgress: |████████████████████████████████████████████------| 88.7% Complete\rProgress: |████████████████████████████████████████████------| 88.7% Complete\rProgress: |████████████████████████████████████████████------| 88.8% Complete\rProgress: |████████████████████████████████████████████------| 88.9% Complete\rProgress: |████████████████████████████████████████████------| 88.9% Complete\rProgress: |████████████████████████████████████████████------| 89.0% Complete\rProgress: |████████████████████████████████████████████------| 89.0% Complete\rProgress: |████████████████████████████████████████████------| 89.1% Complete\rProgress: |████████████████████████████████████████████------| 89.1% Complete\rProgress: |████████████████████████████████████████████------| 89.2% Complete\rProgress: |████████████████████████████████████████████------| 89.3% Complete\rProgress: |████████████████████████████████████████████------| 89.3% Complete\rProgress: |████████████████████████████████████████████------| 89.4% Complete\rProgress: |████████████████████████████████████████████------| 89.4% Complete\rProgress: |████████████████████████████████████████████------| 89.5% Complete\rProgress: |████████████████████████████████████████████------| 89.6% Complete\rProgress: |████████████████████████████████████████████------| 89.6% Complete\rProgress: |████████████████████████████████████████████------| 89.7% Complete\rProgress: |████████████████████████████████████████████------| 89.7% Complete\rProgress: |████████████████████████████████████████████------| 89.8% Complete\rProgress: |████████████████████████████████████████████------| 89.8% Complete\rProgress: |████████████████████████████████████████████------| 89.9% Complete\rProgress: |████████████████████████████████████████████------| 90.0% Complete\rProgress: |█████████████████████████████████████████████-----| 90.0% Complete\rProgress: |█████████████████████████████████████████████-----| 90.1% Complete\rProgress: |█████████████████████████████████████████████-----| 90.1% Complete\rProgress: |█████████████████████████████████████████████-----| 90.2% Complete\rProgress: |█████████████████████████████████████████████-----| 90.3% Complete\rProgress: |█████████████████████████████████████████████-----| 90.3% Complete\rProgress: |█████████████████████████████████████████████-----| 90.4% Complete\rProgress: |█████████████████████████████████████████████-----| 90.4% Complete\rProgress: |█████████████████████████████████████████████-----| 90.5% Complete\rProgress: |█████████████████████████████████████████████-----| 90.5% Complete\rProgress: |█████████████████████████████████████████████-----| 90.6% Complete\rProgress: |█████████████████████████████████████████████-----| 90.7% Complete\rProgress: |█████████████████████████████████████████████-----| 90.7% Complete\rProgress: |█████████████████████████████████████████████-----| 90.8% Complete\rProgress: |█████████████████████████████████████████████-----| 90.8% Complete\rProgress: |█████████████████████████████████████████████-----| 90.9% Complete\rProgress: |█████████████████████████████████████████████-----| 91.0% Complete\rProgress: |█████████████████████████████████████████████-----| 91.0% Complete\rProgress: |█████████████████████████████████████████████-----| 91.1% Complete\rProgress: |█████████████████████████████████████████████-----| 91.1% Complete\rProgress: |█████████████████████████████████████████████-----| 91.2% Complete\rProgress: |█████████████████████████████████████████████-----| 91.2% Complete\rProgress: |█████████████████████████████████████████████-----| 91.3% Complete\rProgress: |█████████████████████████████████████████████-----| 91.4% Complete\rProgress: |█████████████████████████████████████████████-----| 91.4% Complete\rProgress: |█████████████████████████████████████████████-----| 91.5% Complete\rProgress: |█████████████████████████████████████████████-----| 91.5% Complete\rProgress: |█████████████████████████████████████████████-----| 91.6% Complete\rProgress: |█████████████████████████████████████████████-----| 91.7% Complete\rProgress: |█████████████████████████████████████████████-----| 91.7% Complete\rProgress: |█████████████████████████████████████████████-----| 91.8% Complete\rProgress: |█████████████████████████████████████████████-----| 91.8% Complete\rProgress: |█████████████████████████████████████████████-----| 91.9% Complete\rProgress: |█████████████████████████████████████████████-----| 91.9% Complete\rProgress: |██████████████████████████████████████████████----| 92.0% Complete\rProgress: |██████████████████████████████████████████████----| 92.1% Complete\rProgress: |██████████████████████████████████████████████----| 92.1% Complete\rProgress: |██████████████████████████████████████████████----| 92.2% Complete\rProgress: |██████████████████████████████████████████████----| 92.2% Complete\rProgress: |██████████████████████████████████████████████----| 92.3% Complete\rProgress: |██████████████████████████████████████████████----| 92.4% Complete\rProgress: |██████████████████████████████████████████████----| 92.4% Complete\rProgress: |██████████████████████████████████████████████----| 92.5% Complete\rProgress: |██████████████████████████████████████████████----| 92.5% Complete\rProgress: |██████████████████████████████████████████████----| 92.6% Complete\rProgress: |██████████████████████████████████████████████----| 92.6% Complete\rProgress: |██████████████████████████████████████████████----| 92.7% Complete\rProgress: |██████████████████████████████████████████████----| 92.8% Complete\rProgress: |██████████████████████████████████████████████----| 92.8% Complete\rProgress: |██████████████████████████████████████████████----| 92.9% Complete\rProgress: |██████████████████████████████████████████████----| 92.9% Complete\rProgress: |██████████████████████████████████████████████----| 93.0% Complete\rProgress: |██████████████████████████████████████████████----| 93.1% Complete\rProgress: |██████████████████████████████████████████████----| 93.1% Complete\rProgress: |██████████████████████████████████████████████----| 93.2% Complete\rProgress: |██████████████████████████████████████████████----| 93.2% Complete\rProgress: |██████████████████████████████████████████████----| 93.3% Complete\rProgress: |██████████████████████████████████████████████----| 93.3% Complete\rProgress: |██████████████████████████████████████████████----| 93.4% Complete\rProgress: |██████████████████████████████████████████████----| 93.5% Complete\rProgress: |██████████████████████████████████████████████----| 93.5% Complete\rProgress: |██████████████████████████████████████████████----| 93.6% Complete\rProgress: |██████████████████████████████████████████████----| 93.6% Complete\rProgress: |██████████████████████████████████████████████----| 93.7% Complete\rProgress: |██████████████████████████████████████████████----| 93.8% Complete\rProgress: |██████████████████████████████████████████████----| 93.8% Complete\rProgress: |██████████████████████████████████████████████----| 93.9% Complete\rProgress: |██████████████████████████████████████████████----| 93.9% Complete\rProgress: |██████████████████████████████████████████████----| 94.0% Complete\rProgress: |███████████████████████████████████████████████---| 94.0% Complete\rProgress: |███████████████████████████████████████████████---| 94.1% Complete\rProgress: |███████████████████████████████████████████████---| 94.2% Complete\rProgress: |███████████████████████████████████████████████---| 94.2% Complete\rProgress: |███████████████████████████████████████████████---| 94.3% Complete\rProgress: |███████████████████████████████████████████████---| 94.3% Complete\rProgress: |███████████████████████████████████████████████---| 94.4% Complete\rProgress: |███████████████████████████████████████████████---| 94.5% Complete\rProgress: |███████████████████████████████████████████████---| 94.5% Complete\rProgress: |███████████████████████████████████████████████---| 94.6% Complete\rProgress: |███████████████████████████████████████████████---| 94.6% Complete\rProgress: |███████████████████████████████████████████████---| 94.7% Complete\rProgress: |███████████████████████████████████████████████---| 94.8% Complete\rProgress: |███████████████████████████████████████████████---| 94.8% Complete\rProgress: |███████████████████████████████████████████████---| 94.9% Complete\rProgress: |███████████████████████████████████████████████---| 94.9% Complete\rProgress: |███████████████████████████████████████████████---| 95.0% Complete\rProgress: |███████████████████████████████████████████████---| 95.0% Complete\rProgress: |███████████████████████████████████████████████---| 95.1% Complete\rProgress: |███████████████████████████████████████████████---| 95.2% Complete\rProgress: |███████████████████████████████████████████████---| 95.2% Complete\rProgress: |███████████████████████████████████████████████---| 95.3% Complete\rProgress: |███████████████████████████████████████████████---| 95.3% Complete\rProgress: |███████████████████████████████████████████████---| 95.4% Complete\rProgress: |███████████████████████████████████████████████---| 95.5% Complete\rProgress: |███████████████████████████████████████████████---| 95.5% Complete\rProgress: |███████████████████████████████████████████████---| 95.6% Complete\rProgress: |███████████████████████████████████████████████---| 95.6% Complete\rProgress: |███████████████████████████████████████████████---| 95.7% Complete\rProgress: |███████████████████████████████████████████████---| 95.7% Complete\rProgress: |███████████████████████████████████████████████---| 95.8% Complete\rProgress: |███████████████████████████████████████████████---| 95.9% Complete\rProgress: |███████████████████████████████████████████████---| 95.9% Complete\rProgress: |███████████████████████████████████████████████---| 96.0% Complete\rProgress: |████████████████████████████████████████████████--| 96.0% Complete\rProgress: |████████████████████████████████████████████████--| 96.1% Complete\rProgress: |████████████████████████████████████████████████--| 96.2% Complete\rProgress: |████████████████████████████████████████████████--| 96.2% Complete\rProgress: |████████████████████████████████████████████████--| 96.3% Complete\rProgress: |████████████████████████████████████████████████--| 96.3% Complete\rProgress: |████████████████████████████████████████████████--| 96.4% Complete\rProgress: |████████████████████████████████████████████████--| 96.4% Complete\rProgress: |████████████████████████████████████████████████--| 96.5% Complete\rProgress: |████████████████████████████████████████████████--| 96.6% Complete\rProgress: |████████████████████████████████████████████████--| 96.6% Complete\rProgress: |████████████████████████████████████████████████--| 96.7% Complete\rProgress: |████████████████████████████████████████████████--| 96.7% Complete\rProgress: |████████████████████████████████████████████████--| 96.8% Complete\rProgress: |████████████████████████████████████████████████--| 96.9% Complete\rProgress: |████████████████████████████████████████████████--| 96.9% Complete\rProgress: |████████████████████████████████████████████████--| 97.0% Complete\rProgress: |████████████████████████████████████████████████--| 97.0% Complete\rProgress: |████████████████████████████████████████████████--| 97.1% Complete\rProgress: |████████████████████████████████████████████████--| 97.1% Complete\rProgress: |████████████████████████████████████████████████--| 97.2% Complete\rProgress: |████████████████████████████████████████████████--| 97.3% Complete\rProgress: |████████████████████████████████████████████████--| 97.3% Complete\rProgress: |████████████████████████████████████████████████--| 97.4% Complete\rProgress: |████████████████████████████████████████████████--| 97.4% Complete\rProgress: |████████████████████████████████████████████████--| 97.5% Complete\rProgress: |████████████████████████████████████████████████--| 97.6% Complete\rProgress: |████████████████████████████████████████████████--| 97.6% Complete\rProgress: |████████████████████████████████████████████████--| 97.7% Complete\rProgress: |████████████████████████████████████████████████--| 97.7% Complete\rProgress: |████████████████████████████████████████████████--| 97.8% Complete\rProgress: |████████████████████████████████████████████████--| 97.8% Complete\rProgress: |████████████████████████████████████████████████--| 97.9% Complete\rProgress: |████████████████████████████████████████████████--| 98.0% Complete\rProgress: |█████████████████████████████████████████████████-| 98.0% Complete\rProgress: |█████████████████████████████████████████████████-| 98.1% Complete\rProgress: |█████████████████████████████████████████████████-| 98.1% Complete\rProgress: |█████████████████████████████████████████████████-| 98.2% Complete\rProgress: |█████████████████████████████████████████████████-| 98.3% Complete\rProgress: |█████████████████████████████████████████████████-| 98.3% Complete\rProgress: |█████████████████████████████████████████████████-| 98.4% Complete\rProgress: |█████████████████████████████████████████████████-| 98.4% Complete\rProgress: |█████████████████████████████████████████████████-| 98.5% Complete\rProgress: |█████████████████████████████████████████████████-| 98.5% Complete\rProgress: |█████████████████████████████████████████████████-| 98.6% Complete\rProgress: |█████████████████████████████████████████████████-| 98.7% Complete\rProgress: |█████████████████████████████████████████████████-| 98.7% Complete\rProgress: |█████████████████████████████████████████████████-| 98.8% Complete\rProgress: |█████████████████████████████████████████████████-| 98.8% Complete\rProgress: |█████████████████████████████████████████████████-| 98.9% Complete\rProgress: |█████████████████████████████████████████████████-| 99.0% Complete\rProgress: |█████████████████████████████████████████████████-| 99.0% Complete\rProgress: |█████████████████████████████████████████████████-| 99.1% Complete\rProgress: |█████████████████████████████████████████████████-| 99.1% Complete\rProgress: |█████████████████████████████████████████████████-| 99.2% Complete\rProgress: |█████████████████████████████████████████████████-| 99.2% Complete\rProgress: |█████████████████████████████████████████████████-| 99.3% Complete\rProgress: |█████████████████████████████████████████████████-| 99.4% Complete\rProgress: |█████████████████████████████████████████████████-| 99.4% Complete\rProgress: |█████████████████████████████████████████████████-| 99.5% Complete\rProgress: |█████████████████████████████████████████████████-| 99.5% Complete\rProgress: |█████████████████████████████████████████████████-| 99.6% Complete\rProgress: |█████████████████████████████████████████████████-| 99.7% Complete\rProgress: |█████████████████████████████████████████████████-| 99.7% Complete\rProgress: |█████████████████████████████████████████████████-| 99.8% Complete\rProgress: |█████████████████████████████████████████████████-| 99.8% Complete\rProgress: |█████████████████████████████████████████████████-| 99.9% Complete\rProgress: |█████████████████████████████████████████████████-| 99.9% Complete\rProgress: |██████████████████████████████████████████████████| 100.0% Complete"]}],"source":["import torch\n","import cv2\n","import easyocr\n","from transformers import BertTokenizer, BertModel\n","import torch.nn.functional as F\n","\n","# Define device\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Path to the trained fusion model\n","MODEL_PATH = \"/content/drive/MyDrive/MINI-1/fusion_model.pth\"\n","CLASS_NAMES = [\"Legitimate\", \"Phishing\"]\n","\n","# Model parameters\n","BATCH_SIZE = 16\n","EPOCHS = 5\n","LEARNING_RATE = 0.001\n","GRAD_CLIP = 1.0\n","\n","# Load YOLO model (using pre-trained weights for object detection)\n","yolo_model = cv2.dnn.readNetFromDarknet(\"yolov4.cfg\", \"yolov4.weights\")\n","layer_names = yolo_model.getLayerNames()\n","output_layers = [layer_names[i - 1] for i in yolo_model.getUnconnectedOutLayers()]\n","\n","# Load TinyBERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n","tinybert = BertModel.from_pretrained(\"prajjwal1/bert-tiny\").to(DEVICE)\n","tinybert.eval()\n","\n","# Initialize EasyOCR reader for text extraction\n","reader = easyocr.Reader(['en'])\n","\n","def extract_text_from_image(image_path):\n","    \"\"\"\n","    Extract text from an image using EasyOCR.\n","    \"\"\"\n","    result = reader.readtext(image_path)\n","    extracted_text = \" \".join([detection[1] for detection in result])  # Collect all text detections\n","    return extracted_text\n","\n","def predict_text_class(text):\n","    \"\"\"\n","    Predict the class (Legitimate or Phishing) based on the extracted text using TinyBERT.\n","    \"\"\"\n","    # Tokenize the text input\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    inputs = {key: val.to(DEVICE) for key, val in inputs.items()}\n","\n","    with torch.no_grad():\n","        outputs = tinybert(**inputs)\n","\n","    # Take the output corresponding to the [CLS] token\n","    logits = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n","\n","    # Apply softmax to get probabilities\n","    probs = F.softmax(logits, dim=-1)\n","\n","    # Get predicted class index\n","    predicted_class_index = torch.argmax(probs).item()\n","\n","    # Check if predicted_class_index is within the valid range\n","    predicted_class_index = min(predicted_class_index, len(CLASS_NAMES) - 1)\n","\n","    # Return the class name\n","    return CLASS_NAMES[predicted_class_index], probs[0][predicted_class_index].item()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4634,"status":"ok","timestamp":1747802820872,"user":{"displayName":"228W1A5430_Sec-A Kavuri Kushalava","userId":"01804207717655710695"},"user_tz":-330},"id":"-KJ86ppufIn_","outputId":"c1b5b559-e65e-446b-e3e0-f1fa063ce6ff"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Extracted Text: Coogle Cllck han enahla deskon nomtcationg mal arnMcia Gmail Corrosm Lutosir com GCismarei Inink Faster Train you 6rai7 %t7 Lurosity Ira Perscnal turer Jes Jrs Unbar Q) 5-me0 Confirm account owrership Hmconani Cnats ConalerAtaNi03ol cmaltautotatajmul comz 6 38 AM [24 Mnutes 292 enia Drats Heio Herechror 4huil Sjnt Mnta Since Insichanned Vclnaselard Plees9 conlim account Oanaihig ckur] Dnls Irke Bearch ct 84\"5 ud 5olOoojicom conwmaccounl'eMciznbacchcioriJmoilcom Taar gerur  Ilne 6o:ab c0m ieeinica\n","Prediction: Phishing\n"]}],"source":["image_path = \"/content/drive/MyDrive/MINI-1/test.png\"\n","\n","# Step 1: Extract text from the image\n","extracted_text = extract_text_from_image(image_path)\n","print(f\"Extracted Text: {extracted_text}\")\n","\n","# Step 2: Predict the class (Legitimate or Phishing) based on the extracted text\n","prediction, confidence = predict_text_class(extracted_text)\n","print(f\"Prediction: {prediction}\")"]},{"cell_type":"markdown","metadata":{"id":"sBQ9CawAJxeL"},"source":["**WITH INTERFACE**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":958},"executionInfo":{"elapsed":17792,"status":"ok","timestamp":1747803261772,"user":{"displayName":"228W1A5430_Sec-A Kavuri Kushalava","userId":"01804207717655710695"},"user_tz":-330},"id":"tdnHePaeEt7Y","outputId":"79ba4c59-f212-42c0-b31b-56b440c980f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n","Epoch 1/5:\n","Train Loss: 0.3134, Train Accuracy: 100.00%\n","Val Loss: 0.3133, Val Accuracy: 100.00%\n","Epoch 2/5:\n","Train Loss: 0.3133, Train Accuracy: 100.00%\n","Val Loss: 0.3133, Val Accuracy: 100.00%\n","Epoch 3/5:\n","Train Loss: 0.3133, Train Accuracy: 100.00%\n","Val Loss: 0.3133, Val Accuracy: 100.00%\n","Epoch 4/5:\n","Train Loss: 0.3133, Train Accuracy: 100.00%\n","Val Loss: 0.3133, Val Accuracy: 100.00%\n","Epoch 5/5:\n","Train Loss: 0.3133, Train Accuracy: 100.00%\n","Val Loss: 0.3133, Val Accuracy: 100.00%\n","Trained model saved to /content/drive/MyDrive/MINI-1/fusion_model.pth\n","Launching Gradio UI...\n","It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://830c05fe2f20c2b09d.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://830c05fe2f20c2b09d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from transformers import BertTokenizer, BertModel\n","import easyocr\n","import pytesseract\n","from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont\n","import numpy as np\n","import gradio as gr\n","import os\n","import random\n","import cv2\n","\n","# ======================\n","# Configuration\n","# ======================\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","MODEL_PATH = \"/content/drive/MyDrive/MINI-1/fusion_model.pth\"\n","CLASS_NAMES = [\"Legitimate\", \"Phishing\"]\n","BATCH_SIZE = 16\n","EPOCHS = 5\n","LEARNING_RATE = 0.001\n","GRAD_CLIP = 1.0\n","\n","class PhishingDataset(Dataset):\n","    def __init__(self, num_samples=100, image_size=(224, 224)):\n","        self.num_samples = num_samples\n","        self.image_size = image_size\n","        self.data = []\n","        self.labels = []\n","        self.tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n","\n","        phishing_phrases = [\"urgent\", \"verify\", \"secure\", \"account\", \"compromised\", \"action required\"]\n","        phishing_actions = [\"click\", \"visit\", \"login to\", \"update\"]\n","        domains = [\"secure-login.com\", \"account-verify.com\", \"reset-now.com\", \"safe-access.net\"]\n","        legit_phrases = [\"thank you\", \"order confirmed\", \"meeting scheduled\", \"subscription renewed\"]\n","        legit_actions = [\"contact support\", \"view details\", \"confirm attendance\", \"track order\"]\n","\n","        for i in range(num_samples):\n","            is_phishing = random.choice([0, 1])  # 0: Legitimate, 1: Phishing\n","            if is_phishing:\n","                phrase = random.choice(phishing_phrases)\n","                action = random.choice(phishing_actions)\n","                domain = random.choice(domains)\n","                text = f\"{phrase.title()}: {action} https://{domain} to secure your account.\"\n","            else:\n","                phrase = random.choice(legit_phrases)\n","                action = random.choice(legit_actions)\n","                text = f\"{phrase.title()}. Please {action} for more information.\"\n","\n","            if not isinstance(text, str):\n","                text = \"Default text\"\n","\n","            image = Image.new('RGB', image_size, color=(255, 255, 255))\n","            draw = ImageDraw.Draw(image)\n","            try:\n","                font = ImageFont.truetype(\"arial.ttf\", 20)\n","            except:\n","                font = ImageFont.load_default()\n","            draw.text((10, 10), text, fill=(0, 0, 0), font=font)\n","\n","            self.data.append((image, text))\n","            self.labels.append(is_phishing)\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        image, text = self.data[idx]\n","        label = self.labels[idx]\n","        return image, text, label\n","\n","# ======================\n","# Model Definitions\n","# ======================\n","class FusionModel(nn.Module):\n","    def __init__(self, image_feat_size=1280, text_feat_size=128):\n","        super(FusionModel, self).__init__()\n","        self.fc1 = nn.Linear(image_feat_size + text_feat_size, 256)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(256, 2)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, image_feat, text_feat):\n","        x = torch.cat((image_feat, text_feat), dim=1)\n","        x = self.relu(self.fc1(x))\n","        return self.softmax(self.fc2(x))\n","\n","# ======================\n","# Model Loading\n","# ======================\n","def load_models():\n","    tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n","    tinybert = BertModel.from_pretrained(\"prajjwal1/bert-tiny\").to(DEVICE)\n","    tinybert.eval()\n","\n","    efficientnet = models.efficientnet_b0(weights=\"IMAGENET1K_V1\").to(DEVICE)\n","    efficientnet.classifier = nn.Identity()\n","    efficientnet.eval()\n","\n","    fusion_model = FusionModel().to(DEVICE)\n","    if os.path.exists(MODEL_PATH):\n","        fusion_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","    else:\n","        print(\"Warning: No pretrained fusion model found. Training required.\")\n","    fusion_model.train()\n","\n","    reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n","\n","    # Load YOLO model\n","    yolo_model = cv2.dnn.readNetFromDarknet(\"yolov4.cfg\", \"yolov4.weights\")\n","    layer_names = yolo_model.getLayerNames()\n","    output_layers = [layer_names[i - 1] for i in yolo_model.getUnconnectedOutLayers()]\n","\n","    return tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers\n","\n","# ======================\n","# Preprocessing\n","# ======================\n","def preprocess_image(images):\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    if isinstance(images, Image.Image):\n","        images = [images]\n","    images = [img.convert('RGB') for img in images]\n","    return torch.stack([transform(img) for img in images]).to(DEVICE)\n","\n","def preprocess_for_ocr(image):\n","    if not isinstance(image, Image.Image):\n","        image = Image.open(image)\n","    image = image.convert('L')\n","    enhancer = ImageEnhance.Contrast(image)\n","    image = enhancer.enhance(2.0)\n","    image = image.filter(ImageFilter.MedianFilter())\n","    return np.array(image)\n","\n","# ======================\n","# Custom Collate Function\n","# ======================\n","def custom_collate_fn(batch):\n","    images, texts, labels = zip(*batch)\n","    image_tensor = preprocess_image(images)\n","    texts = list(texts)\n","    labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n","    return image_tensor, texts, labels\n","\n","# ======================\n","# Feature Extraction\n","# ======================\n","def extract_text_with_yolo(image, yolo_model, output_layers, reader, use_pytesseract_fallback=True):\n","    if isinstance(image, Image.Image):\n","        image_np = np.array(image.convert('RGB'))\n","    else:\n","        image_np = cv2.imread(image)\n","    height, width = image_np.shape[:2]\n","\n","    # Prepare image for YOLO\n","    blob = cv2.dnn.blobFromImage(image_np, 1/255.0, (416, 416), swapRB=True, crop=False)\n","    yolo_model.setInput(blob)\n","    outputs = yolo_model.forward(output_layers)\n","\n","    boxes = []\n","    confidences = []\n","    class_ids = []\n","    for output in outputs:\n","        for detection in output:\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            if confidence > 0.5:  # Confidence threshold\n","                center_x = int(detection[0] * width)\n","                center_y = int(detection[1] * height)\n","                w = int(detection[2] * width)\n","                h = int(detection[3] * height)\n","                x = int(center_x - w / 2)\n","                y = int(center_y - h / 2)\n","                boxes.append([x, y, w, h])\n","                confidences.append(float(confidence))\n","                class_ids.append(class_id)\n","\n","    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","    extracted_text = []\n","\n","    if len(indices) > 0:\n","        for i in indices.flatten():\n","            x, y, w, h = boxes[i]\n","            x = max(0, x)\n","            y = max(0, y)\n","            w = min(w, width - x)\n","            h = min(h, height - y)\n","            if w <= 0 or h <= 0:\n","                continue\n","            roi = image_np[y:y+h, x:x+w]\n","            if roi.size == 0:\n","                continue\n","            roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n","            roi_array = preprocess_for_ocr(roi_pil)\n","            try:\n","                result = reader.readtext(roi_array, detail=0, paragraph=True)\n","                text = \" \".join(result).strip()\n","                if text:\n","                    extracted_text.append(text)\n","                elif use_pytesseract_fallback:\n","                    text = pytesseract.image_to_string(roi_array, config='--psm 6').strip()\n","                    if text:\n","                        extracted_text.append(text)\n","            except Exception as e:\n","                print(f\"Text extraction failed for ROI: {e}\")\n","\n","    final_text = \" \".join(extracted_text).strip() if extracted_text else extract_text_from_image(np.array(image))  # Fallback to full image\n","    return final_text if final_text else \"No text extracted\"\n","\n","def extract_text_from_image(image):\n","    result = reader.readtext(np.array(image))\n","    return \" \".join([detection[1] for detection in result]).strip()\n","\n","def extract_image_features(image_tensor, efficientnet):\n","    with torch.no_grad():\n","        features = efficientnet(image_tensor)\n","    return features.float()\n","\n","def extract_text_features(texts, tokenizer, tinybert):\n","    if isinstance(texts, str):\n","        texts = [texts]\n","    texts = [str(text) if text else \" \" for text in texts]\n","    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n","    with torch.no_grad():\n","        outputs = tinybert(**inputs)\n","    return outputs.last_hidden_state[:, 0, :].float()\n","\n","# ======================\n","# Training\n","# ======================\n","def train_model(fusion_model, efficientnet, tinybert, tokenizer, train_loader, val_loader, epochs=EPOCHS):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE)\n","    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n","\n","    for epoch in range(epochs):\n","        fusion_model.train()\n","        train_loss = 0.0\n","        train_correct = 0\n","        train_total = 0\n","\n","        for image_tensor, texts, labels in train_loader:\n","            optimizer.zero_grad()\n","            image_features = extract_image_features(image_tensor, efficientnet)\n","            text_features = extract_text_features(texts, tokenizer, tinybert)\n","            labels = labels.to(DEVICE)\n","\n","            outputs = fusion_model(image_features, text_features)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(fusion_model.parameters(), GRAD_CLIP)\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            train_total += labels.size(0)\n","            train_correct += (predicted == labels).sum().item()\n","\n","        scheduler.step()\n","        train_accuracy = 100 * train_correct / train_total\n","\n","        fusion_model.eval()\n","        val_loss = 0.0\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for image_tensor, texts, labels in val_loader:\n","                image_features = extract_image_features(image_tensor, efficientnet)\n","                text_features = extract_text_features(texts, tokenizer, tinybert)\n","                labels = labels.to(DEVICE)\n","\n","                outputs = fusion_model(image_features, text_features)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item()\n","                _, predicted = torch.max(outputs, 1)\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","\n","        val_accuracy = 100 * val_correct / val_total\n","\n","        print(f\"Epoch {epoch+1}/{epochs}:\")\n","        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n","        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n","\n","    torch.save(fusion_model.state_dict(), MODEL_PATH)\n","    print(f\"Trained model saved to {MODEL_PATH}\")\n","\n","# ======================\n","# Classification\n","# ======================\n","def classify_input(input_data, is_image, tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers):\n","    if is_image:\n","        image_tensor = preprocess_image([input_data])[0:1]\n","        image_features = extract_image_features(image_tensor, efficientnet)\n","        extracted_text = extract_text_with_yolo(input_data, yolo_model, output_layers, reader)\n","        text_features = extract_text_features([extracted_text], tokenizer, tinybert)\n","        model_used = \"EfficientNet + TinyBERT\"\n","    else:\n","        image_features = torch.zeros((1, 1280)).to(DEVICE)\n","        text_features = extract_text_features([input_data], tokenizer, tinybert)\n","        extracted_text = input_data\n","        model_used = \"TinyBERT\"\n","\n","    with torch.no_grad():\n","        prediction_tensor = fusion_model(image_features, text_features)\n","        predicted_label = torch.argmax(prediction_tensor).item()\n","\n","    prediction = CLASS_NAMES[predicted_label]\n","\n","    return {\n","        \"input_type\": \"image\" if is_image else \"text\",\n","        \"prediction\": prediction,\n","        \"model_used\": model_used,\n","        \"extracted_text\": extracted_text if is_image else None\n","    }\n","\n","# ======================\n","# Gradio Interface\n","# ======================\n","def launch_gradio_interface():\n","    tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers = load_models()\n","\n","    def predict(image, text):\n","        if image is not None:\n","            result = classify_input(image, is_image=True, tokenizer=tokenizer, tinybert=tinybert,\n","                                   efficientnet=efficientnet, fusion_model=fusion_model, reader=reader,\n","                                   yolo_model=yolo_model, output_layers=output_layers)\n","            return (f\"Prediction: {result['prediction']}\\n\"\n","                    f\"Model used: {result['model_used']}\\n\"\n","                    f\"Extracted Text: {result['extracted_text']}\")\n","        elif text:\n","            result = classify_input(text, is_image=False, tokenizer=tokenizer, tinybert=tinybert,\n","                                   efficientnet=efficientnet, fusion_model=fusion_model, reader=reader,\n","                                   yolo_model=yolo_model, output_layers=output_layers)\n","            return f\"Prediction: {result['prediction']}\\nModel used: {result['model_used']}\"\n","        else:\n","            return \"No input provided.\"\n","\n","    with gr.Blocks() as demo:\n","        gr.Markdown(\"## 🛡️ Phishing Detection System\")\n","        with gr.Row():\n","            image_input = gr.Image(label=\"Upload Image\", type=\"pil\")\n","            text_input = gr.Textbox(label=\"Enter Text\", placeholder=\"Type email or message here...\")\n","        with gr.Row():\n","            submit_btn = gr.Button(\"Detect Phishing\")\n","        output = gr.Textbox(label=\"Prediction\", interactive=False)\n","\n","        submit_btn.click(fn=predict, inputs=[image_input, text_input], outputs=output)\n","\n","    demo.launch()\n","\n","# ======================\n","# Main Execution\n","# ======================\n","if __name__ == \"__main__\":\n","    train_dataset = PhishingDataset(num_samples=200)\n","    val_dataset = PhishingDataset(num_samples=50)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n","\n","    tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers = load_models()\n","\n","    print(\"Starting training...\")\n","    train_model(fusion_model, efficientnet, tinybert, tokenizer, train_loader, val_loader)\n","    print(\"Launching Gradio UI...\")\n","    launch_gradio_interface()"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from transformers import BertTokenizer, BertModel\n","import easyocr\n","import pytesseract\n","from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont\n","import numpy as np\n","import gradio as gr\n","import os\n","import random\n","import cv2\n","import uuid\n","\n","# ======================\n","# Configuration\n","# ======================\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","MODEL_PATH = \"/content/drive/MyDrive/MINI-1/fusion_model.pth\"\n","CLASS_NAMES = [\"Legitimate\", \"Phishing\"]\n","BATCH_SIZE = 16\n","EPOCHS = 5\n","LEARNING_RATE = 0.001\n","GRAD_CLIP = 1.0\n","\n","# ======================\n","# Synthetic Dataset\n","# ======================\n","class PhishingDataset(Dataset):\n","    def __init__(self, num_samples=100, image_size=(224, 224)):\n","        self.num_samples = num_samples\n","        self.image_size = image_size\n","        self.data = []\n","        self.labels = []\n","        self.tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n","\n","        phishing_phrases = [\"urgent\", \"verify\", \"secure\", \"account\", \"compromised\", \"action required\"]\n","        phishing_actions = [\"click\", \"visit\", \"login to\", \"update\"]\n","        domains = [\"secure-login.com\", \"account-verify.com\", \"reset-now.com\", \"safe-access.net\"]\n","        legit_phrases = [\"thank you\", \"order confirmed\", \"meeting scheduled\", \"subscription renewed\"]\n","        legit_actions = [\"contact support\", \"view details\", \"confirm attendance\", \"track order\"]\n","\n","        for i in range(num_samples):\n","            is_phishing = random.choice([0, 1])  # 0: Legitimate, 1: Phishing\n","            if is_phishing:\n","                phrase = random.choice(phishing_phrases)\n","                action = random.choice(phishing_actions)\n","                domain = random.choice(domains)\n","                text = f\"{phrase.title()}: {action} https://{domain} to secure your account.\"\n","            else:\n","                phrase = random.choice(legit_phrases)\n","                action = random.choice(legit_actions)\n","                text = f\"{phrase.title()}. Please {action} for more information.\"\n","\n","            if not isinstance(text, str):\n","                text = \"Default text\"\n","\n","            image = Image.new('RGB', image_size, color=(255, 255, 255))\n","            draw = ImageDraw.Draw(image)\n","            try:\n","                font = ImageFont.truetype(\"arial.ttf\", 20)\n","            except:\n","                font = ImageFont.load_default()\n","            draw.text((10, 10), text, fill=(0, 0, 0), font=font)\n","\n","            self.data.append((image, text))\n","            self.labels.append(is_phishing)\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        image, text = self.data[idx]\n","        label = self.labels[idx]\n","        return image, text, label\n","\n","# ======================\n","# Model Definitions\n","# ======================\n","class FusionModel(nn.Module):\n","    def __init__(self, image_feat_size=1280, text_feat_size=128):\n","        super(FusionModel, self).__init__()\n","        self.fc1 = nn.Linear(image_feat_size + text_feat_size, 256)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(256, 2)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, image_feat, text_feat):\n","        x = torch.cat((image_feat, text_feat), dim=1)\n","        x = self.relu(self.fc1(x))\n","        return self.softmax(self.fc2(x))\n","\n","# ======================\n","# Model Loading\n","# ======================\n","def load_models():\n","    tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n","    tinybert = BertModel.from_pretrained(\"prajjwal1/bert-tiny\").to(DEVICE)\n","    tinybert.eval()\n","\n","    efficientnet = models.efficientnet_b0(weights=\"IMAGENET1K_V1\").to(DEVICE)\n","    efficientnet.classifier = nn.Identity()\n","    efficientnet.eval()\n","\n","    # Initialize a new FusionModel instead of loading mismatched state dict\n","    fusion_model = FusionModel(image_feat_size=1280, text_feat_size=128).to(DEVICE)\n","    print(\"Initialized new FusionModel for training.\")\n","\n","    reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n","\n","    # Load YOLO model (ensure yolov4.cfg and yolov4.weights are available)\n","    try:\n","        yolo_model = cv2.dnn.readNetFromDarknet(\"yolov4.cfg\", \"yolov4.weights\")\n","        layer_names = yolo_model.getLayerNames()\n","        output_layers = [layer_names[i - 1] for i in yolo_model.getUnconnectedOutLayers()]\n","    except Exception as e:\n","        print(f\"Error loading YOLO model: {e}\")\n","        raise FileNotFoundError(\"Ensure yolov4.cfg and yolov4.weights are in the working directory.\")\n","\n","    return tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers\n","\n","# ======================\n","# Preprocessing\n","# ======================\n","def preprocess_image(images):\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    if isinstance(images, Image.Image):\n","        images = [images]\n","    images = [img.convert('RGB') for img in images]\n","    return torch.stack([transform(img) for img in images]).to(DEVICE)\n","\n","def preprocess_for_ocr(image):\n","    if not isinstance(image, Image.Image):\n","        image = Image.open(image)\n","    image = image.convert('L')\n","    enhancer = ImageEnhance.Contrast(image)\n","    image = enhancer.enhance(2.0)\n","    image = image.filter(ImageFilter.MedianFilter())\n","    return np.array(image)\n","\n","# ======================\n","# Custom Collate Function\n","# ======================\n","def custom_collate_fn(batch):\n","    images, texts, labels = zip(*batch)\n","    image_tensor = preprocess_image(images)\n","    texts = list(texts)\n","    labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n","    return image_tensor, texts, labels\n","\n","# ======================\n","# Feature Extraction\n","# ======================\n","def extract_text_with_yolo(image, yolo_model, output_layers, reader, use_pytesseract_fallback=True):\n","    if isinstance(image, Image.Image):\n","        image_np = np.array(image.convert('RGB'))\n","    else:\n","        image_np = cv2.imread(image)\n","    height, width = image_np.shape[:2]\n","\n","    # Prepare image for YOLO\n","    blob = cv2.dnn.blobFromImage(image_np, 1/255.0, (416, 416), swapRB=True, crop=False)\n","    yolo_model.setInput(blob)\n","    outputs = yolo_model.forward(output_layers)\n","\n","    boxes = []\n","    confidences = []\n","    class_ids = []\n","    for output in outputs:\n","        for detection in output:\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            if confidence > 0.5:  # Confidence threshold\n","                center_x = int(detection[0] * width)\n","                center_y = int(detection[1] * height)\n","                w = int(detection[2] * width)\n","                h = int(detection[3] * height)\n","                x = int(center_x - w / 2)\n","                y = int(center_y - h / 2)\n","                boxes.append([x, y, w, h])\n","                confidences.append(float(confidence))\n","                class_ids.append(class_id)\n","\n","    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","    extracted_text = []\n","\n","    if len(indices) > 0:\n","        for i in indices.flatten():\n","            x, y, w, h = boxes[i]\n","            x = max(0, x)\n","            y = max(0, y)\n","            w = min(w, width - x)\n","            h = min(h, height - y)\n","            if w <= 0 or h <= 0:\n","                continue\n","            roi = image_np[y:y+h, x:x+w]\n","            if roi.size == 0:\n","                continue\n","            roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n","            roi_array = preprocess_for_ocr(roi_pil)\n","            try:\n","                result = reader.readtext(roi_array, detail=0, paragraph=True)\n","                text = \" \".join(result).strip()\n","                if text:\n","                    extracted_text.append(text)\n","                elif use_pytesseract_fallback:\n","                    text = pytesseract.image_to_string(roi_array, config='--psm 6').strip()\n","                    if text:\n","                        extracted_text.append(text)\n","            except Exception as e:\n","                print(f\"Text extraction failed for ROI: {e}\")\n","\n","    final_text = \" \".join(extracted_text).strip() if extracted_text else extract_text_from_image(np.array(image))  # Fallback to full image\n","    return final_text if final_text else \"No text extracted\"\n","\n","def extract_text_from_image(image):\n","    result = reader.readtext(np.array(image))\n","    return \" \".join([detection[1] for detection in result]).strip()\n","\n","def extract_image_features(image_tensor, efficientnet):\n","    with torch.no_grad():\n","        features = efficientnet(image_tensor)\n","    return features.float()\n","\n","def extract_text_features(texts, tokenizer, tinybert):\n","    if isinstance(texts, str):\n","        texts = [texts]\n","    texts = [str(text) if text else \" \" for text in texts]\n","    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n","    with torch.no_grad():\n","        outputs = tinybert(**inputs)\n","    return outputs.last_hidden_state[:, 0, :].float()\n","\n","# ======================\n","# Training\n","# ======================\n","def train_model(fusion_model, efficientnet, tinybert, tokenizer, train_loader, val_loader, epochs=EPOCHS):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE)\n","    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n","\n","    for epoch in range(epochs):\n","        fusion_model.train()\n","        train_loss = 0.0\n","        train_correct = 0\n","        train_total = 0\n","\n","        for image_tensor, texts, labels in train_loader:\n","            optimizer.zero_grad()\n","            image_features = extract_image_features(image_tensor, efficientnet)\n","            text_features = extract_text_features(texts, tokenizer, tinybert)\n","            labels = labels.to(DEVICE)\n","\n","            outputs = fusion_model(image_features, text_features)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(fusion_model.parameters(), GRAD_CLIP)\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            train_total += labels.size(0)\n","            train_correct += (predicted == labels).sum().item()\n","\n","        scheduler.step()\n","        train_accuracy = 100 * train_correct / train_total\n","\n","        fusion_model.eval()\n","        val_loss = 0.0\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for image_tensor, texts, labels in val_loader:\n","                image_features = extract_image_features(image_tensor, efficientnet)\n","                text_features = extract_text_features(texts, tokenizer, tinybert)\n","                labels = labels.to(DEVICE)\n","\n","                outputs = fusion_model(image_features, text_features)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item()\n","                _, predicted = torch.max(outputs, 1)\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","\n","        val_accuracy = 100 * val_correct / val_total\n","\n","        print(f\"Epoch {epoch+1}/{epochs}:\")\n","        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n","        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n","\n","    # Save the trained model\n","    torch.save(fusion_model.state_dict(), MODEL_PATH)\n","    print(f\"Trained model saved to {MODEL_PATH}\")\n","\n","# ======================\n","# Classification\n","# ======================\n","def classify_input(input_data, is_image, tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers):\n","    fusion_model.eval()  # Ensure model is in eval mode for inference\n","    if is_image:\n","        image_tensor = preprocess_image([input_data])[0:1]\n","        image_features = extract_image_features(image_tensor, efficientnet)\n","        extracted_text = extract_text_with_yolo(input_data, yolo_model, output_layers, reader)\n","        text_features = extract_text_features([extracted_text], tokenizer, tinybert)\n","        model_used = \"YOLO + EfficientNet + TinyBERT\"\n","    else:\n","        image_features = torch.zeros((1, 1280)).to(DEVICE)\n","        text_features = extract_text_features([input_data], tokenizer, tinybert)\n","        extracted_text = input_data\n","        model_used = \"TinyBERT\"\n","\n","    with torch.no_grad():\n","        prediction_tensor = fusion_model(image_features, text_features)\n","        predicted_label = torch.argmax(prediction_tensor).item()\n","\n","    prediction = CLASS_NAMES[predicted_label]\n","\n","    return {\n","        \"input_type\": \"image\" if is_image else \"text\",\n","        \"prediction\": prediction,\n","        \"model_used\": model_used,\n","        \"extracted_text\": extracted_text if is_image else None\n","    }\n","\n","# ======================\n","# Gradio Interface\n","# ======================\n","def launch_gradio_interface():\n","    tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers = load_models()\n","\n","    def predict(image, text):\n","        if image is not None:\n","            result = classify_input(image, is_image=True, tokenizer=tokenizer, tinybert=tinybert,\n","                                   efficientnet=efficientnet, fusion_model=fusion_model, reader=reader,\n","                                   yolo_model=yolo_model, output_layers=output_layers)\n","            return (f\"Prediction: {result['prediction']}\\n\"\n","                    f\"Model used: {result['model_used']}\\n\"\n","                    f\"Extracted Text: {result['extracted_text']}\")\n","        elif text:\n","            result = classify_input(text, is_image=False, tokenizer=tokenizer, tinybert=tinybert,\n","                                   efficientnet=efficientnet, fusion_model=fusion_model, reader=reader,\n","                                   yolo_model=yolo_model, output_layers=output_layers)\n","            return f\"Prediction: {result['prediction']}\\nModel used: {result['model_used']}\"\n","        else:\n","            return \"No input provided.\"\n","\n","    with gr.Blocks() as demo:\n","        gr.Markdown(\"## 🛡️ Phishing Detection System\")\n","        with gr.Row():\n","            image_input = gr.Image(label=\"Upload Image\", type=\"pil\")\n","            text_input = gr.Textbox(label=\"Enter Text\", placeholder=\"Type email or message here...\")\n","        with gr.Row():\n","            submit_btn = gr.Button(\"Detect Phishing\")\n","        output = gr.Textbox(label=\"Prediction\", interactive=False)\n","\n","        submit_btn.click(fn=predict, inputs=[image_input, text_input], outputs=output)\n","\n","    demo.launch()\n","\n","# ======================\n","# Main Execution\n","# ======================\n","if __name__ == \"__main__\":\n","    train_dataset = PhishingDataset(num_samples=200)\n","    val_dataset = PhishingDataset(num_samples=50)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n","\n","    tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers = load_models()\n","\n","    print(\"Starting training...\")\n","    train_model(fusion_model, efficientnet, tinybert, tokenizer, train_loader, val_loader)\n","    print(\"Launching Gradio UI...\")\n","    launch_gradio_interface()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":993},"id":"PeQQwc4BLoiO","executionInfo":{"status":"ok","timestamp":1747803104309,"user_tz":-330,"elapsed":21233,"user":{"displayName":"228W1A5430_Sec-A Kavuri Kushalava","userId":"01804207717655710695"}},"outputId":"92b36783-91e0-4200-ebe2-086e8813f016"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized new FusionModel for training.\n","Starting training...\n","Epoch 1/5:\n","Train Loss: 0.5311, Train Accuracy: 84.00%\n","Val Loss: 0.3812, Val Accuracy: 100.00%\n","Epoch 2/5:\n","Train Loss: 0.3468, Train Accuracy: 100.00%\n","Val Loss: 0.3240, Val Accuracy: 100.00%\n","Epoch 3/5:\n","Train Loss: 0.3230, Train Accuracy: 100.00%\n","Val Loss: 0.3218, Val Accuracy: 100.00%\n","Epoch 4/5:\n","Train Loss: 0.3213, Train Accuracy: 100.00%\n","Val Loss: 0.3207, Val Accuracy: 100.00%\n","Epoch 5/5:\n","Train Loss: 0.3207, Train Accuracy: 100.00%\n","Val Loss: 0.3206, Val Accuracy: 100.00%\n","Trained model saved to /content/drive/MyDrive/MINI-1/fusion_model.pth\n","Launching Gradio UI...\n","Initialized new FusionModel for training.\n","It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://bf908e5c96ee7c75e0.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://bf908e5c96ee7c75e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"5xdmnhZghyV1","outputId":"c6063d5d-152f-4642-b0cf-35d27a6701bf"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Failed to load FusionModel checkpoint due to mismatch: Error(s) in loading state_dict for FusionModel:\n","\tsize mismatch for fc1.weight: copying a param with shape torch.Size([256, 1408]) from checkpoint, the shape in current model is torch.Size([256, 2048]).. Initializing new model.\n"]},{"name":"stdout","output_type":"stream","text":["Starting training...\n","Epoch 1, Batch 1: Loss=1.2353, Grad Norm=17.7004\n","Epoch 1, Batch 2: Loss=1.1135, Grad Norm=17.0362\n","Epoch 1, Batch 3: Loss=0.7246, Grad Norm=12.0095\n","Epoch 1, Batch 4: Loss=0.9018, Grad Norm=15.0696\n","Epoch 1, Batch 5: Loss=0.2636, Grad Norm=7.6425\n","Epoch 1, Batch 6: Loss=0.8490, Grad Norm=13.4782\n","Epoch 1, Batch 7: Loss=0.4338, Grad Norm=11.5275\n","Epoch 1, Batch 8: Loss=0.2956, Grad Norm=7.0975\n","Epoch 1, Batch 9: Loss=0.0524, Grad Norm=1.6823\n","Epoch 1, Batch 10: Loss=0.1755, Grad Norm=5.6281\n","Epoch 1, Batch 11: Loss=0.0615, Grad Norm=2.3639\n","Epoch 1, Batch 12: Loss=0.1396, Grad Norm=4.6013\n","Epoch 1, Batch 13: Loss=0.0140, Grad Norm=0.6491\n","Epoch 1, Batch 14: Loss=0.0035, Grad Norm=0.2041\n","Epoch 1, Batch 15: Loss=0.1107, Grad Norm=3.0699\n","Epoch 1, Batch 16: Loss=0.1074, Grad Norm=3.5226\n","Epoch 1, Batch 17: Loss=0.0368, Grad Norm=1.6432\n","Epoch 1, Batch 18: Loss=0.0064, Grad Norm=0.1726\n","Epoch 1, Batch 19: Loss=0.1034, Grad Norm=2.9685\n","Epoch 1, Batch 20: Loss=0.0625, Grad Norm=3.5827\n","Epoch 1, Batch 21: Loss=0.0538, Grad Norm=2.4267\n","Epoch 1, Batch 22: Loss=0.1091, Grad Norm=5.5146\n","Epoch 1, Batch 23: Loss=0.2878, Grad Norm=7.6788\n","Epoch 1, Batch 24: Loss=0.0107, Grad Norm=0.4464\n","Epoch 1, Batch 25: Loss=0.4170, Grad Norm=8.6548\n","Epoch 1, Batch 26: Loss=0.1081, Grad Norm=2.7081\n","Epoch 1, Batch 27: Loss=0.0117, Grad Norm=0.5422\n","Epoch 1, Batch 28: Loss=0.0208, Grad Norm=0.6241\n","Epoch 1, Batch 29: Loss=0.0700, Grad Norm=1.9634\n","Epoch 1, Batch 30: Loss=0.0720, Grad Norm=2.7729\n","Epoch 1, Batch 31: Loss=0.0554, Grad Norm=2.6308\n","Epoch 1, Batch 32: Loss=0.1112, Grad Norm=2.9943\n","Epoch 1, Batch 33: Loss=0.0111, Grad Norm=0.4617\n","Epoch 1, Batch 34: Loss=0.0256, Grad Norm=0.7298\n","Epoch 1, Batch 35: Loss=0.0323, Grad Norm=1.0549\n","Epoch 1, Batch 36: Loss=0.1409, Grad Norm=4.1213\n","Epoch 1, Batch 37: Loss=0.0631, Grad Norm=2.7656\n","Epoch 1, Batch 38: Loss=0.0219, Grad Norm=0.8808\n","Epoch 1, Batch 39: Loss=0.0175, Grad Norm=0.7181\n","Epoch 1, Batch 40: Loss=0.0214, Grad Norm=0.6305\n","Epoch 1, Batch 41: Loss=0.1402, Grad Norm=3.7870\n","Epoch 1, Batch 42: Loss=0.2487, Grad Norm=4.2985\n","Epoch 1, Batch 43: Loss=0.0706, Grad Norm=3.3852\n","Epoch 1, Batch 44: Loss=0.0190, Grad Norm=0.6618\n","Epoch 1, Batch 45: Loss=0.0126, Grad Norm=0.5398\n","Epoch 1, Batch 46: Loss=0.0024, Grad Norm=0.0758\n","Epoch 1, Batch 47: Loss=0.0796, Grad Norm=4.5677\n","Epoch 1, Batch 48: Loss=0.0345, Grad Norm=0.9464\n","Epoch 1, Batch 49: Loss=0.0060, Grad Norm=0.2337\n","Epoch 1, Batch 50: Loss=0.0402, Grad Norm=1.9033\n","Epoch 1, Batch 51: Loss=0.0923, Grad Norm=4.0283\n","Epoch 1, Batch 52: Loss=0.2309, Grad Norm=4.4928\n","Epoch 1, Batch 53: Loss=0.0082, Grad Norm=0.2794\n","Epoch 1, Batch 54: Loss=0.0556, Grad Norm=2.5790\n","Epoch 1, Batch 55: Loss=0.0013, Grad Norm=0.0491\n","Epoch 1, Batch 56: Loss=0.0026, Grad Norm=0.1122\n","Epoch 1, Batch 57: Loss=0.0327, Grad Norm=1.3394\n","Epoch 1, Batch 58: Loss=0.0048, Grad Norm=0.1718\n","Epoch 1, Batch 59: Loss=0.0060, Grad Norm=0.2027\n","Epoch 1, Batch 60: Loss=0.0471, Grad Norm=1.9746\n","Epoch 1, Batch 61: Loss=0.2178, Grad Norm=4.3063\n","Epoch 1, Batch 62: Loss=0.0211, Grad Norm=1.1563\n","Epoch 1, Batch 63: Loss=0.0038, Grad Norm=0.2682\n","Epoch 1, Batch 64: Loss=0.0139, Grad Norm=0.5355\n","Epoch 1, Batch 65: Loss=0.4462, Grad Norm=6.7522\n","Epoch 1, Batch 66: Loss=0.0036, Grad Norm=0.1270\n","Epoch 1, Batch 67: Loss=0.0119, Grad Norm=0.4501\n","Epoch 1, Batch 68: Loss=0.0459, Grad Norm=1.6690\n","Epoch 1, Batch 69: Loss=0.0407, Grad Norm=2.0883\n","Epoch 1, Batch 70: Loss=0.0066, Grad Norm=0.2135\n","Epoch 1, Batch 71: Loss=0.4039, Grad Norm=6.8720\n","Epoch 1, Batch 72: Loss=0.2226, Grad Norm=5.0388\n","Epoch 1, Batch 73: Loss=0.0050, Grad Norm=0.2361\n","Epoch 1, Batch 74: Loss=0.0710, Grad Norm=2.6622\n","Epoch 1, Batch 75: Loss=0.0109, Grad Norm=0.3530\n","Epoch 1, Batch 76: Loss=0.0358, Grad Norm=1.2012\n","Epoch 1, Batch 77: Loss=0.2267, Grad Norm=4.8348\n","Epoch 1, Batch 78: Loss=0.0145, Grad Norm=0.4733\n","Epoch 1, Batch 79: Loss=0.0002, Grad Norm=0.0099\n","Epoch 1, Batch 80: Loss=0.0199, Grad Norm=0.8465\n","Epoch 1, Batch 81: Loss=0.0012, Grad Norm=0.0415\n","Epoch 1, Batch 82: Loss=0.0157, Grad Norm=0.5612\n","Epoch 1, Batch 83: Loss=0.1741, Grad Norm=4.3550\n","Epoch 1, Batch 84: Loss=0.0766, Grad Norm=4.6651\n","Epoch 1, Batch 85: Loss=0.0009, Grad Norm=0.0408\n","Epoch 1, Batch 86: Loss=0.0095, Grad Norm=0.5894\n","Epoch 1, Batch 87: Loss=0.0045, Grad Norm=0.1980\n","Epoch 1, Batch 88: Loss=0.0180, Grad Norm=1.4791\n","Epoch 1, Batch 89: Loss=0.0013, Grad Norm=0.0337\n","Epoch 1, Batch 90: Loss=0.0008, Grad Norm=0.0239\n","Epoch 1, Batch 91: Loss=0.0029, Grad Norm=0.1394\n","Epoch 1, Batch 92: Loss=0.0268, Grad Norm=1.0358\n","Epoch 1, Batch 93: Loss=0.0336, Grad Norm=1.0141\n","Epoch 1, Batch 94: Loss=0.0023, Grad Norm=0.1298\n","Epoch 1, Batch 95: Loss=0.0338, Grad Norm=1.2474\n","Epoch 1, Batch 96: Loss=0.0011, Grad Norm=0.0372\n","Epoch 1, Batch 97: Loss=0.0012, Grad Norm=0.0539\n","Epoch 1, Batch 98: Loss=0.0076, Grad Norm=0.2883\n","Epoch 1, Batch 99: Loss=0.0729, Grad Norm=3.0025\n","Epoch 1, Batch 100: Loss=0.0288, Grad Norm=1.0070\n","Epoch 1, Batch 101: Loss=0.0138, Grad Norm=0.6730\n","Epoch 1, Batch 102: Loss=0.0699, Grad Norm=2.9609\n","Epoch 1, Batch 103: Loss=0.0301, Grad Norm=1.5847\n","Epoch 1, Batch 104: Loss=0.0118, Grad Norm=0.5060\n","Epoch 1, Batch 105: Loss=0.0041, Grad Norm=0.1715\n","Epoch 1, Batch 106: Loss=0.0070, Grad Norm=0.2403\n","Epoch 1, Batch 107: Loss=0.0054, Grad Norm=0.2446\n","Epoch 1, Batch 108: Loss=0.0007, Grad Norm=0.0220\n","Epoch 1, Batch 109: Loss=0.0015, Grad Norm=0.0640\n","Epoch 1, Batch 110: Loss=0.0205, Grad Norm=1.1696\n","Epoch 1, Batch 111: Loss=0.0654, Grad Norm=3.1456\n","Epoch 1, Batch 112: Loss=0.2736, Grad Norm=4.6506\n","Epoch 1, Batch 113: Loss=0.0036, Grad Norm=0.1751\n","Epoch 1, Batch 114: Loss=0.0328, Grad Norm=1.5718\n","Epoch 1, Batch 115: Loss=0.1113, Grad Norm=3.4238\n","Epoch 1, Batch 116: Loss=0.1166, Grad Norm=3.3735\n","Epoch 1, Batch 117: Loss=0.0192, Grad Norm=0.7747\n","Epoch 1, Batch 118: Loss=0.2710, Grad Norm=6.8711\n","Epoch 1, Batch 119: Loss=0.0031, Grad Norm=0.1165\n","Epoch 1, Batch 120: Loss=0.0055, Grad Norm=0.2271\n","Epoch 1, Batch 121: Loss=0.2049, Grad Norm=6.2494\n","Epoch 1, Batch 122: Loss=0.0017, Grad Norm=0.0872\n","Epoch 1, Batch 123: Loss=0.0020, Grad Norm=0.0679\n","Epoch 1, Batch 124: Loss=0.0029, Grad Norm=0.1727\n","Epoch 1, Batch 125: Loss=0.0954, Grad Norm=3.1028\n","Epoch 1, Batch 126: Loss=0.0003, Grad Norm=0.0110\n","Epoch 1, Batch 127: Loss=0.0006, Grad Norm=0.0180\n","Epoch 1, Batch 128: Loss=0.0010, Grad Norm=0.0429\n","Epoch 1, Batch 129: Loss=0.0030, Grad Norm=0.3054\n","Epoch 1, Batch 130: Loss=0.0382, Grad Norm=1.4592\n","Epoch 1, Batch 131: Loss=0.0048, Grad Norm=0.2460\n","Epoch 1, Batch 132: Loss=0.0060, Grad Norm=0.2676\n","Epoch 1, Batch 133: Loss=0.0091, Grad Norm=0.4294\n","Epoch 1, Batch 134: Loss=0.1535, Grad Norm=4.7962\n","Epoch 1, Batch 135: Loss=0.0865, Grad Norm=3.5228\n","Epoch 1, Batch 136: Loss=0.0067, Grad Norm=0.1931\n","Epoch 1, Batch 137: Loss=0.0077, Grad Norm=0.5674\n","Epoch 1, Batch 138: Loss=0.1773, Grad Norm=6.1629\n","Epoch 1, Batch 139: Loss=0.0026, Grad Norm=0.0773\n","Epoch 1, Batch 140: Loss=0.0003, Grad Norm=0.0107\n","Epoch 1, Batch 141: Loss=0.2228, Grad Norm=5.7970\n","Epoch 1, Batch 142: Loss=0.0072, Grad Norm=0.2696\n","Epoch 1, Batch 143: Loss=0.0012, Grad Norm=0.0561\n","Epoch 1, Batch 144: Loss=0.0255, Grad Norm=1.8158\n","Epoch 1, Batch 145: Loss=0.0146, Grad Norm=0.6009\n","Epoch 1, Batch 146: Loss=0.0034, Grad Norm=0.1125\n","Epoch 1, Batch 147: Loss=0.0035, Grad Norm=0.1723\n","Epoch 1, Batch 148: Loss=0.0004, Grad Norm=0.0126\n","Epoch 1, Batch 149: Loss=0.0053, Grad Norm=0.2705\n","Epoch 1, Batch 150: Loss=0.0111, Grad Norm=0.4753\n","Epoch 1, Batch 151: Loss=0.0005, Grad Norm=0.0222\n","Epoch 1, Batch 152: Loss=0.0009, Grad Norm=0.0274\n","Epoch 1, Batch 153: Loss=0.0032, Grad Norm=0.1367\n","Epoch 1, Batch 154: Loss=0.0065, Grad Norm=0.2810\n","Epoch 1, Batch 155: Loss=0.0100, Grad Norm=0.3555\n","Epoch 1, Batch 156: Loss=0.0387, Grad Norm=1.3119\n","Epoch 1, Batch 157: Loss=0.0477, Grad Norm=1.8682\n","Epoch 1, Batch 158: Loss=0.2779, Grad Norm=5.1486\n","Epoch 1, Batch 159: Loss=0.0090, Grad Norm=0.5250\n","Epoch 1, Batch 160: Loss=0.0095, Grad Norm=0.3720\n","Epoch 1, Batch 161: Loss=0.0036, Grad Norm=0.2214\n","Epoch 1, Batch 162: Loss=0.0004, Grad Norm=0.0129\n","Epoch 1, Batch 163: Loss=0.0029, Grad Norm=0.1251\n","Epoch 1, Batch 164: Loss=0.0220, Grad Norm=0.6168\n","Epoch 1, Batch 165: Loss=0.5900, Grad Norm=7.9421\n","Epoch 1, Batch 166: Loss=0.0085, Grad Norm=0.5600\n","Epoch 1, Batch 167: Loss=0.0043, Grad Norm=0.1731\n","Epoch 1, Batch 168: Loss=0.0006, Grad Norm=0.0219\n","Epoch 1, Batch 169: Loss=0.0015, Grad Norm=0.0434\n","Epoch 1, Batch 170: Loss=0.0001, Grad Norm=0.0022\n","Epoch 1, Batch 171: Loss=0.0620, Grad Norm=1.1771\n","Epoch 1, Batch 172: Loss=0.0020, Grad Norm=0.0707\n","Epoch 1, Batch 173: Loss=0.2453, Grad Norm=6.0055\n","Epoch 1, Batch 174: Loss=0.2406, Grad Norm=3.9312\n","Epoch 1, Batch 175: Loss=0.0001, Grad Norm=0.0037\n","Epoch 1, Batch 176: Loss=0.0643, Grad Norm=2.3872\n","Epoch 1, Batch 177: Loss=0.0124, Grad Norm=0.5416\n","Epoch 1, Batch 178: Loss=0.2000, Grad Norm=4.1875\n","Epoch 1, Batch 179: Loss=0.0020, Grad Norm=0.0615\n","Epoch 1, Batch 180: Loss=0.0080, Grad Norm=0.3286\n","Epoch 1, Batch 181: Loss=0.0013, Grad Norm=0.0446\n","Epoch 1, Batch 182: Loss=0.0094, Grad Norm=0.4225\n","Epoch 1, Batch 183: Loss=0.0337, Grad Norm=1.1602\n","Epoch 1, Batch 184: Loss=0.0087, Grad Norm=0.3159\n","Epoch 1, Batch 185: Loss=0.0141, Grad Norm=0.9556\n","Epoch 1, Batch 186: Loss=0.0303, Grad Norm=0.9848\n","Epoch 1, Batch 187: Loss=0.0012, Grad Norm=0.0463\n","Epoch 1, Batch 188: Loss=0.0019, Grad Norm=0.1101\n","Epoch 1/15:\n","Train Loss: 0.0849, Train Accuracy: 96.53%\n","Val Loss: 0.0187, Val Accuracy: 99.33%\n","Model saved (improved val loss: 0.0187)\n","Epoch 2, Batch 1: Loss=0.0024, Grad Norm=0.0760\n","Epoch 2, Batch 2: Loss=0.0062, Grad Norm=0.2493\n","Epoch 2, Batch 3: Loss=0.0065, Grad Norm=0.4083\n","Epoch 2, Batch 4: Loss=0.0210, Grad Norm=1.0436\n","Epoch 2, Batch 5: Loss=0.0085, Grad Norm=0.4292\n","Epoch 2, Batch 6: Loss=0.0010, Grad Norm=0.0277\n","Epoch 2, Batch 7: Loss=0.0061, Grad Norm=0.2346\n","Epoch 2, Batch 8: Loss=0.1042, Grad Norm=3.2088\n","Epoch 2, Batch 9: Loss=0.0127, Grad Norm=0.4204\n","Epoch 2, Batch 10: Loss=0.0004, Grad Norm=0.0127\n","Epoch 2, Batch 11: Loss=0.0003, Grad Norm=0.0072\n","Epoch 2, Batch 12: Loss=0.0007, Grad Norm=0.0265\n","Epoch 2, Batch 13: Loss=0.0004, Grad Norm=0.0116\n","Epoch 2, Batch 14: Loss=0.0245, Grad Norm=1.3067\n","Epoch 2, Batch 15: Loss=0.0001, Grad Norm=0.0030\n","Epoch 2, Batch 16: Loss=0.0033, Grad Norm=0.2409\n","Epoch 2, Batch 17: Loss=0.0074, Grad Norm=0.2978\n","Epoch 2, Batch 18: Loss=0.0025, Grad Norm=0.1356\n","Epoch 2, Batch 19: Loss=0.0722, Grad Norm=3.0546\n","Epoch 2, Batch 20: Loss=0.0028, Grad Norm=0.1245\n","Epoch 2, Batch 21: Loss=0.0274, Grad Norm=1.2525\n","Epoch 2, Batch 22: Loss=0.0010, Grad Norm=0.0403\n","Epoch 2, Batch 23: Loss=0.0004, Grad Norm=0.0158\n","Epoch 2, Batch 24: Loss=0.0440, Grad Norm=2.1267\n","Epoch 2, Batch 25: Loss=0.0001, Grad Norm=0.0041\n","Epoch 2, Batch 26: Loss=0.0028, Grad Norm=0.1726\n","Epoch 2, Batch 27: Loss=0.0016, Grad Norm=0.0871\n","Epoch 2, Batch 28: Loss=0.0045, Grad Norm=0.1897\n","Epoch 2, Batch 29: Loss=0.0044, Grad Norm=0.1857\n","Epoch 2, Batch 30: Loss=0.0004, Grad Norm=0.0158\n","Epoch 2, Batch 31: Loss=0.0004, Grad Norm=0.0129\n","Epoch 2, Batch 32: Loss=0.0001, Grad Norm=0.0057\n","Epoch 2, Batch 33: Loss=0.0024, Grad Norm=0.1212\n","Epoch 2, Batch 34: Loss=0.0008, Grad Norm=0.0301\n","Epoch 2, Batch 35: Loss=0.0047, Grad Norm=0.3270\n","Epoch 2, Batch 36: Loss=0.0281, Grad Norm=0.9687\n","Epoch 2, Batch 37: Loss=0.0001, Grad Norm=0.0024\n","Epoch 2, Batch 38: Loss=0.3991, Grad Norm=6.6410\n","Epoch 2, Batch 39: Loss=0.0003, Grad Norm=0.0084\n","Epoch 2, Batch 40: Loss=0.0011, Grad Norm=0.0485\n","Epoch 2, Batch 41: Loss=0.0016, Grad Norm=0.0620\n","Epoch 2, Batch 42: Loss=0.0013, Grad Norm=0.0487\n","Epoch 2, Batch 43: Loss=0.0040, Grad Norm=0.1616\n","Epoch 2, Batch 44: Loss=0.1036, Grad Norm=2.8211\n","Epoch 2, Batch 45: Loss=0.0011, Grad Norm=0.0355\n","Epoch 2, Batch 46: Loss=0.0044, Grad Norm=0.3348\n","Epoch 2, Batch 47: Loss=0.0102, Grad Norm=0.4086\n","Epoch 2, Batch 48: Loss=0.0085, Grad Norm=0.3501\n","Epoch 2, Batch 49: Loss=0.0006, Grad Norm=0.0163\n","Epoch 2, Batch 50: Loss=0.2584, Grad Norm=5.0446\n","Epoch 2, Batch 51: Loss=0.0001, Grad Norm=0.0035\n","Epoch 2, Batch 52: Loss=0.0824, Grad Norm=2.3278\n","Epoch 2, Batch 53: Loss=0.0029, Grad Norm=0.1303\n","Epoch 2, Batch 54: Loss=0.1552, Grad Norm=4.4144\n","Epoch 2, Batch 55: Loss=0.0001, Grad Norm=0.0041\n","Epoch 2, Batch 56: Loss=0.0004, Grad Norm=0.0196\n","Epoch 2, Batch 57: Loss=0.0002, Grad Norm=0.0049\n","Epoch 2, Batch 58: Loss=0.0193, Grad Norm=0.7172\n","Epoch 2, Batch 59: Loss=0.0235, Grad Norm=1.5114\n","Epoch 2, Batch 60: Loss=0.0012, Grad Norm=0.0403\n","Epoch 2, Batch 61: Loss=0.0201, Grad Norm=1.2926\n","Epoch 2, Batch 62: Loss=0.0201, Grad Norm=0.8419\n","Epoch 2, Batch 63: Loss=0.0003, Grad Norm=0.0144\n","Epoch 2, Batch 64: Loss=0.0371, Grad Norm=1.9283\n","Epoch 2, Batch 65: Loss=0.0002, Grad Norm=0.0079\n","Epoch 2, Batch 66: Loss=0.0085, Grad Norm=0.3931\n","Epoch 2, Batch 67: Loss=0.0001, Grad Norm=0.0016\n","Epoch 2, Batch 68: Loss=0.0003, Grad Norm=0.0115\n","Epoch 2, Batch 69: Loss=0.0010, Grad Norm=0.0377\n","Epoch 2, Batch 70: Loss=0.0128, Grad Norm=0.6410\n","Epoch 2, Batch 71: Loss=0.0013, Grad Norm=0.0516\n","Epoch 2, Batch 72: Loss=0.0047, Grad Norm=0.2118\n","Epoch 2, Batch 73: Loss=0.2356, Grad Norm=5.0224\n","Epoch 2, Batch 74: Loss=0.0141, Grad Norm=0.7876\n","Epoch 2, Batch 75: Loss=0.0043, Grad Norm=0.1366\n","Epoch 2, Batch 76: Loss=0.0002, Grad Norm=0.0086\n","Epoch 2, Batch 77: Loss=0.0002, Grad Norm=0.0064\n","Epoch 2, Batch 78: Loss=0.0012, Grad Norm=0.0662\n","Epoch 2, Batch 79: Loss=0.0005, Grad Norm=0.0280\n","Epoch 2, Batch 80: Loss=0.0006, Grad Norm=0.0218\n","Epoch 2, Batch 81: Loss=0.0020, Grad Norm=0.0855\n","Epoch 2, Batch 82: Loss=0.0067, Grad Norm=0.4253\n","Epoch 2, Batch 83: Loss=0.0016, Grad Norm=0.0712\n","Epoch 2, Batch 84: Loss=0.0012, Grad Norm=0.0381\n","Epoch 2, Batch 85: Loss=0.0000, Grad Norm=0.0019\n","Epoch 2, Batch 86: Loss=0.0134, Grad Norm=0.6839\n","Epoch 2, Batch 87: Loss=0.0018, Grad Norm=0.0816\n","Epoch 2, Batch 88: Loss=0.0012, Grad Norm=0.0478\n","Epoch 2, Batch 89: Loss=0.0007, Grad Norm=0.0381\n","Epoch 2, Batch 90: Loss=0.0876, Grad Norm=3.1535\n","Epoch 2, Batch 91: Loss=0.0012, Grad Norm=0.0410\n","Epoch 2, Batch 92: Loss=0.0102, Grad Norm=0.5285\n","Epoch 2, Batch 93: Loss=0.0006, Grad Norm=0.0189\n","Epoch 2, Batch 94: Loss=0.0005, Grad Norm=0.0176\n","Epoch 2, Batch 95: Loss=0.0013, Grad Norm=0.0607\n","Epoch 2, Batch 96: Loss=0.0025, Grad Norm=0.1040\n","Epoch 2, Batch 97: Loss=0.0009, Grad Norm=0.0345\n","Epoch 2, Batch 98: Loss=0.0024, Grad Norm=0.0858\n","Epoch 2, Batch 99: Loss=0.0022, Grad Norm=0.0773\n","Epoch 2, Batch 100: Loss=0.0009, Grad Norm=0.0456\n","Epoch 2, Batch 101: Loss=0.0013, Grad Norm=0.0643\n","Epoch 2, Batch 102: Loss=0.0007, Grad Norm=0.0218\n","Epoch 2, Batch 103: Loss=0.1075, Grad Norm=3.1844\n","Epoch 2, Batch 104: Loss=0.2620, Grad Norm=4.6202\n","Epoch 2, Batch 105: Loss=0.0003, Grad Norm=0.0137\n","Epoch 2, Batch 106: Loss=0.0011, Grad Norm=0.0638\n","Epoch 2, Batch 107: Loss=0.0000, Grad Norm=0.0015\n","Epoch 2, Batch 108: Loss=0.0005, Grad Norm=0.0232\n","Epoch 2, Batch 109: Loss=0.0004, Grad Norm=0.0162\n","Epoch 2, Batch 110: Loss=0.0002, Grad Norm=0.0063\n","Epoch 2, Batch 111: Loss=0.0008, Grad Norm=0.0366\n","Epoch 2, Batch 112: Loss=0.0004, Grad Norm=0.0159\n","Epoch 2, Batch 113: Loss=0.1103, Grad Norm=2.4208\n","Epoch 2, Batch 114: Loss=0.0004, Grad Norm=0.0149\n","Epoch 2, Batch 115: Loss=0.0277, Grad Norm=1.2908\n","Epoch 2, Batch 116: Loss=0.0095, Grad Norm=0.3308\n","Epoch 2, Batch 117: Loss=0.0001, Grad Norm=0.0027\n","Epoch 2, Batch 118: Loss=0.0029, Grad Norm=0.1073\n","Epoch 2, Batch 119: Loss=0.0042, Grad Norm=0.2160\n","Epoch 2, Batch 120: Loss=0.0437, Grad Norm=1.6919\n","Epoch 2, Batch 121: Loss=0.0005, Grad Norm=0.0161\n","Epoch 2, Batch 122: Loss=0.0001, Grad Norm=0.0035\n","Epoch 2, Batch 123: Loss=0.0016, Grad Norm=0.0767\n","Epoch 2, Batch 124: Loss=0.1853, Grad Norm=3.9127\n","Epoch 2, Batch 125: Loss=0.0018, Grad Norm=0.0992\n","Epoch 2, Batch 126: Loss=0.0001, Grad Norm=0.0029\n","Epoch 2, Batch 127: Loss=0.0061, Grad Norm=0.3554\n","Epoch 2, Batch 128: Loss=0.0007, Grad Norm=0.0223\n","Epoch 2, Batch 129: Loss=0.0001, Grad Norm=0.0023\n","Epoch 2, Batch 130: Loss=0.0127, Grad Norm=0.5198\n","Epoch 2, Batch 131: Loss=0.0003, Grad Norm=0.0078\n","Epoch 2, Batch 132: Loss=0.0011, Grad Norm=0.0491\n","Epoch 2, Batch 133: Loss=0.0012, Grad Norm=0.0545\n","Epoch 2, Batch 134: Loss=0.0001, Grad Norm=0.0040\n","Epoch 2, Batch 135: Loss=0.0001, Grad Norm=0.0038\n","Epoch 2, Batch 136: Loss=0.0006, Grad Norm=0.0249\n","Epoch 2, Batch 137: Loss=0.0079, Grad Norm=0.3700\n","Epoch 2, Batch 138: Loss=0.0003, Grad Norm=0.0081\n","Epoch 2, Batch 139: Loss=0.0002, Grad Norm=0.0104\n","Epoch 2, Batch 140: Loss=0.1493, Grad Norm=3.4835\n","Epoch 2, Batch 141: Loss=0.0125, Grad Norm=0.5492\n","Epoch 2, Batch 142: Loss=0.0506, Grad Norm=1.5226\n","Epoch 2, Batch 143: Loss=0.0002, Grad Norm=0.0102\n","Epoch 2, Batch 144: Loss=0.0031, Grad Norm=0.2421\n","Epoch 2, Batch 145: Loss=0.0011, Grad Norm=0.0511\n","Epoch 2, Batch 146: Loss=0.0002, Grad Norm=0.0078\n","Epoch 2, Batch 147: Loss=0.0004, Grad Norm=0.0091\n","Epoch 2, Batch 148: Loss=0.0008, Grad Norm=0.0352\n","Epoch 2, Batch 149: Loss=0.0014, Grad Norm=0.0456\n","Epoch 2, Batch 150: Loss=0.0017, Grad Norm=0.0759\n","Epoch 2, Batch 151: Loss=0.0013, Grad Norm=0.0605\n","Epoch 2, Batch 152: Loss=0.0078, Grad Norm=0.4391\n","Epoch 2, Batch 153: Loss=0.0188, Grad Norm=1.2181\n","Epoch 2, Batch 154: Loss=0.0001, Grad Norm=0.0018\n","Epoch 2, Batch 155: Loss=0.0001, Grad Norm=0.0057\n","Epoch 2, Batch 156: Loss=0.3039, Grad Norm=5.0966\n","Epoch 2, Batch 157: Loss=0.0968, Grad Norm=3.7263\n","Epoch 2, Batch 158: Loss=0.0001, Grad Norm=0.0024\n","Epoch 2, Batch 159: Loss=0.0001, Grad Norm=0.0044\n","Epoch 2, Batch 160: Loss=0.1212, Grad Norm=3.0305\n","Epoch 2, Batch 161: Loss=0.0003, Grad Norm=0.0136\n","Epoch 2, Batch 162: Loss=0.0007, Grad Norm=0.0347\n","Epoch 2, Batch 163: Loss=0.0003, Grad Norm=0.0096\n","Epoch 2, Batch 164: Loss=0.0021, Grad Norm=0.1198\n","Epoch 2, Batch 165: Loss=0.0014, Grad Norm=0.0424\n","Epoch 2, Batch 166: Loss=0.0080, Grad Norm=0.4419\n","Epoch 2, Batch 167: Loss=0.0003, Grad Norm=0.0092\n","Epoch 2, Batch 168: Loss=0.0232, Grad Norm=0.9172\n","Epoch 2, Batch 169: Loss=0.0078, Grad Norm=0.2995\n","Epoch 2, Batch 170: Loss=0.0061, Grad Norm=0.2091\n","Epoch 2, Batch 171: Loss=0.0013, Grad Norm=0.0336\n","Epoch 2, Batch 172: Loss=0.0010, Grad Norm=0.0380\n","Epoch 2, Batch 173: Loss=0.0086, Grad Norm=0.4128\n","Epoch 2, Batch 174: Loss=0.4522, Grad Norm=6.1131\n","Epoch 2, Batch 175: Loss=0.0075, Grad Norm=0.2810\n","Epoch 2, Batch 176: Loss=0.0260, Grad Norm=1.7493\n","Epoch 2, Batch 177: Loss=0.0002, Grad Norm=0.0053\n","Epoch 2, Batch 178: Loss=0.0260, Grad Norm=0.9198\n","Epoch 2, Batch 179: Loss=0.0012, Grad Norm=0.0471\n","Epoch 2, Batch 180: Loss=0.0012, Grad Norm=0.0635\n","Epoch 2, Batch 181: Loss=0.0010, Grad Norm=0.0461\n","Epoch 2, Batch 182: Loss=0.0007, Grad Norm=0.0217\n","Epoch 2, Batch 183: Loss=0.0083, Grad Norm=0.3496\n","Epoch 2, Batch 184: Loss=0.0041, Grad Norm=0.1550\n","Epoch 2, Batch 185: Loss=0.0526, Grad Norm=1.5920\n","Epoch 2, Batch 186: Loss=0.0001, Grad Norm=0.0029\n","Epoch 2, Batch 187: Loss=0.0037, Grad Norm=0.1756\n","Epoch 2, Batch 188: Loss=0.0182, Grad Norm=0.9191\n","Epoch 2/15:\n","Train Loss: 0.0224, Train Accuracy: 99.17%\n","Val Loss: 0.0187, Val Accuracy: 99.50%\n","Epoch 3, Batch 1: Loss=0.0001, Grad Norm=0.0031\n","Epoch 3, Batch 2: Loss=0.0421, Grad Norm=2.0341\n","Epoch 3, Batch 3: Loss=0.0001, Grad Norm=0.0026\n","Epoch 3, Batch 4: Loss=0.1609, Grad Norm=5.1243\n","Epoch 3, Batch 5: Loss=0.0032, Grad Norm=0.1598\n","Epoch 3, Batch 6: Loss=0.0766, Grad Norm=2.4730\n","Epoch 3, Batch 7: Loss=0.1311, Grad Norm=3.2789\n","Epoch 3, Batch 8: Loss=0.0111, Grad Norm=0.8589\n","Epoch 3, Batch 9: Loss=0.0041, Grad Norm=0.1118\n","Epoch 3, Batch 10: Loss=0.0007, Grad Norm=0.0266\n","Epoch 3, Batch 11: Loss=0.0005, Grad Norm=0.0246\n","Epoch 3, Batch 12: Loss=0.0001, Grad Norm=0.0036\n","Epoch 3, Batch 13: Loss=0.0000, Grad Norm=0.0005\n","Epoch 3, Batch 14: Loss=0.0017, Grad Norm=0.0733\n","Epoch 3, Batch 15: Loss=0.0007, Grad Norm=0.0374\n","Epoch 3, Batch 16: Loss=0.0005, Grad Norm=0.0163\n","Epoch 3, Batch 17: Loss=0.0167, Grad Norm=0.8052\n","Epoch 3, Batch 18: Loss=0.0106, Grad Norm=0.4785\n","Epoch 3, Batch 19: Loss=0.0001, Grad Norm=0.0041\n","Epoch 3, Batch 20: Loss=0.0001, Grad Norm=0.0041\n","Epoch 3, Batch 21: Loss=0.0002, Grad Norm=0.0067\n","Epoch 3, Batch 22: Loss=0.0001, Grad Norm=0.0041\n","Epoch 3, Batch 23: Loss=0.2196, Grad Norm=2.7370\n","Epoch 3, Batch 24: Loss=0.7455, Grad Norm=6.8302\n","Epoch 3, Batch 25: Loss=0.0057, Grad Norm=0.2098\n","Epoch 3, Batch 26: Loss=0.0039, Grad Norm=0.1717\n","Epoch 3, Batch 27: Loss=0.0069, Grad Norm=0.4097\n","Epoch 3, Batch 28: Loss=0.0004, Grad Norm=0.0253\n","Epoch 3, Batch 29: Loss=0.0004, Grad Norm=0.0111\n","Epoch 3, Batch 30: Loss=0.0275, Grad Norm=0.7134\n","Epoch 3, Batch 31: Loss=0.0109, Grad Norm=0.3294\n","Epoch 3, Batch 32: Loss=0.0293, Grad Norm=0.8533\n","Epoch 3, Batch 33: Loss=0.0313, Grad Norm=1.7339\n","Epoch 3, Batch 34: Loss=0.0007, Grad Norm=0.0205\n","Epoch 3, Batch 35: Loss=0.0023, Grad Norm=0.0907\n","Epoch 3, Batch 36: Loss=0.0016, Grad Norm=0.0476\n","Epoch 3, Batch 37: Loss=0.0122, Grad Norm=0.5271\n","Epoch 3, Batch 38: Loss=0.1514, Grad Norm=4.9368\n","Epoch 3, Batch 39: Loss=0.0002, Grad Norm=0.0063\n","Epoch 3, Batch 40: Loss=0.0013, Grad Norm=0.0424\n","Epoch 3, Batch 41: Loss=0.0858, Grad Norm=2.4225\n","Epoch 3, Batch 42: Loss=0.0000, Grad Norm=0.0008\n","Epoch 3, Batch 43: Loss=0.0020, Grad Norm=0.0918\n","Epoch 3, Batch 44: Loss=0.0005, Grad Norm=0.0180\n","Epoch 3, Batch 45: Loss=0.0025, Grad Norm=0.1482\n","Epoch 3, Batch 46: Loss=0.0042, Grad Norm=0.3228\n","Epoch 3, Batch 47: Loss=0.0004, Grad Norm=0.0201\n","Epoch 3, Batch 48: Loss=0.0008, Grad Norm=0.0217\n","Epoch 3, Batch 49: Loss=0.0008, Grad Norm=0.0231\n","Epoch 3, Batch 50: Loss=0.0052, Grad Norm=0.1777\n","Epoch 3, Batch 51: Loss=0.0016, Grad Norm=0.0646\n","Epoch 3, Batch 52: Loss=0.0035, Grad Norm=0.1305\n","Epoch 3, Batch 53: Loss=0.0026, Grad Norm=0.1243\n","Epoch 3, Batch 54: Loss=0.0085, Grad Norm=0.4537\n","Epoch 3, Batch 55: Loss=0.0023, Grad Norm=0.0846\n","Epoch 3, Batch 56: Loss=0.0040, Grad Norm=0.1127\n","Epoch 3, Batch 57: Loss=0.0022, Grad Norm=0.1476\n","Epoch 3, Batch 58: Loss=0.1544, Grad Norm=2.7991\n","Epoch 3, Batch 59: Loss=0.0157, Grad Norm=0.7634\n","Epoch 3, Batch 60: Loss=0.0021, Grad Norm=0.0944\n","Epoch 3, Batch 61: Loss=0.0029, Grad Norm=0.1429\n","Epoch 3, Batch 62: Loss=0.0001, Grad Norm=0.0036\n","Epoch 3, Batch 63: Loss=0.0001, Grad Norm=0.0025\n","Epoch 3, Batch 64: Loss=0.0117, Grad Norm=0.4219\n","Epoch 3, Batch 65: Loss=0.0002, Grad Norm=0.0064\n","Epoch 3, Batch 66: Loss=0.0019, Grad Norm=0.0627\n","Epoch 3, Batch 67: Loss=0.0002, Grad Norm=0.0047\n","Epoch 3, Batch 68: Loss=0.0002, Grad Norm=0.0051\n","Epoch 3, Batch 69: Loss=0.0003, Grad Norm=0.0089\n","Epoch 3, Batch 70: Loss=0.0004, Grad Norm=0.0099\n","Epoch 3, Batch 71: Loss=0.0021, Grad Norm=0.1269\n","Epoch 3, Batch 72: Loss=0.0045, Grad Norm=0.2487\n","Epoch 3, Batch 73: Loss=0.0017, Grad Norm=0.0963\n","Epoch 3, Batch 74: Loss=0.0011, Grad Norm=0.0307\n","Epoch 3, Batch 75: Loss=0.0017, Grad Norm=0.0530\n","Epoch 3, Batch 76: Loss=0.0005, Grad Norm=0.0253\n","Epoch 3, Batch 77: Loss=0.0007, Grad Norm=0.0204\n","Epoch 3, Batch 78: Loss=0.0050, Grad Norm=0.1331\n","Epoch 3, Batch 79: Loss=0.0013, Grad Norm=0.0448\n","Epoch 3, Batch 80: Loss=0.0005, Grad Norm=0.0162\n","Epoch 3, Batch 81: Loss=0.0016, Grad Norm=0.1400\n","Epoch 3, Batch 82: Loss=0.0009, Grad Norm=0.0282\n","Epoch 3, Batch 83: Loss=0.0004, Grad Norm=0.0100\n","Epoch 3, Batch 84: Loss=0.0066, Grad Norm=0.3054\n","Epoch 3, Batch 85: Loss=0.0326, Grad Norm=1.5001\n","Epoch 3, Batch 86: Loss=0.0001, Grad Norm=0.0013\n","Epoch 3, Batch 87: Loss=0.0051, Grad Norm=0.2285\n","Epoch 3, Batch 88: Loss=0.0004, Grad Norm=0.0193\n","Epoch 3, Batch 89: Loss=0.0031, Grad Norm=0.0749\n","Epoch 3, Batch 90: Loss=0.0883, Grad Norm=2.8031\n","Epoch 3, Batch 91: Loss=0.0722, Grad Norm=2.1572\n","Epoch 3, Batch 92: Loss=0.0056, Grad Norm=0.2144\n","Epoch 3, Batch 93: Loss=0.0006, Grad Norm=0.0147\n","Epoch 3, Batch 94: Loss=0.0002, Grad Norm=0.0105\n","Epoch 3, Batch 95: Loss=0.0004, Grad Norm=0.0159\n","Epoch 3, Batch 96: Loss=0.0001, Grad Norm=0.0036\n","Epoch 3, Batch 97: Loss=0.0078, Grad Norm=0.3071\n","Epoch 3, Batch 98: Loss=0.0006, Grad Norm=0.0218\n","Epoch 3, Batch 99: Loss=0.0008, Grad Norm=0.0319\n","Epoch 3, Batch 100: Loss=0.0002, Grad Norm=0.0089\n","Epoch 3, Batch 101: Loss=0.2231, Grad Norm=3.5930\n","Epoch 3, Batch 102: Loss=0.0185, Grad Norm=0.8015\n","Epoch 3, Batch 103: Loss=0.0030, Grad Norm=0.0809\n","Epoch 3, Batch 104: Loss=0.0004, Grad Norm=0.0099\n","Epoch 3, Batch 105: Loss=0.0017, Grad Norm=0.0422\n","Epoch 3, Batch 106: Loss=0.0069, Grad Norm=0.2754\n","Epoch 3, Batch 107: Loss=0.0008, Grad Norm=0.0324\n","Epoch 3, Batch 108: Loss=0.0000, Grad Norm=0.0015\n","Epoch 3, Batch 109: Loss=0.0093, Grad Norm=0.3533\n","Epoch 3, Batch 110: Loss=0.2237, Grad Norm=5.0545\n","Epoch 3, Batch 111: Loss=0.1200, Grad Norm=2.8673\n","Epoch 3, Batch 112: Loss=0.0965, Grad Norm=3.7921\n","Epoch 3, Batch 113: Loss=0.0015, Grad Norm=0.0897\n","Epoch 3, Batch 114: Loss=0.0003, Grad Norm=0.0102\n","Epoch 3, Batch 115: Loss=0.0002, Grad Norm=0.0051\n","Epoch 3, Batch 116: Loss=0.0012, Grad Norm=0.0608\n","Epoch 3, Batch 117: Loss=0.0002, Grad Norm=0.0050\n","Epoch 3, Batch 118: Loss=0.0033, Grad Norm=0.1447\n","Epoch 3, Batch 119: Loss=0.0087, Grad Norm=0.3934\n","Epoch 3, Batch 120: Loss=0.0124, Grad Norm=0.4937\n","Epoch 3, Batch 121: Loss=0.0053, Grad Norm=0.3216\n","Epoch 3, Batch 122: Loss=0.0007, Grad Norm=0.0206\n","Epoch 3, Batch 123: Loss=0.0335, Grad Norm=1.3503\n","Epoch 3, Batch 124: Loss=0.0024, Grad Norm=0.0773\n","Epoch 3, Batch 125: Loss=0.0017, Grad Norm=0.0603\n","Epoch 3, Batch 126: Loss=0.0007, Grad Norm=0.0210\n","Epoch 3, Batch 127: Loss=0.3674, Grad Norm=4.8374\n","Epoch 3, Batch 128: Loss=0.0001, Grad Norm=0.0046\n","Epoch 3, Batch 129: Loss=0.0021, Grad Norm=0.0781\n","Epoch 3, Batch 130: Loss=0.0007, Grad Norm=0.0413\n","Epoch 3, Batch 131: Loss=0.0007, Grad Norm=0.0280\n","Epoch 3, Batch 132: Loss=0.0122, Grad Norm=0.5759\n","Epoch 3, Batch 133: Loss=0.0046, Grad Norm=0.1583\n","Epoch 3, Batch 134: Loss=0.0194, Grad Norm=0.8932\n","Epoch 3, Batch 135: Loss=0.0035, Grad Norm=0.1674\n","Epoch 3, Batch 136: Loss=0.0023, Grad Norm=0.0721\n","Epoch 3, Batch 137: Loss=0.0009, Grad Norm=0.0269\n","Epoch 3, Batch 138: Loss=0.0056, Grad Norm=0.1953\n","Epoch 3, Batch 139: Loss=0.0015, Grad Norm=0.0562\n","Epoch 3, Batch 140: Loss=0.0003, Grad Norm=0.0081\n","Epoch 3, Batch 141: Loss=0.0002, Grad Norm=0.0036\n","Epoch 3, Batch 142: Loss=0.0002, Grad Norm=0.0052\n","Epoch 3, Batch 143: Loss=0.0030, Grad Norm=0.1110\n","Epoch 3, Batch 144: Loss=0.0028, Grad Norm=0.1277\n","Epoch 3, Batch 145: Loss=0.0071, Grad Norm=0.4233\n","Epoch 3, Batch 146: Loss=0.0005, Grad Norm=0.0194\n","Epoch 3, Batch 147: Loss=0.0006, Grad Norm=0.0274\n","Epoch 3, Batch 148: Loss=0.0157, Grad Norm=0.4767\n","Epoch 3, Batch 149: Loss=0.0133, Grad Norm=0.3912\n","Epoch 3, Batch 150: Loss=0.1333, Grad Norm=4.1181\n","Epoch 3, Batch 151: Loss=0.0005, Grad Norm=0.0142\n","Epoch 3, Batch 152: Loss=0.0091, Grad Norm=0.3109\n","Epoch 3, Batch 153: Loss=0.0242, Grad Norm=1.2533\n","Epoch 3, Batch 154: Loss=0.0001, Grad Norm=0.0051\n","Epoch 3, Batch 155: Loss=0.0017, Grad Norm=0.0595\n","Epoch 3, Batch 156: Loss=0.0150, Grad Norm=0.5738\n","Epoch 3, Batch 157: Loss=0.0002, Grad Norm=0.0064\n","Epoch 3, Batch 158: Loss=0.0013, Grad Norm=0.0479\n","Epoch 3, Batch 159: Loss=0.3474, Grad Norm=4.0778\n","Epoch 3, Batch 160: Loss=0.0008, Grad Norm=0.0278\n","Epoch 3, Batch 161: Loss=0.0010, Grad Norm=0.0538\n","Epoch 3, Batch 162: Loss=0.0044, Grad Norm=0.1888\n","Epoch 3, Batch 163: Loss=0.0636, Grad Norm=1.5812\n","Epoch 3, Batch 164: Loss=0.0454, Grad Norm=1.9122\n","Epoch 3, Batch 165: Loss=0.0003, Grad Norm=0.0123\n","Epoch 3, Batch 166: Loss=0.0017, Grad Norm=0.0479\n","Epoch 3, Batch 167: Loss=0.0032, Grad Norm=0.0867\n","Epoch 3, Batch 168: Loss=0.0061, Grad Norm=0.2949\n","Epoch 3, Batch 169: Loss=0.0072, Grad Norm=0.2776\n","Epoch 3, Batch 170: Loss=0.0007, Grad Norm=0.0253\n","Epoch 3, Batch 171: Loss=0.0003, Grad Norm=0.0071\n","Epoch 3, Batch 172: Loss=0.0052, Grad Norm=0.2389\n","Epoch 3, Batch 173: Loss=0.0003, Grad Norm=0.0085\n","Epoch 3, Batch 174: Loss=0.0012, Grad Norm=0.0674\n","Epoch 3, Batch 175: Loss=0.0007, Grad Norm=0.0242\n","Epoch 3, Batch 176: Loss=0.0007, Grad Norm=0.0379\n","Epoch 3, Batch 177: Loss=0.0575, Grad Norm=1.6259\n","Epoch 3, Batch 178: Loss=0.0034, Grad Norm=0.1028\n","Epoch 3, Batch 179: Loss=0.0024, Grad Norm=0.0697\n","Epoch 3, Batch 180: Loss=0.0023, Grad Norm=0.0770\n","Epoch 3, Batch 181: Loss=0.2363, Grad Norm=3.3424\n","Epoch 3, Batch 182: Loss=0.0000, Grad Norm=0.0010\n","Epoch 3, Batch 183: Loss=0.0089, Grad Norm=0.3310\n","Epoch 3, Batch 184: Loss=0.0006, Grad Norm=0.0178\n","Epoch 3, Batch 185: Loss=0.0005, Grad Norm=0.0111\n","Epoch 3, Batch 186: Loss=0.0011, Grad Norm=0.0442\n","Epoch 3, Batch 187: Loss=0.0224, Grad Norm=0.9467\n","Epoch 3, Batch 188: Loss=0.7475, Grad Norm=12.6376\n","Epoch 3/15:\n","Train Loss: 0.0281, Train Accuracy: 99.20%\n","Val Loss: 0.0080, Val Accuracy: 99.67%\n","Epoch 4, Batch 1: Loss=0.0033, Grad Norm=0.0837\n","Epoch 4, Batch 2: Loss=0.0153, Grad Norm=0.5714\n","Epoch 4, Batch 3: Loss=0.0003, Grad Norm=0.0105\n","Epoch 4, Batch 4: Loss=0.0000, Grad Norm=0.0004\n","Epoch 4, Batch 5: Loss=0.0008, Grad Norm=0.0255\n","Epoch 4, Batch 6: Loss=0.0043, Grad Norm=0.1633\n","Epoch 4, Batch 7: Loss=1.0778, Grad Norm=5.4117\n","Epoch 4, Batch 8: Loss=0.0003, Grad Norm=0.0077\n","Epoch 4, Batch 9: Loss=0.0316, Grad Norm=1.5379\n","Epoch 4, Batch 10: Loss=0.1937, Grad Norm=3.1034\n","Epoch 4, Batch 11: Loss=0.0650, Grad Norm=1.9777\n","Epoch 4, Batch 12: Loss=0.0002, Grad Norm=0.0037\n","Epoch 4, Batch 13: Loss=0.1755, Grad Norm=3.9482\n","Epoch 4, Batch 14: Loss=0.0005, Grad Norm=0.0125\n","Epoch 4, Batch 15: Loss=0.0001, Grad Norm=0.0018\n","Epoch 4, Batch 16: Loss=0.0027, Grad Norm=0.0793\n","Epoch 4, Batch 17: Loss=0.0014, Grad Norm=0.0410\n","Epoch 4, Batch 18: Loss=0.0101, Grad Norm=0.3278\n","Epoch 4, Batch 19: Loss=0.0024, Grad Norm=0.0904\n","Epoch 4, Batch 20: Loss=0.0022, Grad Norm=0.0625\n","Epoch 4, Batch 21: Loss=0.0011, Grad Norm=0.0384\n","Epoch 4, Batch 22: Loss=0.0008, Grad Norm=0.0312\n","Epoch 4, Batch 23: Loss=0.0224, Grad Norm=0.5337\n","Epoch 4, Batch 24: Loss=0.0002, Grad Norm=0.0080\n","Epoch 4, Batch 25: Loss=0.0061, Grad Norm=0.1857\n","Epoch 4, Batch 26: Loss=0.0009, Grad Norm=0.0226\n","Epoch 4, Batch 27: Loss=0.0021, Grad Norm=0.0708\n","Epoch 4, Batch 28: Loss=0.0150, Grad Norm=0.4427\n","Epoch 4, Batch 29: Loss=0.0002, Grad Norm=0.0065\n","Epoch 4, Batch 30: Loss=0.0003, Grad Norm=0.0067\n","Epoch 4, Batch 31: Loss=0.0019, Grad Norm=0.0608\n","Epoch 4, Batch 32: Loss=0.0562, Grad Norm=1.6296\n","Epoch 4, Batch 33: Loss=0.0004, Grad Norm=0.0119\n","Epoch 4, Batch 34: Loss=0.0012, Grad Norm=0.0600\n","Epoch 4, Batch 35: Loss=0.0153, Grad Norm=0.4385\n","Epoch 4, Batch 36: Loss=0.0055, Grad Norm=0.1532\n","Epoch 4, Batch 37: Loss=0.0171, Grad Norm=0.5885\n","Epoch 4, Batch 38: Loss=0.0008, Grad Norm=0.0188\n","Epoch 4, Batch 39: Loss=0.0005, Grad Norm=0.0121\n","Epoch 4, Batch 40: Loss=0.0504, Grad Norm=1.5037\n","Epoch 4, Batch 41: Loss=0.0042, Grad Norm=0.1305\n","Epoch 4, Batch 42: Loss=0.0002, Grad Norm=0.0037\n","Epoch 4, Batch 43: Loss=0.0005, Grad Norm=0.0182\n","Epoch 4, Batch 44: Loss=0.0007, Grad Norm=0.0164\n","Epoch 4, Batch 45: Loss=0.0037, Grad Norm=0.1832\n","Epoch 4, Batch 46: Loss=0.0007, Grad Norm=0.0133\n","Epoch 4, Batch 47: Loss=0.0053, Grad Norm=0.1573\n","Epoch 4, Batch 48: Loss=0.0020, Grad Norm=0.0563\n","Epoch 4, Batch 49: Loss=0.0012, Grad Norm=0.0331\n","Epoch 4, Batch 50: Loss=0.0159, Grad Norm=0.3641\n","Epoch 4, Batch 51: Loss=0.0014, Grad Norm=0.0381\n","Epoch 4, Batch 52: Loss=0.0007, Grad Norm=0.0460\n","Epoch 4, Batch 53: Loss=0.0004, Grad Norm=0.0134\n","Epoch 4, Batch 54: Loss=0.0002, Grad Norm=0.0066\n","Epoch 4, Batch 55: Loss=0.0122, Grad Norm=0.4084\n","Epoch 4, Batch 56: Loss=0.0095, Grad Norm=0.4258\n","Epoch 4, Batch 57: Loss=0.0083, Grad Norm=0.2385\n","Epoch 4, Batch 58: Loss=0.0011, Grad Norm=0.0294\n","Epoch 4, Batch 59: Loss=0.0004, Grad Norm=0.0110\n","Epoch 4, Batch 60: Loss=0.0019, Grad Norm=0.0796\n","Epoch 4, Batch 61: Loss=0.0007, Grad Norm=0.0349\n","Epoch 4, Batch 62: Loss=0.0003, Grad Norm=0.0095\n","Epoch 4, Batch 63: Loss=0.0149, Grad Norm=0.5137\n","Epoch 4, Batch 64: Loss=0.2257, Grad Norm=3.7383\n","Epoch 4, Batch 65: Loss=0.0244, Grad Norm=0.8277\n","Epoch 4, Batch 66: Loss=0.0754, Grad Norm=1.8904\n","Epoch 4, Batch 67: Loss=0.0015, Grad Norm=0.0594\n","Epoch 4, Batch 68: Loss=0.0017, Grad Norm=0.0447\n","Epoch 4, Batch 69: Loss=0.0006, Grad Norm=0.0226\n","Epoch 4, Batch 70: Loss=0.0014, Grad Norm=0.0522\n","Epoch 4, Batch 71: Loss=0.0006, Grad Norm=0.0183\n","Epoch 4, Batch 72: Loss=0.0041, Grad Norm=0.1747\n","Epoch 4, Batch 73: Loss=0.0011, Grad Norm=0.0490\n","Epoch 4, Batch 74: Loss=0.0077, Grad Norm=0.2484\n","Epoch 4, Batch 75: Loss=0.2659, Grad Norm=3.1908\n","Epoch 4, Batch 76: Loss=0.2146, Grad Norm=4.5704\n","Epoch 4, Batch 77: Loss=0.0127, Grad Norm=0.5008\n","Epoch 4, Batch 78: Loss=0.0873, Grad Norm=2.3516\n","Epoch 4, Batch 79: Loss=0.0002, Grad Norm=0.0070\n","Epoch 4, Batch 80: Loss=0.0001, Grad Norm=0.0019\n","Epoch 4, Batch 81: Loss=0.0041, Grad Norm=0.1031\n","Epoch 4, Batch 82: Loss=0.0030, Grad Norm=0.0818\n","Epoch 4, Batch 83: Loss=0.1970, Grad Norm=4.8322\n","Epoch 4, Batch 84: Loss=0.0009, Grad Norm=0.0182\n","Epoch 4, Batch 85: Loss=0.0002, Grad Norm=0.0059\n","Epoch 4, Batch 86: Loss=0.0005, Grad Norm=0.0138\n","Epoch 4, Batch 87: Loss=0.0099, Grad Norm=0.6653\n","Epoch 4, Batch 88: Loss=0.0009, Grad Norm=0.0250\n","Epoch 4, Batch 89: Loss=0.0308, Grad Norm=1.4018\n","Epoch 4, Batch 90: Loss=0.2736, Grad Norm=4.1142\n","Epoch 4, Batch 91: Loss=0.0017, Grad Norm=0.0410\n","Epoch 4, Batch 92: Loss=0.0007, Grad Norm=0.0197\n","Epoch 4, Batch 93: Loss=0.0015, Grad Norm=0.0399\n","Epoch 4, Batch 94: Loss=0.0002, Grad Norm=0.0050\n","Epoch 4, Batch 95: Loss=0.0840, Grad Norm=2.4739\n","Epoch 4, Batch 96: Loss=0.0021, Grad Norm=0.0912\n","Epoch 4, Batch 97: Loss=0.0016, Grad Norm=0.0736\n","Epoch 4, Batch 98: Loss=0.0001, Grad Norm=0.0033\n","Epoch 4, Batch 99: Loss=0.1985, Grad Norm=3.6495\n","Epoch 4, Batch 100: Loss=0.0003, Grad Norm=0.0068\n","Epoch 4, Batch 101: Loss=0.0002, Grad Norm=0.0076\n","Epoch 4, Batch 102: Loss=0.0013, Grad Norm=0.0386\n","Epoch 4, Batch 103: Loss=0.0015, Grad Norm=0.0372\n","Epoch 4, Batch 104: Loss=0.0329, Grad Norm=1.5617\n","Epoch 4, Batch 105: Loss=0.0333, Grad Norm=0.9823\n","Epoch 4, Batch 106: Loss=0.0070, Grad Norm=0.2300\n","Epoch 4, Batch 107: Loss=0.0003, Grad Norm=0.0086\n","Epoch 4, Batch 108: Loss=0.0039, Grad Norm=0.1379\n","Epoch 4, Batch 109: Loss=0.0003, Grad Norm=0.0094\n","Epoch 4, Batch 110: Loss=0.0075, Grad Norm=0.2116\n","Epoch 4, Batch 111: Loss=0.0053, Grad Norm=0.2318\n","Epoch 4, Batch 112: Loss=0.0099, Grad Norm=0.4557\n","Epoch 4, Batch 113: Loss=0.0021, Grad Norm=0.0488\n","Epoch 4, Batch 114: Loss=0.2679, Grad Norm=4.0068\n","Epoch 4, Batch 115: Loss=0.0017, Grad Norm=0.0485\n","Epoch 4, Batch 116: Loss=0.0233, Grad Norm=0.8966\n","Epoch 4, Batch 117: Loss=0.0024, Grad Norm=0.0695\n","Epoch 4, Batch 118: Loss=0.0009, Grad Norm=0.0245\n","Epoch 4, Batch 119: Loss=0.0106, Grad Norm=0.4947\n","Epoch 4, Batch 120: Loss=0.0002, Grad Norm=0.0049\n","Epoch 4, Batch 121: Loss=0.1342, Grad Norm=2.4187\n","Epoch 4, Batch 122: Loss=0.0007, Grad Norm=0.0241\n","Epoch 4, Batch 123: Loss=0.0012, Grad Norm=0.0552\n","Epoch 4, Batch 124: Loss=0.0024, Grad Norm=0.0861\n","Epoch 4, Batch 125: Loss=0.0013, Grad Norm=0.0334\n","Epoch 4, Batch 126: Loss=0.0015, Grad Norm=0.0336\n","Epoch 4, Batch 127: Loss=0.3629, Grad Norm=5.5565\n","Epoch 4, Batch 128: Loss=0.0004, Grad Norm=0.0136\n","Epoch 4, Batch 129: Loss=0.0006, Grad Norm=0.0267\n","Epoch 4, Batch 130: Loss=0.0018, Grad Norm=0.0829\n","Epoch 4, Batch 131: Loss=0.0004, Grad Norm=0.0114\n","Epoch 4, Batch 132: Loss=0.0213, Grad Norm=1.3076\n","Epoch 4, Batch 133: Loss=0.0387, Grad Norm=1.7289\n","Epoch 4, Batch 134: Loss=0.0001, Grad Norm=0.0020\n","Epoch 4, Batch 135: Loss=0.2092, Grad Norm=3.1506\n","Epoch 4, Batch 136: Loss=0.0163, Grad Norm=0.5212\n","Epoch 4, Batch 137: Loss=0.1173, Grad Norm=3.0026\n","Epoch 4, Batch 138: Loss=0.0014, Grad Norm=0.0410\n","Epoch 4, Batch 139: Loss=0.0009, Grad Norm=0.0200\n","Epoch 4, Batch 140: Loss=0.0003, Grad Norm=0.0075\n","Epoch 4, Batch 141: Loss=0.0038, Grad Norm=0.1109\n","Epoch 4, Batch 142: Loss=0.0004, Grad Norm=0.0138\n","Epoch 4, Batch 143: Loss=0.0001, Grad Norm=0.0020\n","Epoch 4, Batch 144: Loss=0.0042, Grad Norm=0.1807\n","Epoch 4, Batch 145: Loss=0.0016, Grad Norm=0.0396\n","Epoch 4, Batch 146: Loss=0.0070, Grad Norm=0.3444\n","Epoch 4, Batch 147: Loss=0.0019, Grad Norm=0.0720\n","Epoch 4, Batch 148: Loss=0.2935, Grad Norm=3.1634\n","Epoch 4, Batch 149: Loss=0.0501, Grad Norm=2.1727\n","Epoch 4, Batch 150: Loss=0.0004, Grad Norm=0.0093\n","Epoch 4, Batch 151: Loss=0.0028, Grad Norm=0.0812\n","Epoch 4, Batch 152: Loss=0.1138, Grad Norm=2.8406\n","Epoch 4, Batch 153: Loss=0.0000, Grad Norm=0.0003\n","Epoch 4, Batch 154: Loss=0.0001, Grad Norm=0.0043\n","Epoch 4, Batch 155: Loss=0.0027, Grad Norm=0.0716\n","Epoch 4, Batch 156: Loss=0.0002, Grad Norm=0.0063\n","Epoch 4, Batch 157: Loss=0.0032, Grad Norm=0.1315\n","Epoch 4, Batch 158: Loss=0.0003, Grad Norm=0.0101\n","Epoch 4, Batch 159: Loss=0.3522, Grad Norm=2.6673\n","Epoch 4, Batch 160: Loss=0.0012, Grad Norm=0.0428\n","Epoch 4, Batch 161: Loss=0.0006, Grad Norm=0.0135\n","Epoch 4, Batch 162: Loss=0.1129, Grad Norm=2.9979\n","Epoch 4, Batch 163: Loss=0.0005, Grad Norm=0.0186\n","Epoch 4, Batch 164: Loss=0.0833, Grad Norm=2.1331\n","Epoch 4, Batch 165: Loss=0.0013, Grad Norm=0.0425\n","Epoch 4, Batch 166: Loss=0.0235, Grad Norm=0.5131\n","Epoch 4, Batch 167: Loss=0.0932, Grad Norm=2.4239\n","Epoch 4, Batch 168: Loss=0.0031, Grad Norm=0.0679\n","Epoch 4, Batch 169: Loss=0.0040, Grad Norm=0.1004\n","Epoch 4, Batch 170: Loss=0.0007, Grad Norm=0.0199\n","Epoch 4, Batch 171: Loss=0.0003, Grad Norm=0.0104\n","Epoch 4, Batch 172: Loss=0.0004, Grad Norm=0.0077\n","Epoch 4, Batch 173: Loss=0.0134, Grad Norm=0.6451\n","Epoch 4, Batch 174: Loss=0.0031, Grad Norm=0.0833\n","Epoch 4, Batch 175: Loss=0.0011, Grad Norm=0.0276\n","Epoch 4, Batch 176: Loss=0.0414, Grad Norm=1.1784\n","Epoch 4, Batch 177: Loss=0.0098, Grad Norm=0.3616\n","Epoch 4, Batch 178: Loss=0.0050, Grad Norm=0.2279\n","Epoch 4, Batch 179: Loss=0.0994, Grad Norm=1.8158\n","Epoch 4, Batch 180: Loss=0.0007, Grad Norm=0.0186\n","Epoch 4, Batch 181: Loss=0.0020, Grad Norm=0.0621\n","Epoch 4, Batch 182: Loss=0.0071, Grad Norm=0.3135\n","Epoch 4, Batch 183: Loss=0.0048, Grad Norm=0.2368\n","Epoch 4, Batch 184: Loss=0.0025, Grad Norm=0.0903\n","Epoch 4, Batch 185: Loss=0.0109, Grad Norm=0.2552\n","Epoch 4, Batch 186: Loss=0.0005, Grad Norm=0.0120\n","Epoch 4, Batch 187: Loss=0.0107, Grad Norm=0.3303\n","Epoch 4, Batch 188: Loss=0.0005, Grad Norm=0.0127\n","Epoch 4/15:\n","Train Loss: 0.0336, Train Accuracy: 99.03%\n","Val Loss: 0.0162, Val Accuracy: 99.50%\n","Model saved (improved val loss: 0.0162)\n","Epoch 5, Batch 1: Loss=0.0002, Grad Norm=0.0064\n","Epoch 5, Batch 2: Loss=0.0257, Grad Norm=0.6081\n","Epoch 5, Batch 3: Loss=0.0011, Grad Norm=0.0279\n","Epoch 5, Batch 4: Loss=0.0016, Grad Norm=0.0540\n","Epoch 5, Batch 5: Loss=0.0002, Grad Norm=0.0033\n","Epoch 5, Batch 6: Loss=0.0057, Grad Norm=0.2744\n","Epoch 5, Batch 7: Loss=0.0065, Grad Norm=0.1808\n","Epoch 5, Batch 8: Loss=0.0011, Grad Norm=0.0510\n","Epoch 5, Batch 9: Loss=0.0543, Grad Norm=1.6957\n","Epoch 5, Batch 10: Loss=0.0043, Grad Norm=0.1292\n","Epoch 5, Batch 11: Loss=0.0001, Grad Norm=0.0028\n","Epoch 5, Batch 12: Loss=0.0256, Grad Norm=0.6882\n","Epoch 5, Batch 13: Loss=0.0002, Grad Norm=0.0045\n","Epoch 5, Batch 14: Loss=0.0013, Grad Norm=0.0453\n","Epoch 5, Batch 15: Loss=0.0147, Grad Norm=0.4299\n","Epoch 5, Batch 16: Loss=0.0011, Grad Norm=0.0228\n","Epoch 5, Batch 17: Loss=0.0011, Grad Norm=0.0416\n","Epoch 5, Batch 18: Loss=0.0002, Grad Norm=0.0078\n","Epoch 5, Batch 19: Loss=0.0113, Grad Norm=0.2484\n","Epoch 5, Batch 20: Loss=0.0020, Grad Norm=0.0490\n","Epoch 5, Batch 21: Loss=0.0279, Grad Norm=0.7128\n","Epoch 5, Batch 22: Loss=0.0002, Grad Norm=0.0042\n","Epoch 5, Batch 23: Loss=0.0057, Grad Norm=0.2648\n","Epoch 5, Batch 24: Loss=0.0016, Grad Norm=0.0389\n","Epoch 5, Batch 25: Loss=0.0003, Grad Norm=0.0073\n","Epoch 5, Batch 26: Loss=0.0003, Grad Norm=0.0090\n","Epoch 5, Batch 27: Loss=0.0138, Grad Norm=0.5435\n","Epoch 5, Batch 28: Loss=0.0004, Grad Norm=0.0097\n","Epoch 5, Batch 29: Loss=0.0309, Grad Norm=0.7372\n","Epoch 5, Batch 30: Loss=0.0035, Grad Norm=0.1130\n","Epoch 5, Batch 31: Loss=0.0010, Grad Norm=0.0218\n","Epoch 5, Batch 32: Loss=0.0217, Grad Norm=0.6993\n","Epoch 5, Batch 33: Loss=0.0114, Grad Norm=0.4091\n","Epoch 5, Batch 34: Loss=0.0001, Grad Norm=0.0016\n","Epoch 5, Batch 35: Loss=0.0219, Grad Norm=0.7034\n","Epoch 5, Batch 36: Loss=0.0020, Grad Norm=0.0910\n","Epoch 5, Batch 37: Loss=0.0240, Grad Norm=0.6056\n","Epoch 5, Batch 38: Loss=0.0007, Grad Norm=0.0261\n","Epoch 5, Batch 39: Loss=0.0015, Grad Norm=0.0475\n","Epoch 5, Batch 40: Loss=0.0007, Grad Norm=0.0162\n","Epoch 5, Batch 41: Loss=0.0061, Grad Norm=0.1851\n","Epoch 5, Batch 42: Loss=0.0032, Grad Norm=0.0706\n","Epoch 5, Batch 43: Loss=0.0610, Grad Norm=1.8385\n","Epoch 5, Batch 44: Loss=0.1174, Grad Norm=2.9228\n","Epoch 5, Batch 45: Loss=0.0225, Grad Norm=0.9029\n","Epoch 5, Batch 46: Loss=0.0016, Grad Norm=0.0401\n","Epoch 5, Batch 47: Loss=0.0002, Grad Norm=0.0060\n","Epoch 5, Batch 48: Loss=0.0005, Grad Norm=0.0206\n","Epoch 5, Batch 49: Loss=0.0015, Grad Norm=0.0312\n","Epoch 5, Batch 50: Loss=0.0001, Grad Norm=0.0021\n","Epoch 5, Batch 51: Loss=0.0006, Grad Norm=0.0210\n","Epoch 5, Batch 52: Loss=0.0004, Grad Norm=0.0114\n","Epoch 5, Batch 53: Loss=0.0013, Grad Norm=0.0360\n","Epoch 5, Batch 54: Loss=0.0110, Grad Norm=0.3344\n","Epoch 5, Batch 55: Loss=0.0003, Grad Norm=0.0065\n","Epoch 5, Batch 56: Loss=0.0168, Grad Norm=0.4730\n","Epoch 5, Batch 57: Loss=0.0007, Grad Norm=0.0206\n","Epoch 5, Batch 58: Loss=0.1130, Grad Norm=2.7986\n","Epoch 5, Batch 59: Loss=0.0012, Grad Norm=0.0506\n","Epoch 5, Batch 60: Loss=0.0003, Grad Norm=0.0077\n","Epoch 5, Batch 61: Loss=0.0001, Grad Norm=0.0023\n","Epoch 5, Batch 62: Loss=0.0009, Grad Norm=0.0260\n","Epoch 5, Batch 63: Loss=0.0001, Grad Norm=0.0039\n","Epoch 5, Batch 64: Loss=0.0013, Grad Norm=0.0335\n","Epoch 5, Batch 65: Loss=0.0001, Grad Norm=0.0045\n","Epoch 5, Batch 66: Loss=0.0215, Grad Norm=0.7222\n","Epoch 5, Batch 67: Loss=0.0002, Grad Norm=0.0057\n","Epoch 5, Batch 68: Loss=0.0007, Grad Norm=0.0250\n","Epoch 5, Batch 69: Loss=0.0000, Grad Norm=0.0004\n","Epoch 5, Batch 70: Loss=0.0010, Grad Norm=0.0372\n","Epoch 5, Batch 71: Loss=0.0001, Grad Norm=0.0029\n","Epoch 5, Batch 72: Loss=0.0157, Grad Norm=0.4652\n","Epoch 5, Batch 73: Loss=0.0260, Grad Norm=0.7587\n","Epoch 5, Batch 74: Loss=0.0569, Grad Norm=2.5079\n","Epoch 5, Batch 75: Loss=0.0007, Grad Norm=0.0266\n","Epoch 5, Batch 76: Loss=0.0088, Grad Norm=0.3594\n","Epoch 5, Batch 77: Loss=0.0931, Grad Norm=1.8904\n","Epoch 5, Batch 78: Loss=0.0013, Grad Norm=0.0366\n","Epoch 5, Batch 79: Loss=0.0527, Grad Norm=1.1228\n","Epoch 5, Batch 80: Loss=0.0025, Grad Norm=0.0734\n","Epoch 5, Batch 81: Loss=0.0008, Grad Norm=0.0242\n","Epoch 5, Batch 82: Loss=0.0000, Grad Norm=0.0013\n","Epoch 5, Batch 83: Loss=0.0001, Grad Norm=0.0014\n","Epoch 5, Batch 84: Loss=0.0005, Grad Norm=0.0124\n","Epoch 5, Batch 85: Loss=0.0004, Grad Norm=0.0071\n","Epoch 5, Batch 86: Loss=0.0001, Grad Norm=0.0029\n","Epoch 5, Batch 87: Loss=0.0008, Grad Norm=0.0285\n","Epoch 5, Batch 88: Loss=0.0108, Grad Norm=0.3090\n","Epoch 5, Batch 89: Loss=0.0002, Grad Norm=0.0057\n","Epoch 5, Batch 90: Loss=0.0004, Grad Norm=0.0107\n","Epoch 5, Batch 91: Loss=0.0008, Grad Norm=0.0248\n","Epoch 5, Batch 92: Loss=0.0026, Grad Norm=0.0716\n","Epoch 5, Batch 93: Loss=0.0002, Grad Norm=0.0043\n","Epoch 5, Batch 94: Loss=0.0035, Grad Norm=0.1057\n","Epoch 5, Batch 95: Loss=0.0003, Grad Norm=0.0080\n","Epoch 5, Batch 96: Loss=0.0001, Grad Norm=0.0023\n","Epoch 5, Batch 97: Loss=0.0018, Grad Norm=0.0844\n","Epoch 5, Batch 98: Loss=0.0003, Grad Norm=0.0116\n","Epoch 5, Batch 99: Loss=0.0236, Grad Norm=0.6535\n","Epoch 5, Batch 100: Loss=0.0040, Grad Norm=0.1190\n","Epoch 5, Batch 101: Loss=0.0024, Grad Norm=0.0707\n","Epoch 5, Batch 102: Loss=0.0139, Grad Norm=0.4813\n","Epoch 5, Batch 103: Loss=0.0057, Grad Norm=0.3019\n","Epoch 5, Batch 104: Loss=0.0002, Grad Norm=0.0063\n","Epoch 5, Batch 105: Loss=0.0001, Grad Norm=0.0033\n","Epoch 5, Batch 106: Loss=0.0012, Grad Norm=0.0324\n","Epoch 5, Batch 107: Loss=0.1906, Grad Norm=4.4268\n","Epoch 5, Batch 108: Loss=0.0000, Grad Norm=0.0008\n","Epoch 5, Batch 109: Loss=0.0013, Grad Norm=0.0311\n","Epoch 5, Batch 110: Loss=0.0009, Grad Norm=0.0306\n","Epoch 5, Batch 111: Loss=0.0003, Grad Norm=0.0076\n","Epoch 5, Batch 112: Loss=0.0041, Grad Norm=0.0983\n","Epoch 5, Batch 113: Loss=0.0034, Grad Norm=0.1345\n","Epoch 5, Batch 114: Loss=0.0002, Grad Norm=0.0075\n","Epoch 5, Batch 115: Loss=0.0008, Grad Norm=0.0222\n","Epoch 5, Batch 116: Loss=0.0011, Grad Norm=0.0316\n","Epoch 5, Batch 117: Loss=0.0004, Grad Norm=0.0093\n","Epoch 5, Batch 118: Loss=0.0032, Grad Norm=0.1631\n","Epoch 5, Batch 119: Loss=0.0012, Grad Norm=0.0597\n","Epoch 5, Batch 120: Loss=0.0449, Grad Norm=0.8321\n","Epoch 5, Batch 121: Loss=0.0009, Grad Norm=0.0269\n","Epoch 5, Batch 122: Loss=0.0003, Grad Norm=0.0081\n","Epoch 5, Batch 123: Loss=0.0005, Grad Norm=0.0210\n","Epoch 5, Batch 124: Loss=0.0002, Grad Norm=0.0075\n","Epoch 5, Batch 125: Loss=0.0005, Grad Norm=0.0141\n","Epoch 5, Batch 126: Loss=0.0043, Grad Norm=0.1397\n","Epoch 5, Batch 127: Loss=0.0045, Grad Norm=0.1734\n","Epoch 5, Batch 128: Loss=0.0715, Grad Norm=1.4457\n","Epoch 5, Batch 129: Loss=0.0003, Grad Norm=0.0138\n","Epoch 5, Batch 130: Loss=0.0013, Grad Norm=0.0553\n","Epoch 5, Batch 131: Loss=0.0007, Grad Norm=0.0216\n","Epoch 5, Batch 132: Loss=0.0001, Grad Norm=0.0027\n","Epoch 5, Batch 133: Loss=0.0057, Grad Norm=0.2616\n","Epoch 5, Batch 134: Loss=0.0035, Grad Norm=0.1076\n","Epoch 5, Batch 135: Loss=0.0003, Grad Norm=0.0067\n","Epoch 5, Batch 136: Loss=0.0053, Grad Norm=0.2130\n","Epoch 5, Batch 137: Loss=0.0085, Grad Norm=0.2045\n","Epoch 5, Batch 138: Loss=0.0004, Grad Norm=0.0085\n","Epoch 5, Batch 139: Loss=0.0000, Grad Norm=0.0006\n","Epoch 5, Batch 140: Loss=0.0009, Grad Norm=0.0224\n","Epoch 5, Batch 141: Loss=0.0092, Grad Norm=0.2255\n","Epoch 5, Batch 142: Loss=0.0005, Grad Norm=0.0177\n","Epoch 5, Batch 143: Loss=0.0002, Grad Norm=0.0047\n","Epoch 5, Batch 144: Loss=0.1040, Grad Norm=3.1038\n","Epoch 5, Batch 145: Loss=0.0011, Grad Norm=0.0226\n","Epoch 5, Batch 146: Loss=0.0001, Grad Norm=0.0039\n","Epoch 5, Batch 147: Loss=0.0002, Grad Norm=0.0059\n","Epoch 5, Batch 148: Loss=0.0001, Grad Norm=0.0022\n","Epoch 5, Batch 149: Loss=0.0003, Grad Norm=0.0080\n","Epoch 5, Batch 150: Loss=0.0106, Grad Norm=0.4129\n","Epoch 5, Batch 151: Loss=0.0005, Grad Norm=0.0151\n","Epoch 5, Batch 152: Loss=0.0001, Grad Norm=0.0024\n","Epoch 5, Batch 153: Loss=0.0084, Grad Norm=0.1993\n","Epoch 5, Batch 154: Loss=0.0032, Grad Norm=0.1222\n","Epoch 5, Batch 155: Loss=0.0006, Grad Norm=0.0231\n","Epoch 5, Batch 156: Loss=0.0004, Grad Norm=0.0123\n","Epoch 5, Batch 157: Loss=0.0049, Grad Norm=0.1472\n","Epoch 5, Batch 158: Loss=0.0002, Grad Norm=0.0070\n","Epoch 5, Batch 159: Loss=0.0010, Grad Norm=0.0326\n","Epoch 5, Batch 160: Loss=0.0041, Grad Norm=0.1620\n","Epoch 5, Batch 161: Loss=0.0004, Grad Norm=0.0080\n","Epoch 5, Batch 162: Loss=0.0533, Grad Norm=1.7168\n","Epoch 5, Batch 163: Loss=0.0022, Grad Norm=0.0527\n","Epoch 5, Batch 164: Loss=0.0002, Grad Norm=0.0095\n","Epoch 5, Batch 165: Loss=0.1153, Grad Norm=2.7320\n","Epoch 5, Batch 166: Loss=0.0022, Grad Norm=0.0854\n","Epoch 5, Batch 167: Loss=0.0242, Grad Norm=0.8476\n","Epoch 5, Batch 168: Loss=0.0096, Grad Norm=0.2698\n","Epoch 5, Batch 169: Loss=0.0001, Grad Norm=0.0013\n","Epoch 5, Batch 170: Loss=0.0018, Grad Norm=0.0754\n","Epoch 5, Batch 171: Loss=0.0016, Grad Norm=0.0425\n","Epoch 5, Batch 172: Loss=0.0000, Grad Norm=0.0006\n","Epoch 5, Batch 173: Loss=0.0011, Grad Norm=0.0286\n","Epoch 5, Batch 174: Loss=0.0021, Grad Norm=0.1543\n","Epoch 5, Batch 175: Loss=0.0004, Grad Norm=0.0091\n","Epoch 5, Batch 176: Loss=0.0001, Grad Norm=0.0030\n","Epoch 5, Batch 177: Loss=0.0003, Grad Norm=0.0063\n","Epoch 5, Batch 178: Loss=0.0025, Grad Norm=0.0627\n","Epoch 5, Batch 179: Loss=0.0007, Grad Norm=0.0187\n","Epoch 5, Batch 180: Loss=0.0003, Grad Norm=0.0085\n","Epoch 5, Batch 181: Loss=0.0002, Grad Norm=0.0091\n","Epoch 5, Batch 182: Loss=0.0020, Grad Norm=0.0536\n","Epoch 5, Batch 183: Loss=0.0404, Grad Norm=2.5541\n","Epoch 5, Batch 184: Loss=0.0039, Grad Norm=0.1479\n","Epoch 5, Batch 185: Loss=0.0023, Grad Norm=0.0660\n","Epoch 5, Batch 186: Loss=0.0004, Grad Norm=0.0114\n","Epoch 5, Batch 187: Loss=0.0406, Grad Norm=1.3392\n","Epoch 5, Batch 188: Loss=0.0023, Grad Norm=0.1038\n","Epoch 5/15:\n","Train Loss: 0.0099, Train Accuracy: 99.60%\n","Val Loss: 0.0095, Val Accuracy: 99.67%\n","Epoch 6, Batch 1: Loss=0.0005, Grad Norm=0.0130\n","Epoch 6, Batch 2: Loss=0.0001, Grad Norm=0.0020\n","Epoch 6, Batch 3: Loss=0.0023, Grad Norm=0.0813\n","Epoch 6, Batch 4: Loss=0.0036, Grad Norm=0.1096\n","Epoch 6, Batch 5: Loss=0.0001, Grad Norm=0.0011\n","Epoch 6, Batch 6: Loss=0.0011, Grad Norm=0.0286\n","Epoch 6, Batch 7: Loss=0.0019, Grad Norm=0.0741\n","Epoch 6, Batch 8: Loss=0.0001, Grad Norm=0.0029\n","Epoch 6, Batch 9: Loss=0.0011, Grad Norm=0.0362\n","Epoch 6, Batch 10: Loss=0.0325, Grad Norm=0.9362\n","Epoch 6, Batch 11: Loss=0.0001, Grad Norm=0.0020\n","Epoch 6, Batch 12: Loss=0.0001, Grad Norm=0.0046\n","Epoch 6, Batch 13: Loss=0.0013, Grad Norm=0.0335\n","Epoch 6, Batch 14: Loss=0.0003, Grad Norm=0.0067\n","Epoch 6, Batch 15: Loss=0.1014, Grad Norm=2.0577\n","Epoch 6, Batch 16: Loss=0.0002, Grad Norm=0.0034\n","Epoch 6, Batch 17: Loss=0.0007, Grad Norm=0.0258\n","Epoch 6, Batch 18: Loss=0.0005, Grad Norm=0.0126\n","Epoch 6, Batch 19: Loss=0.0014, Grad Norm=0.0308\n","Epoch 6, Batch 20: Loss=0.0002, Grad Norm=0.0054\n","Epoch 6, Batch 21: Loss=0.0018, Grad Norm=0.0535\n","Epoch 6, Batch 22: Loss=0.0001, Grad Norm=0.0038\n","Epoch 6, Batch 23: Loss=0.0003, Grad Norm=0.0092\n","Epoch 6, Batch 24: Loss=0.0012, Grad Norm=0.0533\n","Epoch 6, Batch 25: Loss=0.0037, Grad Norm=0.0847\n","Epoch 6, Batch 26: Loss=0.0519, Grad Norm=1.2868\n","Epoch 6, Batch 27: Loss=0.0011, Grad Norm=0.0235\n","Epoch 6, Batch 28: Loss=0.0000, Grad Norm=0.0009\n","Epoch 6, Batch 29: Loss=0.0028, Grad Norm=0.0568\n","Epoch 6, Batch 30: Loss=0.0003, Grad Norm=0.0103\n","Epoch 6, Batch 31: Loss=0.0003, Grad Norm=0.0066\n","Epoch 6, Batch 32: Loss=0.0000, Grad Norm=0.0004\n","Epoch 6, Batch 33: Loss=0.0059, Grad Norm=0.3258\n","Epoch 6, Batch 34: Loss=0.0024, Grad Norm=0.0651\n","Epoch 6, Batch 35: Loss=0.0134, Grad Norm=0.5141\n","Epoch 6, Batch 36: Loss=0.0002, Grad Norm=0.0065\n","Epoch 6, Batch 37: Loss=0.0004, Grad Norm=0.0095\n","Epoch 6, Batch 38: Loss=0.0015, Grad Norm=0.0349\n","Epoch 6, Batch 39: Loss=0.0012, Grad Norm=0.0292\n","Epoch 6, Batch 40: Loss=0.0007, Grad Norm=0.0160\n","Epoch 6, Batch 41: Loss=0.0010, Grad Norm=0.0428\n","Epoch 6, Batch 42: Loss=0.0026, Grad Norm=0.0729\n","Epoch 6, Batch 43: Loss=0.0006, Grad Norm=0.0144\n","Epoch 6, Batch 44: Loss=0.0001, Grad Norm=0.0011\n","Epoch 6, Batch 45: Loss=0.0002, Grad Norm=0.0051\n","Epoch 6, Batch 46: Loss=0.0006, Grad Norm=0.0178\n","Epoch 6, Batch 47: Loss=0.0000, Grad Norm=0.0011\n","Epoch 6, Batch 48: Loss=0.0004, Grad Norm=0.0123\n","Epoch 6, Batch 49: Loss=0.0001, Grad Norm=0.0037\n","Epoch 6, Batch 50: Loss=0.0023, Grad Norm=0.0555\n","Epoch 6, Batch 51: Loss=0.0001, Grad Norm=0.0030\n","Epoch 6, Batch 52: Loss=0.0424, Grad Norm=1.3459\n","Epoch 6, Batch 53: Loss=0.0271, Grad Norm=0.9524\n","Epoch 6, Batch 54: Loss=0.0002, Grad Norm=0.0068\n","Epoch 6, Batch 55: Loss=0.0044, Grad Norm=0.0821\n","Epoch 6, Batch 56: Loss=0.0003, Grad Norm=0.0095\n","Epoch 6, Batch 57: Loss=0.0098, Grad Norm=0.2744\n","Epoch 6, Batch 58: Loss=0.0005, Grad Norm=0.0095\n","Epoch 6, Batch 59: Loss=0.0001, Grad Norm=0.0025\n","Epoch 6, Batch 60: Loss=0.0029, Grad Norm=0.1008\n","Epoch 6, Batch 61: Loss=0.0001, Grad Norm=0.0063\n","Epoch 6, Batch 62: Loss=0.0300, Grad Norm=0.7246\n","Epoch 6, Batch 63: Loss=0.0011, Grad Norm=0.0332\n","Epoch 6, Batch 64: Loss=0.0004, Grad Norm=0.0114\n","Epoch 6, Batch 65: Loss=0.0007, Grad Norm=0.0248\n","Epoch 6, Batch 66: Loss=0.0006, Grad Norm=0.0147\n","Epoch 6, Batch 67: Loss=0.0001, Grad Norm=0.0019\n","Epoch 6, Batch 68: Loss=0.0171, Grad Norm=0.4860\n","Epoch 6, Batch 69: Loss=0.0656, Grad Norm=1.4395\n","Epoch 6, Batch 70: Loss=0.0004, Grad Norm=0.0133\n","Epoch 6, Batch 71: Loss=0.0005, Grad Norm=0.0160\n","Epoch 6, Batch 72: Loss=0.0002, Grad Norm=0.0069\n","Epoch 6, Batch 73: Loss=0.0440, Grad Norm=1.3686\n","Epoch 6, Batch 74: Loss=0.0018, Grad Norm=0.0457\n","Epoch 6, Batch 75: Loss=0.0246, Grad Norm=0.8363\n","Epoch 6, Batch 76: Loss=0.0001, Grad Norm=0.0029\n","Epoch 6, Batch 77: Loss=0.0028, Grad Norm=0.0843\n","Epoch 6, Batch 78: Loss=0.0100, Grad Norm=0.2686\n","Epoch 6, Batch 79: Loss=0.0007, Grad Norm=0.0201\n","Epoch 6, Batch 80: Loss=0.0004, Grad Norm=0.0099\n","Epoch 6, Batch 81: Loss=0.0000, Grad Norm=0.0009\n","Epoch 6, Batch 82: Loss=0.0075, Grad Norm=0.1480\n","Epoch 6, Batch 83: Loss=0.0012, Grad Norm=0.0451\n","Epoch 6, Batch 84: Loss=0.0001, Grad Norm=0.0042\n","Epoch 6, Batch 85: Loss=0.0006, Grad Norm=0.0188\n","Epoch 6, Batch 86: Loss=0.0002, Grad Norm=0.0053\n","Epoch 6, Batch 87: Loss=0.0010, Grad Norm=0.0469\n","Epoch 6, Batch 88: Loss=0.0016, Grad Norm=0.0431\n","Epoch 6, Batch 89: Loss=0.0011, Grad Norm=0.0250\n","Epoch 6, Batch 90: Loss=0.0010, Grad Norm=0.0297\n","Epoch 6, Batch 91: Loss=0.0002, Grad Norm=0.0050\n","Epoch 6, Batch 92: Loss=0.0002, Grad Norm=0.0051\n","Epoch 6, Batch 93: Loss=0.0009, Grad Norm=0.0330\n","Epoch 6, Batch 94: Loss=0.0005, Grad Norm=0.0144\n","Epoch 6, Batch 95: Loss=0.0141, Grad Norm=0.3304\n","Epoch 6, Batch 96: Loss=0.0005, Grad Norm=0.0186\n","Epoch 6, Batch 97: Loss=0.0035, Grad Norm=0.0823\n","Epoch 6, Batch 98: Loss=0.0000, Grad Norm=0.0012\n","Epoch 6, Batch 99: Loss=0.0000, Grad Norm=0.0008\n","Epoch 6, Batch 100: Loss=0.0007, Grad Norm=0.0170\n","Epoch 6, Batch 101: Loss=0.0014, Grad Norm=0.0340\n","Epoch 6, Batch 102: Loss=0.0118, Grad Norm=0.4573\n","Epoch 6, Batch 103: Loss=0.0011, Grad Norm=0.0231\n","Epoch 6, Batch 104: Loss=0.0008, Grad Norm=0.0349\n","Epoch 6, Batch 105: Loss=0.0057, Grad Norm=0.2850\n","Epoch 6, Batch 106: Loss=0.0010, Grad Norm=0.0261\n","Epoch 6, Batch 107: Loss=0.0009, Grad Norm=0.0229\n","Epoch 6, Batch 108: Loss=0.0001, Grad Norm=0.0012\n","Epoch 6, Batch 109: Loss=0.0001, Grad Norm=0.0020\n","Epoch 6, Batch 110: Loss=0.0007, Grad Norm=0.0194\n","Epoch 6, Batch 111: Loss=0.0004, Grad Norm=0.0121\n","Epoch 6, Batch 112: Loss=0.0004, Grad Norm=0.0113\n","Epoch 6, Batch 113: Loss=0.0003, Grad Norm=0.0094\n","Epoch 6, Batch 114: Loss=0.0010, Grad Norm=0.0245\n","Epoch 6, Batch 115: Loss=0.0030, Grad Norm=0.0794\n","Epoch 6, Batch 116: Loss=0.0023, Grad Norm=0.0534\n","Epoch 6, Batch 117: Loss=0.0009, Grad Norm=0.0353\n","Epoch 6, Batch 118: Loss=0.0008, Grad Norm=0.0240\n","Epoch 6, Batch 119: Loss=0.0007, Grad Norm=0.0139\n","Epoch 6, Batch 120: Loss=0.0084, Grad Norm=0.2999\n","Epoch 6, Batch 121: Loss=0.0024, Grad Norm=0.0656\n","Epoch 6, Batch 122: Loss=0.0004, Grad Norm=0.0097\n","Epoch 6, Batch 123: Loss=0.0149, Grad Norm=0.5803\n","Epoch 6, Batch 124: Loss=0.0058, Grad Norm=0.2861\n","Epoch 6, Batch 125: Loss=0.0003, Grad Norm=0.0070\n","Epoch 6, Batch 126: Loss=0.0002, Grad Norm=0.0033\n","Epoch 6, Batch 127: Loss=0.0002, Grad Norm=0.0071\n","Epoch 6, Batch 128: Loss=0.0012, Grad Norm=0.0679\n","Epoch 6, Batch 129: Loss=0.0001, Grad Norm=0.0016\n","Epoch 6, Batch 130: Loss=0.0011, Grad Norm=0.0577\n","Epoch 6, Batch 131: Loss=0.0002, Grad Norm=0.0049\n","Epoch 6, Batch 132: Loss=0.0025, Grad Norm=0.0890\n","Epoch 6, Batch 133: Loss=0.0003, Grad Norm=0.0058\n","Epoch 6, Batch 134: Loss=0.0000, Grad Norm=0.0013\n","Epoch 6, Batch 135: Loss=0.0003, Grad Norm=0.0118\n","Epoch 6, Batch 136: Loss=0.0032, Grad Norm=0.1649\n","Epoch 6, Batch 137: Loss=0.0010, Grad Norm=0.0212\n","Epoch 6, Batch 138: Loss=0.0295, Grad Norm=1.1940\n","Epoch 6, Batch 139: Loss=0.0059, Grad Norm=0.1600\n","Epoch 6, Batch 140: Loss=0.0003, Grad Norm=0.0117\n","Epoch 6, Batch 141: Loss=0.0024, Grad Norm=0.0534\n","Epoch 6, Batch 142: Loss=0.0012, Grad Norm=0.0243\n","Epoch 6, Batch 143: Loss=0.0009, Grad Norm=0.0223\n","Epoch 6, Batch 144: Loss=0.0056, Grad Norm=0.1600\n","Epoch 6, Batch 145: Loss=0.0006, Grad Norm=0.0177\n","Epoch 6, Batch 146: Loss=0.0202, Grad Norm=0.7760\n","Epoch 6, Batch 147: Loss=0.0026, Grad Norm=0.0952\n","Epoch 6, Batch 148: Loss=0.0011, Grad Norm=0.0262\n","Epoch 6, Batch 149: Loss=0.4032, Grad Norm=3.2209\n","Epoch 6, Batch 150: Loss=0.0002, Grad Norm=0.0037\n","Epoch 6, Batch 151: Loss=0.0751, Grad Norm=2.8750\n","Epoch 6, Batch 152: Loss=0.0001, Grad Norm=0.0012\n","Epoch 6, Batch 153: Loss=0.0001, Grad Norm=0.0015\n","Epoch 6, Batch 154: Loss=0.1172, Grad Norm=2.2386\n","Epoch 6, Batch 155: Loss=0.0022, Grad Norm=0.0776\n","Epoch 6, Batch 156: Loss=0.0007, Grad Norm=0.0223\n","Epoch 6, Batch 157: Loss=0.0012, Grad Norm=0.0514\n","Epoch 6, Batch 158: Loss=0.0007, Grad Norm=0.0163\n","Epoch 6, Batch 159: Loss=0.0028, Grad Norm=0.0850\n","Epoch 6, Batch 160: Loss=0.0008, Grad Norm=0.0182\n","Epoch 6, Batch 161: Loss=0.0004, Grad Norm=0.0084\n","Epoch 6, Batch 162: Loss=0.0004, Grad Norm=0.0128\n","Epoch 6, Batch 163: Loss=0.0013, Grad Norm=0.0318\n","Epoch 6, Batch 164: Loss=0.0095, Grad Norm=0.4646\n","Epoch 6, Batch 165: Loss=0.0014, Grad Norm=0.0538\n","Epoch 6, Batch 166: Loss=0.0414, Grad Norm=0.9280\n","Epoch 6, Batch 167: Loss=0.0765, Grad Norm=1.6786\n","Epoch 6, Batch 168: Loss=0.0013, Grad Norm=0.0417\n","Epoch 6, Batch 169: Loss=0.0002, Grad Norm=0.0049\n","Epoch 6, Batch 170: Loss=0.0006, Grad Norm=0.0349\n","Epoch 6, Batch 171: Loss=0.0028, Grad Norm=0.0708\n","Epoch 6, Batch 172: Loss=0.0008, Grad Norm=0.0210\n","Epoch 6, Batch 173: Loss=0.0005, Grad Norm=0.0108\n","Epoch 6, Batch 174: Loss=0.2608, Grad Norm=5.2490\n","Epoch 6, Batch 175: Loss=0.0016, Grad Norm=0.0405\n","Epoch 6, Batch 176: Loss=0.0231, Grad Norm=0.5735\n","Epoch 6, Batch 177: Loss=0.0006, Grad Norm=0.0233\n","Epoch 6, Batch 178: Loss=0.0002, Grad Norm=0.0053\n","Epoch 6, Batch 179: Loss=0.0004, Grad Norm=0.0082\n","Epoch 6, Batch 180: Loss=0.0242, Grad Norm=1.2461\n","Epoch 6, Batch 181: Loss=0.0002, Grad Norm=0.0073\n","Epoch 6, Batch 182: Loss=0.0018, Grad Norm=0.0603\n","Epoch 6, Batch 183: Loss=0.0228, Grad Norm=0.8811\n","Epoch 6, Batch 184: Loss=0.0001, Grad Norm=0.0028\n","Epoch 6, Batch 185: Loss=0.0006, Grad Norm=0.0165\n","Epoch 6, Batch 186: Loss=0.0021, Grad Norm=0.0930\n","Epoch 6, Batch 187: Loss=0.0035, Grad Norm=0.0938\n","Epoch 6, Batch 188: Loss=0.0487, Grad Norm=0.9954\n","Epoch 6/15:\n","Train Loss: 0.0098, Train Accuracy: 99.80%\n","Val Loss: 0.0074, Val Accuracy: 99.83%\n","Epoch 7, Batch 1: Loss=0.0054, Grad Norm=0.1656\n","Epoch 7, Batch 2: Loss=0.0004, Grad Norm=0.0094\n","Epoch 7, Batch 3: Loss=0.0010, Grad Norm=0.0281\n","Epoch 7, Batch 4: Loss=0.0005, Grad Norm=0.0219\n","Epoch 7, Batch 5: Loss=0.0001, Grad Norm=0.0011\n","Epoch 7, Batch 6: Loss=0.0001, Grad Norm=0.0017\n","Epoch 7, Batch 7: Loss=0.0014, Grad Norm=0.0301\n","Epoch 7, Batch 8: Loss=0.0057, Grad Norm=0.1824\n","Epoch 7, Batch 9: Loss=0.0001, Grad Norm=0.0036\n","Epoch 7, Batch 10: Loss=0.0003, Grad Norm=0.0117\n","Epoch 7, Batch 11: Loss=0.0001, Grad Norm=0.0026\n","Epoch 7, Batch 12: Loss=0.0781, Grad Norm=2.0315\n","Epoch 7, Batch 13: Loss=0.0000, Grad Norm=0.0009\n","Epoch 7, Batch 14: Loss=0.0001, Grad Norm=0.0035\n","Epoch 7, Batch 15: Loss=0.0015, Grad Norm=0.0463\n","Epoch 7, Batch 16: Loss=0.0010, Grad Norm=0.0302\n","Epoch 7, Batch 17: Loss=0.0015, Grad Norm=0.0298\n","Epoch 7, Batch 18: Loss=0.0002, Grad Norm=0.0059\n","Epoch 7, Batch 19: Loss=0.0006, Grad Norm=0.0119\n","Epoch 7, Batch 20: Loss=0.0670, Grad Norm=2.0888\n","Epoch 7, Batch 21: Loss=0.0001, Grad Norm=0.0027\n","Epoch 7, Batch 22: Loss=0.0015, Grad Norm=0.0420\n","Epoch 7, Batch 23: Loss=0.0001, Grad Norm=0.0026\n","Epoch 7, Batch 24: Loss=0.0005, Grad Norm=0.0178\n","Epoch 7, Batch 25: Loss=0.0010, Grad Norm=0.0306\n","Epoch 7, Batch 26: Loss=0.0002, Grad Norm=0.0046\n","Epoch 7, Batch 27: Loss=0.0011, Grad Norm=0.0308\n","Epoch 7, Batch 28: Loss=0.0001, Grad Norm=0.0018\n","Epoch 7, Batch 29: Loss=0.0015, Grad Norm=0.0446\n","Epoch 7, Batch 30: Loss=0.0000, Grad Norm=0.0006\n","Epoch 7, Batch 31: Loss=0.0001, Grad Norm=0.0033\n","Epoch 7, Batch 32: Loss=0.0004, Grad Norm=0.0098\n","Epoch 7, Batch 33: Loss=0.0002, Grad Norm=0.0046\n","Epoch 7, Batch 34: Loss=0.0006, Grad Norm=0.0156\n","Epoch 7, Batch 35: Loss=0.0001, Grad Norm=0.0016\n","Epoch 7, Batch 36: Loss=0.0001, Grad Norm=0.0011\n","Epoch 7, Batch 37: Loss=0.0019, Grad Norm=0.0631\n","Epoch 7, Batch 38: Loss=0.0005, Grad Norm=0.0087\n","Epoch 7, Batch 39: Loss=0.0001, Grad Norm=0.0026\n","Epoch 7, Batch 40: Loss=0.0007, Grad Norm=0.0426\n","Epoch 7, Batch 41: Loss=0.0006, Grad Norm=0.0229\n","Epoch 7, Batch 42: Loss=0.0004, Grad Norm=0.0123\n","Epoch 7, Batch 43: Loss=0.0007, Grad Norm=0.0188\n","Epoch 7, Batch 44: Loss=0.0035, Grad Norm=0.0817\n","Epoch 7, Batch 45: Loss=0.0007, Grad Norm=0.0284\n","Epoch 7, Batch 46: Loss=0.0005, Grad Norm=0.0226\n","Epoch 7, Batch 47: Loss=0.0092, Grad Norm=0.2820\n","Epoch 7, Batch 48: Loss=0.0002, Grad Norm=0.0037\n","Epoch 7, Batch 49: Loss=0.3517, Grad Norm=4.7596\n","Epoch 7, Batch 50: Loss=0.0012, Grad Norm=0.0300\n","Epoch 7, Batch 51: Loss=0.0001, Grad Norm=0.0032\n","Epoch 7, Batch 52: Loss=0.1144, Grad Norm=4.0804\n","Epoch 7, Batch 53: Loss=0.0144, Grad Norm=0.4370\n","Epoch 7, Batch 54: Loss=0.0001, Grad Norm=0.0034\n","Epoch 7, Batch 55: Loss=0.0000, Grad Norm=0.0009\n","Epoch 7, Batch 56: Loss=0.0001, Grad Norm=0.0017\n","Epoch 7, Batch 57: Loss=0.0010, Grad Norm=0.0371\n","Epoch 7, Batch 58: Loss=0.0006, Grad Norm=0.0210\n","Epoch 7, Batch 59: Loss=0.0003, Grad Norm=0.0131\n","Epoch 7, Batch 60: Loss=0.0001, Grad Norm=0.0022\n","Epoch 7, Batch 61: Loss=0.0014, Grad Norm=0.0280\n","Epoch 7, Batch 62: Loss=0.0029, Grad Norm=0.1009\n","Epoch 7, Batch 63: Loss=0.0003, Grad Norm=0.0079\n","Epoch 7, Batch 64: Loss=0.0006, Grad Norm=0.0149\n","Epoch 7, Batch 65: Loss=0.0013, Grad Norm=0.0591\n","Epoch 7, Batch 66: Loss=0.0001, Grad Norm=0.0022\n","Epoch 7, Batch 67: Loss=0.0001, Grad Norm=0.0033\n","Epoch 7, Batch 68: Loss=0.0023, Grad Norm=0.0622\n","Epoch 7, Batch 69: Loss=0.0040, Grad Norm=0.1517\n","Epoch 7, Batch 70: Loss=0.0009, Grad Norm=0.0195\n","Epoch 7, Batch 71: Loss=0.0005, Grad Norm=0.0164\n","Epoch 7, Batch 72: Loss=0.2728, Grad Norm=4.1356\n","Epoch 7, Batch 73: Loss=0.0025, Grad Norm=0.0584\n","Epoch 7, Batch 74: Loss=0.0002, Grad Norm=0.0065\n","Epoch 7, Batch 75: Loss=0.1107, Grad Norm=2.6550\n","Epoch 7, Batch 76: Loss=0.0002, Grad Norm=0.0044\n","Epoch 7, Batch 77: Loss=0.0008, Grad Norm=0.0335\n","Epoch 7, Batch 78: Loss=0.0001, Grad Norm=0.0032\n","Epoch 7, Batch 79: Loss=0.0028, Grad Norm=0.0748\n","Epoch 7, Batch 80: Loss=0.0002, Grad Norm=0.0062\n","Epoch 7, Batch 81: Loss=0.0024, Grad Norm=0.0494\n","Epoch 7, Batch 82: Loss=0.0004, Grad Norm=0.0143\n","Epoch 7, Batch 83: Loss=0.0003, Grad Norm=0.0092\n","Epoch 7, Batch 84: Loss=0.0015, Grad Norm=0.0430\n","Epoch 7, Batch 85: Loss=0.0001, Grad Norm=0.0019\n","Epoch 7, Batch 86: Loss=0.0001, Grad Norm=0.0042\n","Epoch 7, Batch 87: Loss=0.0007, Grad Norm=0.0165\n","Epoch 7, Batch 88: Loss=0.0006, Grad Norm=0.0185\n","Epoch 7, Batch 89: Loss=0.0028, Grad Norm=0.1152\n","Epoch 7, Batch 90: Loss=0.0001, Grad Norm=0.0031\n","Epoch 7, Batch 91: Loss=0.0001, Grad Norm=0.0019\n","Epoch 7, Batch 92: Loss=0.0017, Grad Norm=0.0436\n","Epoch 7, Batch 93: Loss=0.0008, Grad Norm=0.0357\n","Epoch 7, Batch 94: Loss=0.0005, Grad Norm=0.0111\n","Epoch 7, Batch 95: Loss=0.0027, Grad Norm=0.0828\n","Epoch 7, Batch 96: Loss=0.0000, Grad Norm=0.0006\n","Epoch 7, Batch 97: Loss=0.0097, Grad Norm=0.2676\n","Epoch 7, Batch 98: Loss=0.0063, Grad Norm=0.2007\n","Epoch 7, Batch 99: Loss=0.0005, Grad Norm=0.0209\n","Epoch 7, Batch 100: Loss=0.0005, Grad Norm=0.0146\n","Epoch 7, Batch 101: Loss=0.0026, Grad Norm=0.0868\n","Epoch 7, Batch 102: Loss=0.0013, Grad Norm=0.0412\n","Epoch 7, Batch 103: Loss=0.0057, Grad Norm=0.2920\n","Epoch 7, Batch 104: Loss=0.0024, Grad Norm=0.1592\n","Epoch 7, Batch 105: Loss=0.0497, Grad Norm=1.4203\n","Epoch 7, Batch 106: Loss=0.0178, Grad Norm=0.5220\n","Epoch 7, Batch 107: Loss=0.0133, Grad Norm=0.3095\n","Epoch 7, Batch 108: Loss=0.0039, Grad Norm=0.1218\n","Epoch 7, Batch 109: Loss=0.0001, Grad Norm=0.0040\n","Epoch 7, Batch 110: Loss=0.0067, Grad Norm=0.2463\n","Epoch 7, Batch 111: Loss=0.0001, Grad Norm=0.0013\n","Epoch 7, Batch 112: Loss=0.0001, Grad Norm=0.0035\n","Epoch 7, Batch 113: Loss=0.0007, Grad Norm=0.0183\n","Epoch 7, Batch 114: Loss=0.0002, Grad Norm=0.0046\n","Epoch 7, Batch 115: Loss=0.0006, Grad Norm=0.0197\n","Epoch 7, Batch 116: Loss=0.0058, Grad Norm=0.1428\n","Epoch 7, Batch 117: Loss=0.0001, Grad Norm=0.0032\n","Epoch 7, Batch 118: Loss=0.0005, Grad Norm=0.0259\n","Epoch 7, Batch 119: Loss=0.0075, Grad Norm=0.2133\n","Epoch 7, Batch 120: Loss=0.0008, Grad Norm=0.0281\n","Epoch 7, Batch 121: Loss=0.0023, Grad Norm=0.0754\n","Epoch 7, Batch 122: Loss=0.0008, Grad Norm=0.0214\n","Epoch 7, Batch 123: Loss=0.0004, Grad Norm=0.0166\n","Epoch 7, Batch 124: Loss=0.0001, Grad Norm=0.0017\n","Epoch 7, Batch 125: Loss=0.0009, Grad Norm=0.0328\n","Epoch 7, Batch 126: Loss=0.0343, Grad Norm=1.5977\n","Epoch 7, Batch 127: Loss=0.0006, Grad Norm=0.0136\n","Epoch 7, Batch 128: Loss=0.0006, Grad Norm=0.0103\n","Epoch 7, Batch 129: Loss=0.0005, Grad Norm=0.0142\n","Epoch 7, Batch 130: Loss=0.0007, Grad Norm=0.0158\n","Epoch 7, Batch 131: Loss=0.0007, Grad Norm=0.0292\n","Epoch 7, Batch 132: Loss=0.0005, Grad Norm=0.0177\n","Epoch 7, Batch 133: Loss=0.0010, Grad Norm=0.0366\n","Epoch 7, Batch 134: Loss=0.0004, Grad Norm=0.0147\n","Epoch 7, Batch 135: Loss=0.0000, Grad Norm=0.0015\n","Epoch 7, Batch 136: Loss=0.0470, Grad Norm=1.3627\n","Epoch 7, Batch 137: Loss=0.0033, Grad Norm=0.0887\n","Epoch 7, Batch 138: Loss=0.0004, Grad Norm=0.0105\n","Epoch 7, Batch 139: Loss=0.0005, Grad Norm=0.0214\n","Epoch 7, Batch 140: Loss=0.0010, Grad Norm=0.0217\n","Epoch 7, Batch 141: Loss=0.0000, Grad Norm=0.0013\n","Epoch 7, Batch 142: Loss=0.0001, Grad Norm=0.0019\n","Epoch 7, Batch 143: Loss=0.0003, Grad Norm=0.0125\n","Epoch 7, Batch 144: Loss=0.0150, Grad Norm=0.4194\n","Epoch 7, Batch 145: Loss=0.0208, Grad Norm=0.8736\n","Epoch 7, Batch 146: Loss=0.0007, Grad Norm=0.0258\n","Epoch 7, Batch 147: Loss=0.0003, Grad Norm=0.0068\n","Epoch 7, Batch 148: Loss=0.0006, Grad Norm=0.0195\n","Epoch 7, Batch 149: Loss=0.0002, Grad Norm=0.0035\n","Epoch 7, Batch 150: Loss=0.0026, Grad Norm=0.0597\n","Epoch 7, Batch 151: Loss=0.0011, Grad Norm=0.0340\n","Epoch 7, Batch 152: Loss=0.0013, Grad Norm=0.0356\n","Epoch 7, Batch 153: Loss=0.0045, Grad Norm=0.1837\n","Epoch 7, Batch 154: Loss=0.0004, Grad Norm=0.0096\n","Epoch 7, Batch 155: Loss=0.0000, Grad Norm=0.0011\n","Epoch 7, Batch 156: Loss=0.0250, Grad Norm=1.3531\n","Epoch 7, Batch 157: Loss=0.0001, Grad Norm=0.0031\n","Epoch 7, Batch 158: Loss=0.0002, Grad Norm=0.0060\n","Epoch 7, Batch 159: Loss=0.0246, Grad Norm=1.0920\n","Epoch 7, Batch 160: Loss=0.0128, Grad Norm=0.4336\n","Epoch 7, Batch 161: Loss=0.0022, Grad Norm=0.0946\n","Epoch 7, Batch 162: Loss=0.0052, Grad Norm=0.1520\n","Epoch 7, Batch 163: Loss=0.0052, Grad Norm=0.1598\n","Epoch 7, Batch 164: Loss=0.0004, Grad Norm=0.0094\n","Epoch 7, Batch 165: Loss=0.0030, Grad Norm=0.0779\n","Epoch 7, Batch 166: Loss=0.0000, Grad Norm=0.0003\n","Epoch 7, Batch 167: Loss=0.0000, Grad Norm=0.0008\n","Epoch 7, Batch 168: Loss=0.0002, Grad Norm=0.0053\n","Epoch 7, Batch 169: Loss=0.0008, Grad Norm=0.0196\n","Epoch 7, Batch 170: Loss=0.0907, Grad Norm=3.2957\n","Epoch 7, Batch 171: Loss=0.0006, Grad Norm=0.0188\n","Epoch 7, Batch 172: Loss=0.0000, Grad Norm=0.0014\n","Epoch 7, Batch 173: Loss=0.0007, Grad Norm=0.0167\n","Epoch 7, Batch 174: Loss=0.1125, Grad Norm=2.5742\n","Epoch 7, Batch 175: Loss=0.0001, Grad Norm=0.0037\n","Epoch 7, Batch 176: Loss=0.1423, Grad Norm=3.0826\n","Epoch 7, Batch 177: Loss=0.0019, Grad Norm=0.0559\n","Epoch 7, Batch 178: Loss=0.0013, Grad Norm=0.0345\n","Epoch 7, Batch 179: Loss=0.0001, Grad Norm=0.0014\n","Epoch 7, Batch 180: Loss=0.0005, Grad Norm=0.0128\n","Epoch 7, Batch 181: Loss=0.0005, Grad Norm=0.0109\n","Epoch 7, Batch 182: Loss=0.0168, Grad Norm=0.3786\n","Epoch 7, Batch 183: Loss=0.0011, Grad Norm=0.0422\n","Epoch 7, Batch 184: Loss=0.0011, Grad Norm=0.0331\n","Epoch 7, Batch 185: Loss=0.0004, Grad Norm=0.0139\n","Epoch 7, Batch 186: Loss=0.0001, Grad Norm=0.0017\n","Epoch 7, Batch 187: Loss=0.0079, Grad Norm=0.4637\n","Epoch 7, Batch 188: Loss=0.0005, Grad Norm=0.0189\n","Epoch 7/15:\n","Train Loss: 0.0098, Train Accuracy: 99.63%\n","Val Loss: 0.0053, Val Accuracy: 99.83%\n","Epoch 8, Batch 1: Loss=0.0004, Grad Norm=0.0076\n","Epoch 8, Batch 2: Loss=0.0003, Grad Norm=0.0107\n","Epoch 8, Batch 3: Loss=0.0049, Grad Norm=0.1631\n","Epoch 8, Batch 4: Loss=0.0005, Grad Norm=0.0125\n","Epoch 8, Batch 5: Loss=0.0001, Grad Norm=0.0026\n","Epoch 8, Batch 6: Loss=0.0005, Grad Norm=0.0104\n","Epoch 8, Batch 7: Loss=0.0003, Grad Norm=0.0105\n","Epoch 8, Batch 8: Loss=0.0022, Grad Norm=0.0704\n","Epoch 8, Batch 9: Loss=0.0002, Grad Norm=0.0054\n","Epoch 8, Batch 10: Loss=0.0006, Grad Norm=0.0177\n","Epoch 8, Batch 11: Loss=0.0004, Grad Norm=0.0084\n","Epoch 8, Batch 12: Loss=0.0146, Grad Norm=0.8322\n","Epoch 8, Batch 13: Loss=0.0046, Grad Norm=0.1218\n","Epoch 8, Batch 14: Loss=0.0036, Grad Norm=0.0974\n","Epoch 8, Batch 15: Loss=0.0030, Grad Norm=0.1112\n","Epoch 8, Batch 16: Loss=0.0005, Grad Norm=0.0101\n","Epoch 8, Batch 17: Loss=0.0002, Grad Norm=0.0077\n","Epoch 8, Batch 18: Loss=0.0014, Grad Norm=0.0313\n","Epoch 8, Batch 19: Loss=0.0002, Grad Norm=0.0051\n","Epoch 8, Batch 20: Loss=0.0024, Grad Norm=0.1134\n","Epoch 8, Batch 21: Loss=0.0001, Grad Norm=0.0034\n","Epoch 8, Batch 22: Loss=0.0014, Grad Norm=0.0727\n","Epoch 8, Batch 23: Loss=0.0537, Grad Norm=1.9310\n","Epoch 8, Batch 24: Loss=0.0062, Grad Norm=0.3790\n","Epoch 8, Batch 25: Loss=0.0004, Grad Norm=0.0117\n","Epoch 8, Batch 26: Loss=0.0013, Grad Norm=0.0418\n","Epoch 8, Batch 27: Loss=0.0001, Grad Norm=0.0021\n","Epoch 8, Batch 28: Loss=0.0020, Grad Norm=0.0589\n","Epoch 8, Batch 29: Loss=0.0005, Grad Norm=0.0250\n","Epoch 8, Batch 30: Loss=0.0000, Grad Norm=0.0008\n","Epoch 8, Batch 31: Loss=0.0000, Grad Norm=0.0004\n","Epoch 8, Batch 32: Loss=0.0001, Grad Norm=0.0014\n","Epoch 8, Batch 33: Loss=0.0256, Grad Norm=0.5588\n","Epoch 8, Batch 34: Loss=0.0160, Grad Norm=0.7910\n","Epoch 8, Batch 35: Loss=0.1024, Grad Norm=2.1649\n","Epoch 8, Batch 36: Loss=0.0063, Grad Norm=0.2108\n","Epoch 8, Batch 37: Loss=0.0031, Grad Norm=0.0779\n","Epoch 8, Batch 38: Loss=0.0008, Grad Norm=0.0231\n","Epoch 8, Batch 39: Loss=0.0003, Grad Norm=0.0086\n","Epoch 8, Batch 40: Loss=0.0000, Grad Norm=0.0009\n","Epoch 8, Batch 41: Loss=0.0000, Grad Norm=0.0011\n","Epoch 8, Batch 42: Loss=0.0002, Grad Norm=0.0036\n","Epoch 8, Batch 43: Loss=0.0006, Grad Norm=0.0233\n","Epoch 8, Batch 44: Loss=0.0035, Grad Norm=0.0917\n","Epoch 8, Batch 45: Loss=0.0002, Grad Norm=0.0066\n","Epoch 8, Batch 46: Loss=0.0006, Grad Norm=0.0131\n","Epoch 8, Batch 47: Loss=0.0003, Grad Norm=0.0078\n","Epoch 8, Batch 48: Loss=0.0018, Grad Norm=0.0616\n","Epoch 8, Batch 49: Loss=0.0001, Grad Norm=0.0011\n","Epoch 8, Batch 50: Loss=0.0036, Grad Norm=0.1364\n","Epoch 8, Batch 51: Loss=0.0004, Grad Norm=0.0108\n","Epoch 8, Batch 52: Loss=0.0003, Grad Norm=0.0070\n","Epoch 8, Batch 53: Loss=0.0004, Grad Norm=0.0213\n","Epoch 8, Batch 54: Loss=0.0018, Grad Norm=0.0399\n","Epoch 8, Batch 55: Loss=0.0003, Grad Norm=0.0077\n","Epoch 8, Batch 56: Loss=0.0012, Grad Norm=0.0496\n","Epoch 8, Batch 57: Loss=0.0009, Grad Norm=0.0196\n","Epoch 8, Batch 58: Loss=0.0000, Grad Norm=0.0009\n","Epoch 8, Batch 59: Loss=0.0035, Grad Norm=0.0809\n","Epoch 8, Batch 60: Loss=0.0003, Grad Norm=0.0096\n","Epoch 8, Batch 61: Loss=0.0152, Grad Norm=0.7221\n","Epoch 8, Batch 62: Loss=0.0021, Grad Norm=0.0673\n","Epoch 8, Batch 63: Loss=0.0104, Grad Norm=0.4524\n","Epoch 8, Batch 64: Loss=0.0001, Grad Norm=0.0029\n","Epoch 8, Batch 65: Loss=0.0206, Grad Norm=0.8214\n","Epoch 8, Batch 66: Loss=0.0007, Grad Norm=0.0216\n","Epoch 8, Batch 67: Loss=0.0001, Grad Norm=0.0048\n","Epoch 8, Batch 68: Loss=0.0004, Grad Norm=0.0090\n","Epoch 8, Batch 69: Loss=0.0001, Grad Norm=0.0059\n","Epoch 8, Batch 70: Loss=0.0111, Grad Norm=0.5040\n","Epoch 8, Batch 71: Loss=0.0005, Grad Norm=0.0206\n","Epoch 8, Batch 72: Loss=0.0002, Grad Norm=0.0060\n","Epoch 8, Batch 73: Loss=0.0003, Grad Norm=0.0120\n","Epoch 8, Batch 74: Loss=0.0109, Grad Norm=0.5363\n","Epoch 8, Batch 75: Loss=0.0045, Grad Norm=0.2119\n","Epoch 8, Batch 76: Loss=0.0001, Grad Norm=0.0023\n","Epoch 8, Batch 77: Loss=0.0001, Grad Norm=0.0037\n","Epoch 8, Batch 78: Loss=0.0001, Grad Norm=0.0034\n","Epoch 8, Batch 79: Loss=0.0002, Grad Norm=0.0054\n","Epoch 8, Batch 80: Loss=0.0017, Grad Norm=0.0409\n","Epoch 8, Batch 81: Loss=0.0015, Grad Norm=0.0571\n","Epoch 8, Batch 82: Loss=0.0078, Grad Norm=0.4125\n","Epoch 8, Batch 83: Loss=0.0002, Grad Norm=0.0044\n","Epoch 8, Batch 84: Loss=0.0001, Grad Norm=0.0030\n","Epoch 8, Batch 85: Loss=0.0001, Grad Norm=0.0034\n","Epoch 8, Batch 86: Loss=0.0019, Grad Norm=0.0402\n","Epoch 8, Batch 87: Loss=0.0015, Grad Norm=0.0611\n","Epoch 8, Batch 88: Loss=0.0002, Grad Norm=0.0043\n","Epoch 8, Batch 89: Loss=0.0006, Grad Norm=0.0155\n","Epoch 8, Batch 90: Loss=0.0014, Grad Norm=0.0405\n","Epoch 8, Batch 91: Loss=0.0008, Grad Norm=0.0301\n","Epoch 8, Batch 92: Loss=0.0078, Grad Norm=0.2554\n","Epoch 8, Batch 93: Loss=0.0022, Grad Norm=0.0822\n","Epoch 8, Batch 94: Loss=0.0239, Grad Norm=1.7146\n","Epoch 8, Batch 95: Loss=0.0259, Grad Norm=0.8597\n","Epoch 8, Batch 96: Loss=0.0007, Grad Norm=0.0200\n","Epoch 8, Batch 97: Loss=0.0054, Grad Norm=0.1970\n","Epoch 8, Batch 98: Loss=0.0042, Grad Norm=0.0865\n","Epoch 8, Batch 99: Loss=0.0003, Grad Norm=0.0085\n","Epoch 8, Batch 100: Loss=0.0025, Grad Norm=0.0591\n","Epoch 8, Batch 101: Loss=0.0030, Grad Norm=0.1113\n","Epoch 8, Batch 102: Loss=0.0001, Grad Norm=0.0037\n","Epoch 8, Batch 103: Loss=0.0001, Grad Norm=0.0028\n","Epoch 8, Batch 104: Loss=0.0001, Grad Norm=0.0032\n","Epoch 8, Batch 105: Loss=0.0001, Grad Norm=0.0013\n","Epoch 8, Batch 106: Loss=0.0000, Grad Norm=0.0005\n","Epoch 8, Batch 107: Loss=0.0004, Grad Norm=0.0092\n","Epoch 8, Batch 108: Loss=0.0008, Grad Norm=0.0419\n","Epoch 8, Batch 109: Loss=0.0003, Grad Norm=0.0116\n","Epoch 8, Batch 110: Loss=0.0001, Grad Norm=0.0011\n","Epoch 8, Batch 111: Loss=0.0017, Grad Norm=0.0474\n","Epoch 8, Batch 112: Loss=0.0031, Grad Norm=0.0969\n","Epoch 8, Batch 113: Loss=0.0020, Grad Norm=0.0630\n","Epoch 8, Batch 114: Loss=0.0044, Grad Norm=0.1402\n","Epoch 8, Batch 115: Loss=0.0001, Grad Norm=0.0027\n","Epoch 8, Batch 116: Loss=0.0001, Grad Norm=0.0020\n","Epoch 8, Batch 117: Loss=0.0008, Grad Norm=0.0285\n","Epoch 8, Batch 118: Loss=0.0013, Grad Norm=0.0435\n","Epoch 8, Batch 119: Loss=0.0003, Grad Norm=0.0083\n","Epoch 8, Batch 120: Loss=0.0018, Grad Norm=0.0522\n","Epoch 8, Batch 121: Loss=0.0406, Grad Norm=1.3331\n","Epoch 8, Batch 122: Loss=0.0004, Grad Norm=0.0117\n","Epoch 8, Batch 123: Loss=0.0002, Grad Norm=0.0042\n","Epoch 8, Batch 124: Loss=0.0006, Grad Norm=0.0207\n","Epoch 8, Batch 125: Loss=0.0004, Grad Norm=0.0202\n","Epoch 8, Batch 126: Loss=0.0001, Grad Norm=0.0035\n","Epoch 8, Batch 127: Loss=0.0000, Grad Norm=0.0007\n","Epoch 8, Batch 128: Loss=0.0007, Grad Norm=0.0369\n","Epoch 8, Batch 129: Loss=0.0059, Grad Norm=0.3236\n","Epoch 8, Batch 130: Loss=0.0303, Grad Norm=1.2067\n","Epoch 8, Batch 131: Loss=0.0003, Grad Norm=0.0063\n","Epoch 8, Batch 132: Loss=0.0034, Grad Norm=0.2153\n","Epoch 8, Batch 133: Loss=0.0023, Grad Norm=0.0550\n","Epoch 8, Batch 134: Loss=0.0001, Grad Norm=0.0045\n","Epoch 8, Batch 135: Loss=0.0002, Grad Norm=0.0071\n","Epoch 8, Batch 136: Loss=0.0130, Grad Norm=0.4282\n","Epoch 8, Batch 137: Loss=0.0000, Grad Norm=0.0011\n","Epoch 8, Batch 138: Loss=0.0004, Grad Norm=0.0111\n","Epoch 8, Batch 139: Loss=0.0010, Grad Norm=0.0242\n","Epoch 8, Batch 140: Loss=0.0004, Grad Norm=0.0069\n","Epoch 8, Batch 141: Loss=0.0002, Grad Norm=0.0048\n","Epoch 8, Batch 142: Loss=0.0007, Grad Norm=0.0196\n","Epoch 8, Batch 143: Loss=0.0078, Grad Norm=0.3144\n","Epoch 8, Batch 144: Loss=0.0000, Grad Norm=0.0010\n","Epoch 8, Batch 145: Loss=0.0050, Grad Norm=0.1834\n","Epoch 8, Batch 146: Loss=0.0003, Grad Norm=0.0139\n","Epoch 8, Batch 147: Loss=0.0004, Grad Norm=0.0129\n","Epoch 8, Batch 148: Loss=0.0003, Grad Norm=0.0074\n","Epoch 8, Batch 149: Loss=0.0003, Grad Norm=0.0071\n","Epoch 8, Batch 150: Loss=0.0004, Grad Norm=0.0117\n","Epoch 8, Batch 151: Loss=0.0209, Grad Norm=0.6538\n","Epoch 8, Batch 152: Loss=0.0000, Grad Norm=0.0008\n","Epoch 8, Batch 153: Loss=0.0184, Grad Norm=0.4628\n","Epoch 8, Batch 154: Loss=0.0001, Grad Norm=0.0021\n","Epoch 8, Batch 155: Loss=0.0007, Grad Norm=0.0194\n","Epoch 8, Batch 156: Loss=0.0257, Grad Norm=1.2743\n","Epoch 8, Batch 157: Loss=0.0040, Grad Norm=0.1869\n","Epoch 8, Batch 158: Loss=0.0029, Grad Norm=0.0750\n","Epoch 8, Batch 159: Loss=0.0022, Grad Norm=0.0461\n","Epoch 8, Batch 160: Loss=0.0002, Grad Norm=0.0047\n","Epoch 8, Batch 161: Loss=0.0091, Grad Norm=0.3610\n","Epoch 8, Batch 162: Loss=0.0049, Grad Norm=0.2285\n","Epoch 8, Batch 163: Loss=0.0017, Grad Norm=0.0713\n","Epoch 8, Batch 164: Loss=0.0000, Grad Norm=0.0009\n","Epoch 8, Batch 165: Loss=0.0006, Grad Norm=0.0136\n","Epoch 8, Batch 166: Loss=0.0000, Grad Norm=0.0010\n","Epoch 8, Batch 167: Loss=0.0002, Grad Norm=0.0035\n","Epoch 8, Batch 168: Loss=0.0001, Grad Norm=0.0017\n","Epoch 8, Batch 169: Loss=0.0011, Grad Norm=0.0418\n","Epoch 8, Batch 170: Loss=0.0000, Grad Norm=0.0013\n","Epoch 8, Batch 171: Loss=0.0002, Grad Norm=0.0058\n","Epoch 8, Batch 172: Loss=0.0012, Grad Norm=0.0707\n","Epoch 8, Batch 173: Loss=0.0001, Grad Norm=0.0019\n","Epoch 8, Batch 174: Loss=0.0014, Grad Norm=0.0504\n","Epoch 8, Batch 175: Loss=0.0248, Grad Norm=1.1484\n","Epoch 8, Batch 176: Loss=0.0005, Grad Norm=0.0128\n","Epoch 8, Batch 177: Loss=0.0034, Grad Norm=0.1093\n","Epoch 8, Batch 178: Loss=0.0916, Grad Norm=3.6269\n","Epoch 8, Batch 179: Loss=0.0003, Grad Norm=0.0096\n","Epoch 8, Batch 180: Loss=0.0137, Grad Norm=0.4914\n","Epoch 8, Batch 181: Loss=0.0004, Grad Norm=0.0092\n","Epoch 8, Batch 182: Loss=0.0036, Grad Norm=0.1444\n","Epoch 8, Batch 183: Loss=0.0010, Grad Norm=0.0506\n","Epoch 8, Batch 184: Loss=0.0029, Grad Norm=0.0655\n","Epoch 8, Batch 185: Loss=0.0001, Grad Norm=0.0038\n","Epoch 8, Batch 186: Loss=0.0113, Grad Norm=0.2743\n","Epoch 8, Batch 187: Loss=0.0003, Grad Norm=0.0082\n","Epoch 8, Batch 188: Loss=0.0001, Grad Norm=0.0031\n","Epoch 8/15:\n","Train Loss: 0.0044, Train Accuracy: 99.90%\n","Val Loss: 0.0143, Val Accuracy: 99.67%\n","Model saved (improved val loss: 0.0143)\n","Epoch 9, Batch 1: Loss=0.0000, Grad Norm=0.0006\n","Epoch 9, Batch 2: Loss=0.0015, Grad Norm=0.0539\n","Epoch 9, Batch 3: Loss=0.0010, Grad Norm=0.0379\n","Epoch 9, Batch 4: Loss=0.0036, Grad Norm=0.1505\n","Epoch 9, Batch 5: Loss=0.0005, Grad Norm=0.0184\n","Epoch 9, Batch 6: Loss=0.0037, Grad Norm=0.1246\n","Epoch 9, Batch 7: Loss=0.0002, Grad Norm=0.0069\n","Epoch 9, Batch 8: Loss=0.0004, Grad Norm=0.0143\n","Epoch 9, Batch 9: Loss=0.0016, Grad Norm=0.0425\n","Epoch 9, Batch 10: Loss=0.0001, Grad Norm=0.0028\n","Epoch 9, Batch 11: Loss=0.0002, Grad Norm=0.0047\n","Epoch 9, Batch 12: Loss=0.0006, Grad Norm=0.0176\n","Epoch 9, Batch 13: Loss=0.0030, Grad Norm=0.1141\n","Epoch 9, Batch 14: Loss=0.0057, Grad Norm=0.1494\n","Epoch 9, Batch 15: Loss=0.0008, Grad Norm=0.0211\n","Epoch 9, Batch 16: Loss=0.0055, Grad Norm=0.2552\n","Epoch 9, Batch 17: Loss=0.0006, Grad Norm=0.0182\n","Epoch 9, Batch 18: Loss=0.0003, Grad Norm=0.0065\n","Epoch 9, Batch 19: Loss=0.0003, Grad Norm=0.0129\n","Epoch 9, Batch 20: Loss=0.0004, Grad Norm=0.0101\n","Epoch 9, Batch 21: Loss=0.0001, Grad Norm=0.0021\n","Epoch 9, Batch 22: Loss=0.0000, Grad Norm=0.0004\n","Epoch 9, Batch 23: Loss=0.0002, Grad Norm=0.0048\n","Epoch 9, Batch 24: Loss=0.0707, Grad Norm=1.4949\n","Epoch 9, Batch 25: Loss=0.0001, Grad Norm=0.0020\n","Epoch 9, Batch 26: Loss=0.0120, Grad Norm=0.6699\n","Epoch 9, Batch 27: Loss=0.0016, Grad Norm=0.0356\n","Epoch 9, Batch 28: Loss=0.0853, Grad Norm=1.8612\n","Epoch 9, Batch 29: Loss=0.0050, Grad Norm=0.1864\n","Epoch 9, Batch 30: Loss=0.0008, Grad Norm=0.0231\n","Epoch 9, Batch 31: Loss=0.0020, Grad Norm=0.0588\n","Epoch 9, Batch 32: Loss=0.0001, Grad Norm=0.0038\n","Epoch 9, Batch 33: Loss=0.0001, Grad Norm=0.0036\n","Epoch 9, Batch 34: Loss=0.0001, Grad Norm=0.0014\n","Epoch 9, Batch 35: Loss=0.0047, Grad Norm=0.1135\n","Epoch 9, Batch 36: Loss=0.0003, Grad Norm=0.0130\n","Epoch 9, Batch 37: Loss=0.0000, Grad Norm=0.0010\n","Epoch 9, Batch 38: Loss=0.0924, Grad Norm=2.2978\n","Epoch 9, Batch 39: Loss=0.0033, Grad Norm=0.0826\n","Epoch 9, Batch 40: Loss=0.0023, Grad Norm=0.0586\n","Epoch 9, Batch 41: Loss=0.0002, Grad Norm=0.0058\n","Epoch 9, Batch 42: Loss=0.0947, Grad Norm=2.9859\n","Epoch 9, Batch 43: Loss=0.0503, Grad Norm=1.4248\n","Epoch 9, Batch 44: Loss=0.0010, Grad Norm=0.0501\n","Epoch 9, Batch 45: Loss=0.2086, Grad Norm=3.3012\n","Epoch 9, Batch 46: Loss=0.0003, Grad Norm=0.0059\n","Epoch 9, Batch 47: Loss=0.0001, Grad Norm=0.0030\n","Epoch 9, Batch 48: Loss=0.0007, Grad Norm=0.0371\n","Epoch 9, Batch 49: Loss=0.0009, Grad Norm=0.0299\n","Epoch 9, Batch 50: Loss=0.0006, Grad Norm=0.0149\n","Epoch 9, Batch 51: Loss=0.0013, Grad Norm=0.0516\n","Epoch 9, Batch 52: Loss=0.0152, Grad Norm=0.6509\n","Epoch 9, Batch 53: Loss=0.0001, Grad Norm=0.0020\n","Epoch 9, Batch 54: Loss=0.0000, Grad Norm=0.0005\n","Epoch 9, Batch 55: Loss=0.0015, Grad Norm=0.0472\n","Epoch 9, Batch 56: Loss=0.0001, Grad Norm=0.0020\n","Epoch 9, Batch 57: Loss=0.0020, Grad Norm=0.0530\n","Epoch 9, Batch 58: Loss=0.0052, Grad Norm=0.1355\n","Epoch 9, Batch 59: Loss=0.0001, Grad Norm=0.0023\n","Epoch 9, Batch 60: Loss=0.0123, Grad Norm=0.6960\n","Epoch 9, Batch 61: Loss=0.0001, Grad Norm=0.0015\n","Epoch 9, Batch 62: Loss=0.0010, Grad Norm=0.0358\n","Epoch 9, Batch 63: Loss=0.0041, Grad Norm=0.2973\n","Epoch 9, Batch 64: Loss=0.0001, Grad Norm=0.0036\n","Epoch 9, Batch 65: Loss=0.0001, Grad Norm=0.0021\n","Epoch 9, Batch 66: Loss=0.0006, Grad Norm=0.0207\n","Epoch 9, Batch 67: Loss=0.0003, Grad Norm=0.0083\n","Epoch 9, Batch 68: Loss=0.0032, Grad Norm=0.0943\n","Epoch 9, Batch 69: Loss=0.0020, Grad Norm=0.0608\n","Epoch 9, Batch 70: Loss=0.0002, Grad Norm=0.0040\n","Epoch 9, Batch 71: Loss=0.0014, Grad Norm=0.0443\n","Epoch 9, Batch 72: Loss=0.0009, Grad Norm=0.0438\n","Epoch 9, Batch 73: Loss=0.0001, Grad Norm=0.0036\n","Epoch 9, Batch 74: Loss=0.0087, Grad Norm=0.3375\n","Epoch 9, Batch 75: Loss=0.0003, Grad Norm=0.0065\n","Epoch 9, Batch 76: Loss=0.0003, Grad Norm=0.0086\n","Epoch 9, Batch 77: Loss=0.0002, Grad Norm=0.0037\n","Epoch 9, Batch 78: Loss=0.0001, Grad Norm=0.0012\n","Epoch 9, Batch 79: Loss=0.0001, Grad Norm=0.0028\n","Epoch 9, Batch 80: Loss=0.0006, Grad Norm=0.0198\n","Epoch 9, Batch 81: Loss=0.0004, Grad Norm=0.0089\n","Epoch 9, Batch 82: Loss=0.0001, Grad Norm=0.0038\n","Epoch 9, Batch 83: Loss=0.0000, Grad Norm=0.0011\n","Epoch 9, Batch 84: Loss=0.0004, Grad Norm=0.0099\n","Epoch 9, Batch 85: Loss=0.0006, Grad Norm=0.0202\n","Epoch 9, Batch 86: Loss=0.0075, Grad Norm=0.1554\n","Epoch 9, Batch 87: Loss=0.0001, Grad Norm=0.0014\n","Epoch 9, Batch 88: Loss=0.0011, Grad Norm=0.0273\n","Epoch 9, Batch 89: Loss=0.0009, Grad Norm=0.0344\n","Epoch 9, Batch 90: Loss=0.0046, Grad Norm=0.1666\n","Epoch 9, Batch 91: Loss=0.0789, Grad Norm=2.5349\n","Epoch 9, Batch 92: Loss=0.0007, Grad Norm=0.0175\n","Epoch 9, Batch 93: Loss=0.0002, Grad Norm=0.0071\n","Epoch 9, Batch 94: Loss=0.0015, Grad Norm=0.0475\n","Epoch 9, Batch 95: Loss=0.0011, Grad Norm=0.0366\n","Epoch 9, Batch 96: Loss=0.0031, Grad Norm=0.0799\n","Epoch 9, Batch 97: Loss=0.0002, Grad Norm=0.0060\n","Epoch 9, Batch 98: Loss=0.0009, Grad Norm=0.0393\n","Epoch 9, Batch 99: Loss=0.0001, Grad Norm=0.0021\n","Epoch 9, Batch 100: Loss=0.0054, Grad Norm=0.2187\n","Epoch 9, Batch 101: Loss=0.0000, Grad Norm=0.0008\n","Epoch 9, Batch 102: Loss=0.0334, Grad Norm=1.3199\n","Epoch 9, Batch 103: Loss=0.0003, Grad Norm=0.0078\n","Epoch 9, Batch 104: Loss=0.0005, Grad Norm=0.0119\n","Epoch 9, Batch 105: Loss=0.0002, Grad Norm=0.0060\n","Epoch 9, Batch 106: Loss=0.0000, Grad Norm=0.0005\n","Epoch 9, Batch 107: Loss=0.0014, Grad Norm=0.0534\n","Epoch 9, Batch 108: Loss=0.0019, Grad Norm=0.0968\n","Epoch 9, Batch 109: Loss=0.0003, Grad Norm=0.0080\n","Epoch 9, Batch 110: Loss=0.0001, Grad Norm=0.0023\n","Epoch 9, Batch 111: Loss=0.0338, Grad Norm=1.0789\n","Epoch 9, Batch 112: Loss=0.0007, Grad Norm=0.0189\n","Epoch 9, Batch 113: Loss=0.0001, Grad Norm=0.0028\n","Epoch 9, Batch 114: Loss=0.0012, Grad Norm=0.0586\n","Epoch 9, Batch 115: Loss=0.0005, Grad Norm=0.0150\n","Epoch 9, Batch 116: Loss=0.0404, Grad Norm=1.3357\n","Epoch 9, Batch 117: Loss=0.0010, Grad Norm=0.0218\n","Epoch 9, Batch 118: Loss=0.0000, Grad Norm=0.0008\n","Epoch 9, Batch 119: Loss=0.0000, Grad Norm=0.0008\n","Epoch 9, Batch 120: Loss=0.0011, Grad Norm=0.0383\n","Epoch 9, Batch 121: Loss=0.0001, Grad Norm=0.0034\n","Epoch 9, Batch 122: Loss=0.0002, Grad Norm=0.0100\n","Epoch 9, Batch 123: Loss=0.0000, Grad Norm=0.0003\n","Epoch 9, Batch 124: Loss=0.0008, Grad Norm=0.0189\n","Epoch 9, Batch 125: Loss=0.0006, Grad Norm=0.0176\n","Epoch 9, Batch 126: Loss=0.0001, Grad Norm=0.0025\n","Epoch 9, Batch 127: Loss=0.0051, Grad Norm=0.1810\n","Epoch 9, Batch 128: Loss=0.0301, Grad Norm=1.2549\n","Epoch 9, Batch 129: Loss=0.0005, Grad Norm=0.0130\n","Epoch 9, Batch 130: Loss=0.0019, Grad Norm=0.0463\n","Epoch 9, Batch 131: Loss=0.0018, Grad Norm=0.1108\n","Epoch 9, Batch 132: Loss=0.0004, Grad Norm=0.0079\n","Epoch 9, Batch 133: Loss=0.0003, Grad Norm=0.0069\n","Epoch 9, Batch 134: Loss=0.0002, Grad Norm=0.0074\n","Epoch 9, Batch 135: Loss=0.1254, Grad Norm=4.0611\n","Epoch 9, Batch 136: Loss=0.0005, Grad Norm=0.0173\n","Epoch 9, Batch 137: Loss=0.0004, Grad Norm=0.0103\n","Epoch 9, Batch 138: Loss=0.0001, Grad Norm=0.0020\n","Epoch 9, Batch 139: Loss=0.0013, Grad Norm=0.0337\n","Epoch 9, Batch 140: Loss=0.0018, Grad Norm=0.0463\n","Epoch 9, Batch 141: Loss=0.0001, Grad Norm=0.0038\n","Epoch 9, Batch 142: Loss=0.0180, Grad Norm=0.4529\n","Epoch 9, Batch 143: Loss=0.0003, Grad Norm=0.0106\n","Epoch 9, Batch 144: Loss=0.0069, Grad Norm=0.2429\n","Epoch 9, Batch 145: Loss=0.1056, Grad Norm=2.7980\n","Epoch 9, Batch 146: Loss=0.0001, Grad Norm=0.0013\n","Epoch 9, Batch 147: Loss=0.0013, Grad Norm=0.0470\n","Epoch 9, Batch 148: Loss=0.0073, Grad Norm=0.1802\n","Epoch 9, Batch 149: Loss=0.0018, Grad Norm=0.0383\n","Epoch 9, Batch 150: Loss=0.0002, Grad Norm=0.0065\n","Epoch 9, Batch 151: Loss=0.0001, Grad Norm=0.0034\n","Epoch 9, Batch 152: Loss=0.0001, Grad Norm=0.0038\n","Epoch 9, Batch 153: Loss=0.0001, Grad Norm=0.0055\n","Epoch 9, Batch 154: Loss=0.0007, Grad Norm=0.0268\n","Epoch 9, Batch 155: Loss=0.0009, Grad Norm=0.0225\n","Epoch 9, Batch 156: Loss=0.0003, Grad Norm=0.0093\n","Epoch 9, Batch 157: Loss=0.0003, Grad Norm=0.0120\n","Epoch 9, Batch 158: Loss=0.0032, Grad Norm=0.0726\n","Epoch 9, Batch 159: Loss=0.0006, Grad Norm=0.0132\n","Epoch 9, Batch 160: Loss=0.0284, Grad Norm=0.5697\n","Epoch 9, Batch 161: Loss=0.0002, Grad Norm=0.0037\n","Epoch 9, Batch 162: Loss=0.0005, Grad Norm=0.0092\n","Epoch 9, Batch 163: Loss=0.0012, Grad Norm=0.0425\n","Epoch 9, Batch 164: Loss=0.0071, Grad Norm=0.2887\n","Epoch 9, Batch 165: Loss=0.0004, Grad Norm=0.0132\n","Epoch 9, Batch 166: Loss=0.0119, Grad Norm=0.3934\n","Epoch 9, Batch 167: Loss=0.0060, Grad Norm=0.1969\n","Epoch 9, Batch 168: Loss=0.0003, Grad Norm=0.0057\n","Epoch 9, Batch 169: Loss=0.0960, Grad Norm=2.5065\n","Epoch 9, Batch 170: Loss=0.0003, Grad Norm=0.0059\n","Epoch 9, Batch 171: Loss=0.0007, Grad Norm=0.0168\n","Epoch 9, Batch 172: Loss=0.0003, Grad Norm=0.0072\n","Epoch 9, Batch 173: Loss=0.0001, Grad Norm=0.0015\n","Epoch 9, Batch 174: Loss=0.0002, Grad Norm=0.0057\n","Epoch 9, Batch 175: Loss=0.0004, Grad Norm=0.0138\n","Epoch 9, Batch 176: Loss=0.0008, Grad Norm=0.0346\n","Epoch 9, Batch 177: Loss=0.0005, Grad Norm=0.0194\n","Epoch 9, Batch 178: Loss=0.0223, Grad Norm=0.8504\n","Epoch 9, Batch 179: Loss=0.0020, Grad Norm=0.0721\n","Epoch 9, Batch 180: Loss=0.0010, Grad Norm=0.0216\n","Epoch 9, Batch 181: Loss=0.0118, Grad Norm=0.3789\n","Epoch 9, Batch 182: Loss=0.0075, Grad Norm=0.3018\n","Epoch 9, Batch 183: Loss=0.0001, Grad Norm=0.0044\n","Epoch 9, Batch 184: Loss=0.0283, Grad Norm=1.3130\n","Epoch 9, Batch 185: Loss=0.0088, Grad Norm=0.3045\n","Epoch 9, Batch 186: Loss=0.0204, Grad Norm=0.6605\n","Epoch 9, Batch 187: Loss=0.0002, Grad Norm=0.0080\n","Epoch 9, Batch 188: Loss=0.0003, Grad Norm=0.0088\n","Epoch 9/15:\n","Train Loss: 0.0082, Train Accuracy: 99.77%\n","Val Loss: 0.0153, Val Accuracy: 99.50%\n","Epoch 10, Batch 1: Loss=0.0001, Grad Norm=0.0038\n","Epoch 10, Batch 2: Loss=0.0001, Grad Norm=0.0035\n","Epoch 10, Batch 3: Loss=0.0116, Grad Norm=0.2474\n","Epoch 10, Batch 4: Loss=0.0398, Grad Norm=1.2725\n","Epoch 10, Batch 5: Loss=0.0020, Grad Norm=0.1143\n","Epoch 10, Batch 6: Loss=0.0000, Grad Norm=0.0009\n","Epoch 10, Batch 7: Loss=0.0002, Grad Norm=0.0032\n","Epoch 10, Batch 8: Loss=0.0032, Grad Norm=0.0776\n","Epoch 10, Batch 9: Loss=0.0011, Grad Norm=0.0312\n","Epoch 10, Batch 10: Loss=0.0002, Grad Norm=0.0040\n","Epoch 10, Batch 11: Loss=0.0001, Grad Norm=0.0068\n","Epoch 10, Batch 12: Loss=0.0006, Grad Norm=0.0177\n","Epoch 10, Batch 13: Loss=0.0569, Grad Norm=1.6124\n","Epoch 10, Batch 14: Loss=0.0004, Grad Norm=0.0124\n","Epoch 10, Batch 15: Loss=0.0005, Grad Norm=0.0140\n","Epoch 10, Batch 16: Loss=0.0000, Grad Norm=0.0005\n","Epoch 10, Batch 17: Loss=0.0002, Grad Norm=0.0066\n","Epoch 10, Batch 18: Loss=0.0054, Grad Norm=0.1534\n","Epoch 10, Batch 19: Loss=0.0013, Grad Norm=0.0438\n","Epoch 10, Batch 20: Loss=0.0002, Grad Norm=0.0079\n","Epoch 10, Batch 21: Loss=0.0007, Grad Norm=0.0230\n","Epoch 10, Batch 22: Loss=0.0008, Grad Norm=0.0363\n","Epoch 10, Batch 23: Loss=0.0021, Grad Norm=0.0651\n","Epoch 10, Batch 24: Loss=0.0016, Grad Norm=0.0559\n","Epoch 10, Batch 25: Loss=0.0002, Grad Norm=0.0054\n","Epoch 10, Batch 26: Loss=0.0016, Grad Norm=0.0635\n","Epoch 10, Batch 27: Loss=0.0013, Grad Norm=0.0311\n","Epoch 10, Batch 28: Loss=0.0000, Grad Norm=0.0007\n","Epoch 10, Batch 29: Loss=0.0001, Grad Norm=0.0024\n","Epoch 10, Batch 30: Loss=0.0047, Grad Norm=0.1635\n","Epoch 10, Batch 31: Loss=0.0608, Grad Norm=1.7331\n","Epoch 10, Batch 32: Loss=0.0012, Grad Norm=0.0323\n","Epoch 10, Batch 33: Loss=0.0080, Grad Norm=0.2413\n","Epoch 10, Batch 34: Loss=0.0018, Grad Norm=0.0684\n","Epoch 10, Batch 35: Loss=0.0107, Grad Norm=0.2922\n","Epoch 10, Batch 36: Loss=0.0072, Grad Norm=0.2495\n","Epoch 10, Batch 37: Loss=0.0164, Grad Norm=0.7494\n","Epoch 10, Batch 38: Loss=0.0019, Grad Norm=0.0865\n","Epoch 10, Batch 39: Loss=0.0014, Grad Norm=0.0445\n","Epoch 10, Batch 40: Loss=0.0001, Grad Norm=0.0022\n","Epoch 10, Batch 41: Loss=0.0018, Grad Norm=0.0740\n","Epoch 10, Batch 42: Loss=0.0168, Grad Norm=0.7213\n","Epoch 10, Batch 43: Loss=0.0001, Grad Norm=0.0030\n","Epoch 10, Batch 44: Loss=0.0003, Grad Norm=0.0110\n","Epoch 10, Batch 45: Loss=0.0020, Grad Norm=0.0606\n","Epoch 10, Batch 46: Loss=0.0004, Grad Norm=0.0154\n","Epoch 10, Batch 47: Loss=0.0034, Grad Norm=0.1676\n","Epoch 10, Batch 48: Loss=0.0019, Grad Norm=0.0489\n","Epoch 10, Batch 49: Loss=0.0002, Grad Norm=0.0037\n","Epoch 10, Batch 50: Loss=0.0005, Grad Norm=0.0127\n","Epoch 10, Batch 51: Loss=0.0001, Grad Norm=0.0038\n","Epoch 10, Batch 52: Loss=0.0004, Grad Norm=0.0222\n","Epoch 10, Batch 53: Loss=0.0012, Grad Norm=0.0266\n","Epoch 10, Batch 54: Loss=0.0000, Grad Norm=0.0012\n","Epoch 10, Batch 55: Loss=0.0005, Grad Norm=0.0144\n","Epoch 10, Batch 56: Loss=0.0001, Grad Norm=0.0017\n","Epoch 10, Batch 57: Loss=0.0008, Grad Norm=0.0271\n","Epoch 10, Batch 58: Loss=0.0001, Grad Norm=0.0020\n","Epoch 10, Batch 59: Loss=0.0000, Grad Norm=0.0006\n","Epoch 10, Batch 60: Loss=0.0019, Grad Norm=0.0752\n","Epoch 10, Batch 61: Loss=0.0006, Grad Norm=0.0162\n","Epoch 10, Batch 62: Loss=0.0009, Grad Norm=0.0459\n","Epoch 10, Batch 63: Loss=0.0012, Grad Norm=0.0424\n","Epoch 10, Batch 64: Loss=0.0014, Grad Norm=0.0435\n","Epoch 10, Batch 65: Loss=0.0002, Grad Norm=0.0044\n","Epoch 10, Batch 66: Loss=0.0004, Grad Norm=0.0090\n","Epoch 10, Batch 67: Loss=0.0369, Grad Norm=1.9519\n","Epoch 10, Batch 68: Loss=0.0004, Grad Norm=0.0092\n","Epoch 10, Batch 69: Loss=0.0008, Grad Norm=0.0283\n","Epoch 10, Batch 70: Loss=0.0012, Grad Norm=0.0356\n","Epoch 10, Batch 71: Loss=0.0004, Grad Norm=0.0080\n","Epoch 10, Batch 72: Loss=0.0001, Grad Norm=0.0031\n","Epoch 10, Batch 73: Loss=0.0006, Grad Norm=0.0277\n","Epoch 10, Batch 74: Loss=0.0045, Grad Norm=0.1917\n","Epoch 10, Batch 75: Loss=0.1849, Grad Norm=3.8862\n","Epoch 10, Batch 76: Loss=0.0016, Grad Norm=0.0793\n","Epoch 10, Batch 77: Loss=0.0039, Grad Norm=0.1583\n","Epoch 10, Batch 78: Loss=0.0014, Grad Norm=0.0325\n","Epoch 10, Batch 79: Loss=0.0002, Grad Norm=0.0106\n","Epoch 10, Batch 80: Loss=0.0006, Grad Norm=0.0187\n","Epoch 10, Batch 81: Loss=0.0173, Grad Norm=0.7535\n","Epoch 10, Batch 82: Loss=0.0217, Grad Norm=0.7257\n","Epoch 10, Batch 83: Loss=0.0040, Grad Norm=0.1028\n","Epoch 10, Batch 84: Loss=0.0004, Grad Norm=0.0097\n","Epoch 10, Batch 85: Loss=0.0030, Grad Norm=0.1429\n","Epoch 10, Batch 86: Loss=0.0265, Grad Norm=0.7099\n","Epoch 10, Batch 87: Loss=0.0006, Grad Norm=0.0291\n","Epoch 10, Batch 88: Loss=0.0012, Grad Norm=0.0423\n","Epoch 10, Batch 89: Loss=0.0020, Grad Norm=0.1344\n","Epoch 10, Batch 90: Loss=0.0005, Grad Norm=0.0147\n","Epoch 10, Batch 91: Loss=0.0001, Grad Norm=0.0030\n","Epoch 10, Batch 92: Loss=0.0009, Grad Norm=0.0335\n","Epoch 10, Batch 93: Loss=0.0062, Grad Norm=0.1143\n","Epoch 10, Batch 94: Loss=0.0003, Grad Norm=0.0059\n","Epoch 10, Batch 95: Loss=0.0007, Grad Norm=0.0176\n","Epoch 10, Batch 96: Loss=0.0002, Grad Norm=0.0062\n","Epoch 10, Batch 97: Loss=0.2783, Grad Norm=4.5411\n","Epoch 10, Batch 98: Loss=0.0158, Grad Norm=0.8631\n","Epoch 10, Batch 99: Loss=0.0010, Grad Norm=0.0299\n","Epoch 10, Batch 100: Loss=0.0239, Grad Norm=0.8513\n","Epoch 10, Batch 101: Loss=0.0428, Grad Norm=2.2371\n","Epoch 10, Batch 102: Loss=0.0005, Grad Norm=0.0186\n","Epoch 10, Batch 103: Loss=0.0200, Grad Norm=0.5480\n","Epoch 10, Batch 104: Loss=0.0001, Grad Norm=0.0020\n","Epoch 10, Batch 105: Loss=0.0002, Grad Norm=0.0061\n","Epoch 10, Batch 106: Loss=0.0004, Grad Norm=0.0121\n","Epoch 10, Batch 107: Loss=0.0003, Grad Norm=0.0084\n","Epoch 10, Batch 108: Loss=0.0101, Grad Norm=0.2430\n","Epoch 10, Batch 109: Loss=0.0009, Grad Norm=0.0383\n","Epoch 10, Batch 110: Loss=0.0003, Grad Norm=0.0071\n","Epoch 10, Batch 111: Loss=0.0002, Grad Norm=0.0062\n","Epoch 10, Batch 112: Loss=0.0005, Grad Norm=0.0187\n","Epoch 10, Batch 113: Loss=0.0125, Grad Norm=0.6564\n","Epoch 10, Batch 114: Loss=0.0006, Grad Norm=0.0288\n","Epoch 10, Batch 115: Loss=0.0144, Grad Norm=0.6633\n","Epoch 10, Batch 116: Loss=0.0002, Grad Norm=0.0068\n","Epoch 10, Batch 117: Loss=0.3318, Grad Norm=5.4176\n","Epoch 10, Batch 118: Loss=0.0020, Grad Norm=0.0551\n","Epoch 10, Batch 119: Loss=0.0006, Grad Norm=0.0197\n","Epoch 10, Batch 120: Loss=0.0053, Grad Norm=0.1393\n","Epoch 10, Batch 121: Loss=0.0009, Grad Norm=0.0239\n","Epoch 10, Batch 122: Loss=0.0003, Grad Norm=0.0071\n","Epoch 10, Batch 123: Loss=0.0002, Grad Norm=0.0101\n","Epoch 10, Batch 124: Loss=0.0036, Grad Norm=0.0919\n","Epoch 10, Batch 125: Loss=0.0000, Grad Norm=0.0015\n","Epoch 10, Batch 126: Loss=0.0001, Grad Norm=0.0024\n","Epoch 10, Batch 127: Loss=0.0003, Grad Norm=0.0074\n","Epoch 10, Batch 128: Loss=0.0025, Grad Norm=0.0952\n","Epoch 10, Batch 129: Loss=0.0141, Grad Norm=0.5154\n","Epoch 10, Batch 130: Loss=0.0006, Grad Norm=0.0195\n","Epoch 10, Batch 131: Loss=0.0007, Grad Norm=0.0261\n","Epoch 10, Batch 132: Loss=0.0001, Grad Norm=0.0031\n","Epoch 10, Batch 133: Loss=0.0001, Grad Norm=0.0051\n","Epoch 10, Batch 134: Loss=0.0005, Grad Norm=0.0281\n","Epoch 10, Batch 135: Loss=0.1526, Grad Norm=2.7803\n","Epoch 10, Batch 136: Loss=0.0082, Grad Norm=0.2125\n","Epoch 10, Batch 137: Loss=0.0010, Grad Norm=0.0345\n","Epoch 10, Batch 138: Loss=0.0001, Grad Norm=0.0018\n","Epoch 10, Batch 139: Loss=0.0007, Grad Norm=0.0160\n","Epoch 10, Batch 140: Loss=0.0023, Grad Norm=0.0596\n","Epoch 10, Batch 141: Loss=0.0003, Grad Norm=0.0069\n","Epoch 10, Batch 142: Loss=0.0356, Grad Norm=0.8938\n","Epoch 10, Batch 143: Loss=0.0006, Grad Norm=0.0122\n","Epoch 10, Batch 144: Loss=0.0000, Grad Norm=0.0003\n","Epoch 10, Batch 145: Loss=0.0001, Grad Norm=0.0096\n","Epoch 10, Batch 146: Loss=0.0104, Grad Norm=0.2928\n","Epoch 10, Batch 147: Loss=0.0000, Grad Norm=0.0006\n","Epoch 10, Batch 148: Loss=0.0001, Grad Norm=0.0030\n","Epoch 10, Batch 149: Loss=0.0001, Grad Norm=0.0044\n","Epoch 10, Batch 150: Loss=0.0008, Grad Norm=0.0241\n","Epoch 10, Batch 151: Loss=0.0067, Grad Norm=0.1990\n","Epoch 10, Batch 152: Loss=0.0000, Grad Norm=0.0015\n","Epoch 10, Batch 153: Loss=0.0006, Grad Norm=0.0130\n","Epoch 10, Batch 154: Loss=0.0004, Grad Norm=0.0063\n","Epoch 10, Batch 155: Loss=0.0001, Grad Norm=0.0014\n","Epoch 10, Batch 156: Loss=0.0022, Grad Norm=0.0550\n","Epoch 10, Batch 157: Loss=0.0002, Grad Norm=0.0059\n","Epoch 10, Batch 158: Loss=0.0007, Grad Norm=0.0192\n","Epoch 10, Batch 159: Loss=0.0112, Grad Norm=0.2299\n","Epoch 10, Batch 160: Loss=0.0002, Grad Norm=0.0041\n","Epoch 10, Batch 161: Loss=0.0002, Grad Norm=0.0035\n","Epoch 10, Batch 162: Loss=0.0024, Grad Norm=0.1013\n","Epoch 10, Batch 163: Loss=0.0002, Grad Norm=0.0048\n","Epoch 10, Batch 164: Loss=0.0001, Grad Norm=0.0015\n","Epoch 10, Batch 165: Loss=0.0041, Grad Norm=0.1228\n","Epoch 10, Batch 166: Loss=0.0017, Grad Norm=0.0889\n","Epoch 10, Batch 167: Loss=0.0001, Grad Norm=0.0025\n","Epoch 10, Batch 168: Loss=0.0004, Grad Norm=0.0086\n","Epoch 10, Batch 169: Loss=0.0014, Grad Norm=0.0390\n","Epoch 10, Batch 170: Loss=0.0020, Grad Norm=0.0787\n","Epoch 10, Batch 171: Loss=0.0001, Grad Norm=0.0043\n","Epoch 10, Batch 172: Loss=0.0020, Grad Norm=0.0534\n","Epoch 10, Batch 173: Loss=0.0004, Grad Norm=0.0096\n","Epoch 10, Batch 174: Loss=0.0020, Grad Norm=0.0562\n","Epoch 10, Batch 175: Loss=0.0007, Grad Norm=0.0211\n","Epoch 10, Batch 176: Loss=0.0005, Grad Norm=0.0154\n","Epoch 10, Batch 177: Loss=0.0026, Grad Norm=0.0606\n","Epoch 10, Batch 178: Loss=0.0000, Grad Norm=0.0005\n","Epoch 10, Batch 179: Loss=0.0004, Grad Norm=0.0133\n","Epoch 10, Batch 180: Loss=0.0002, Grad Norm=0.0047\n","Epoch 10, Batch 181: Loss=0.0001, Grad Norm=0.0022\n","Epoch 10, Batch 182: Loss=0.0003, Grad Norm=0.0097\n","Epoch 10, Batch 183: Loss=0.0001, Grad Norm=0.0014\n","Epoch 10, Batch 184: Loss=0.0009, Grad Norm=0.0278\n","Epoch 10, Batch 185: Loss=0.0001, Grad Norm=0.0024\n","Epoch 10, Batch 186: Loss=0.0006, Grad Norm=0.0197\n","Epoch 10, Batch 187: Loss=0.0001, Grad Norm=0.0018\n","Epoch 10, Batch 188: Loss=0.0001, Grad Norm=0.0043\n","Epoch 10/15:\n","Train Loss: 0.0088, Train Accuracy: 99.73%\n","Val Loss: 0.0046, Val Accuracy: 99.83%\n","Epoch 11, Batch 1: Loss=0.0004, Grad Norm=0.0123\n","Epoch 11, Batch 2: Loss=0.0008, Grad Norm=0.0193\n","Epoch 11, Batch 3: Loss=0.0035, Grad Norm=0.1251\n","Epoch 11, Batch 4: Loss=0.0038, Grad Norm=0.1180\n","Epoch 11, Batch 5: Loss=0.0001, Grad Norm=0.0032\n","Epoch 11, Batch 6: Loss=0.0009, Grad Norm=0.0232\n","Epoch 11, Batch 7: Loss=0.0012, Grad Norm=0.0325\n","Epoch 11, Batch 8: Loss=0.0001, Grad Norm=0.0046\n","Epoch 11, Batch 9: Loss=0.0005, Grad Norm=0.0156\n","Epoch 11, Batch 10: Loss=0.0000, Grad Norm=0.0012\n","Epoch 11, Batch 11: Loss=0.0390, Grad Norm=1.3116\n","Epoch 11, Batch 12: Loss=0.0059, Grad Norm=0.1187\n","Epoch 11, Batch 13: Loss=0.0000, Grad Norm=0.0010\n","Epoch 11, Batch 14: Loss=0.0016, Grad Norm=0.0686\n","Epoch 11, Batch 15: Loss=0.0050, Grad Norm=0.1722\n","Epoch 11, Batch 16: Loss=0.0003, Grad Norm=0.0066\n","Epoch 11, Batch 17: Loss=0.0003, Grad Norm=0.0118\n","Epoch 11, Batch 18: Loss=0.0089, Grad Norm=0.3355\n","Epoch 11, Batch 19: Loss=0.0008, Grad Norm=0.0360\n","Epoch 11, Batch 20: Loss=0.0000, Grad Norm=0.0014\n","Epoch 11, Batch 21: Loss=0.0030, Grad Norm=0.0710\n","Epoch 11, Batch 22: Loss=0.0001, Grad Norm=0.0032\n","Epoch 11, Batch 23: Loss=0.0001, Grad Norm=0.0055\n","Epoch 11, Batch 24: Loss=0.0230, Grad Norm=0.8402\n","Epoch 11, Batch 25: Loss=0.0002, Grad Norm=0.0037\n","Epoch 11, Batch 26: Loss=0.0002, Grad Norm=0.0051\n","Epoch 11, Batch 27: Loss=0.0037, Grad Norm=0.0892\n","Epoch 11, Batch 28: Loss=0.0001, Grad Norm=0.0025\n","Epoch 11, Batch 29: Loss=0.0003, Grad Norm=0.0085\n","Epoch 11, Batch 30: Loss=0.0001, Grad Norm=0.0015\n","Epoch 11, Batch 31: Loss=0.0004, Grad Norm=0.0145\n","Epoch 11, Batch 32: Loss=0.0002, Grad Norm=0.0038\n","Epoch 11, Batch 33: Loss=0.0227, Grad Norm=0.5313\n","Epoch 11, Batch 34: Loss=0.0001, Grad Norm=0.0017\n","Epoch 11, Batch 35: Loss=0.0013, Grad Norm=0.0416\n","Epoch 11, Batch 36: Loss=0.0002, Grad Norm=0.0033\n","Epoch 11, Batch 37: Loss=0.0010, Grad Norm=0.0210\n","Epoch 11, Batch 38: Loss=0.0016, Grad Norm=0.0690\n","Epoch 11, Batch 39: Loss=0.0065, Grad Norm=0.2184\n","Epoch 11, Batch 40: Loss=0.0024, Grad Norm=0.1025\n","Epoch 11, Batch 41: Loss=0.0007, Grad Norm=0.0192\n","Epoch 11, Batch 42: Loss=0.0002, Grad Norm=0.0056\n","Epoch 11, Batch 43: Loss=0.0003, Grad Norm=0.0084\n","Epoch 11, Batch 44: Loss=0.0000, Grad Norm=0.0013\n","Epoch 11, Batch 45: Loss=0.0001, Grad Norm=0.0027\n","Epoch 11, Batch 46: Loss=0.0008, Grad Norm=0.0361\n","Epoch 11, Batch 47: Loss=0.0275, Grad Norm=0.9806\n","Epoch 11, Batch 48: Loss=0.0013, Grad Norm=0.0364\n","Epoch 11, Batch 49: Loss=0.0001, Grad Norm=0.0023\n","Epoch 11, Batch 50: Loss=0.0002, Grad Norm=0.0041\n","Epoch 11, Batch 51: Loss=0.0000, Grad Norm=0.0006\n","Epoch 11, Batch 52: Loss=0.0002, Grad Norm=0.0051\n","Epoch 11, Batch 53: Loss=0.0011, Grad Norm=0.0300\n","Epoch 11, Batch 54: Loss=0.0001, Grad Norm=0.0023\n","Epoch 11, Batch 55: Loss=0.0000, Grad Norm=0.0002\n","Epoch 11, Batch 56: Loss=0.0013, Grad Norm=0.0491\n","Epoch 11, Batch 57: Loss=0.0001, Grad Norm=0.0018\n","Epoch 11, Batch 58: Loss=0.0005, Grad Norm=0.0095\n","Epoch 11, Batch 59: Loss=0.0091, Grad Norm=0.2643\n","Epoch 11, Batch 60: Loss=0.0045, Grad Norm=0.1947\n","Epoch 11, Batch 61: Loss=0.0002, Grad Norm=0.0042\n","Epoch 11, Batch 62: Loss=0.0014, Grad Norm=0.0396\n","Epoch 11, Batch 63: Loss=0.0009, Grad Norm=0.0325\n","Epoch 11, Batch 64: Loss=0.0001, Grad Norm=0.0014\n","Epoch 11, Batch 65: Loss=0.0001, Grad Norm=0.0027\n","Epoch 11, Batch 66: Loss=0.0001, Grad Norm=0.0018\n","Epoch 11, Batch 67: Loss=0.0006, Grad Norm=0.0312\n","Epoch 11, Batch 68: Loss=0.0091, Grad Norm=0.3571\n","Epoch 11, Batch 69: Loss=0.0001, Grad Norm=0.0017\n","Epoch 11, Batch 70: Loss=0.0015, Grad Norm=0.0618\n","Epoch 11, Batch 71: Loss=0.1380, Grad Norm=3.3155\n","Epoch 11, Batch 72: Loss=0.0013, Grad Norm=0.0315\n","Epoch 11, Batch 73: Loss=0.3597, Grad Norm=4.8766\n","Epoch 11, Batch 74: Loss=0.0003, Grad Norm=0.0052\n","Epoch 11, Batch 75: Loss=0.0150, Grad Norm=0.9274\n","Epoch 11, Batch 76: Loss=0.0781, Grad Norm=2.0784\n","Epoch 11, Batch 77: Loss=0.0110, Grad Norm=0.2980\n","Epoch 11, Batch 78: Loss=0.0159, Grad Norm=0.5269\n","Epoch 11, Batch 79: Loss=0.0036, Grad Norm=0.1089\n","Epoch 11, Batch 80: Loss=0.0060, Grad Norm=0.1532\n","Epoch 11, Batch 81: Loss=0.0001, Grad Norm=0.0047\n","Epoch 11, Batch 82: Loss=0.0006, Grad Norm=0.0157\n","Epoch 11, Batch 83: Loss=0.0015, Grad Norm=0.0496\n","Epoch 11, Batch 84: Loss=0.0002, Grad Norm=0.0087\n","Epoch 11, Batch 85: Loss=0.0040, Grad Norm=0.1030\n","Epoch 11, Batch 86: Loss=0.0005, Grad Norm=0.0184\n","Epoch 11, Batch 87: Loss=0.0002, Grad Norm=0.0040\n","Epoch 11, Batch 88: Loss=0.0005, Grad Norm=0.0119\n","Epoch 11, Batch 89: Loss=0.0011, Grad Norm=0.0350\n","Epoch 11, Batch 90: Loss=0.1094, Grad Norm=2.5419\n","Epoch 11, Batch 91: Loss=0.0001, Grad Norm=0.0030\n","Epoch 11, Batch 92: Loss=0.0001, Grad Norm=0.0037\n","Epoch 11, Batch 93: Loss=0.0013, Grad Norm=0.0336\n","Epoch 11, Batch 94: Loss=0.0003, Grad Norm=0.0090\n","Epoch 11, Batch 95: Loss=0.0000, Grad Norm=0.0004\n","Epoch 11, Batch 96: Loss=0.0021, Grad Norm=0.0608\n","Epoch 11, Batch 97: Loss=0.0027, Grad Norm=0.0558\n","Epoch 11, Batch 98: Loss=0.0470, Grad Norm=2.0621\n","Epoch 11, Batch 99: Loss=0.0004, Grad Norm=0.0090\n","Epoch 11, Batch 100: Loss=0.0002, Grad Norm=0.0042\n","Epoch 11, Batch 101: Loss=0.0001, Grad Norm=0.0034\n","Epoch 11, Batch 102: Loss=0.0001, Grad Norm=0.0033\n","Epoch 11, Batch 103: Loss=0.0001, Grad Norm=0.0017\n","Epoch 11, Batch 104: Loss=0.0253, Grad Norm=0.6359\n","Epoch 11, Batch 105: Loss=0.0003, Grad Norm=0.0065\n","Epoch 11, Batch 106: Loss=0.0001, Grad Norm=0.0016\n","Epoch 11, Batch 107: Loss=0.0026, Grad Norm=0.0918\n","Epoch 11, Batch 108: Loss=0.0035, Grad Norm=0.0960\n","Epoch 11, Batch 109: Loss=0.0067, Grad Norm=0.2095\n","Epoch 11, Batch 110: Loss=0.0415, Grad Norm=1.0023\n","Epoch 11, Batch 111: Loss=0.0011, Grad Norm=0.0609\n","Epoch 11, Batch 112: Loss=0.0001, Grad Norm=0.0038\n","Epoch 11, Batch 113: Loss=0.0009, Grad Norm=0.0456\n","Epoch 11, Batch 114: Loss=0.0004, Grad Norm=0.0102\n","Epoch 11, Batch 115: Loss=0.0011, Grad Norm=0.0336\n","Epoch 11, Batch 116: Loss=0.0001, Grad Norm=0.0035\n","Epoch 11, Batch 117: Loss=0.0050, Grad Norm=0.1443\n","Epoch 11, Batch 118: Loss=0.0068, Grad Norm=0.1614\n","Epoch 11, Batch 119: Loss=0.0002, Grad Norm=0.0046\n","Epoch 11, Batch 120: Loss=0.0002, Grad Norm=0.0066\n","Epoch 11, Batch 121: Loss=0.0001, Grad Norm=0.0020\n","Epoch 11, Batch 122: Loss=0.0030, Grad Norm=0.0768\n","Epoch 11, Batch 123: Loss=0.0003, Grad Norm=0.0117\n","Epoch 11, Batch 124: Loss=0.0000, Grad Norm=0.0013\n","Epoch 11, Batch 125: Loss=0.0008, Grad Norm=0.0349\n","Epoch 11, Batch 126: Loss=0.0021, Grad Norm=0.0597\n","Epoch 11, Batch 127: Loss=0.0004, Grad Norm=0.0134\n","Epoch 11, Batch 128: Loss=0.0041, Grad Norm=0.1003\n","Epoch 11, Batch 129: Loss=0.0002, Grad Norm=0.0041\n","Epoch 11, Batch 130: Loss=0.0007, Grad Norm=0.0177\n","Epoch 11, Batch 131: Loss=0.2751, Grad Norm=4.5556\n","Epoch 11, Batch 132: Loss=0.0007, Grad Norm=0.0245\n","Epoch 11, Batch 133: Loss=0.0002, Grad Norm=0.0097\n","Epoch 11, Batch 134: Loss=0.0002, Grad Norm=0.0041\n","Epoch 11, Batch 135: Loss=0.0003, Grad Norm=0.0086\n","Epoch 11, Batch 136: Loss=0.0001, Grad Norm=0.0018\n","Epoch 11, Batch 137: Loss=0.0021, Grad Norm=0.0553\n","Epoch 11, Batch 138: Loss=0.0003, Grad Norm=0.0081\n","Epoch 11, Batch 139: Loss=0.0007, Grad Norm=0.0277\n","Epoch 11, Batch 140: Loss=0.0016, Grad Norm=0.0712\n","Epoch 11, Batch 141: Loss=0.0004, Grad Norm=0.0093\n","Epoch 11, Batch 142: Loss=0.0021, Grad Norm=0.0878\n","Epoch 11, Batch 143: Loss=0.0016, Grad Norm=0.0690\n","Epoch 11, Batch 144: Loss=0.0008, Grad Norm=0.0209\n","Epoch 11, Batch 145: Loss=0.0267, Grad Norm=0.8402\n","Epoch 11, Batch 146: Loss=0.0013, Grad Norm=0.0418\n","Epoch 11, Batch 147: Loss=0.0017, Grad Norm=0.0573\n","Epoch 11, Batch 148: Loss=0.0008, Grad Norm=0.0265\n","Epoch 11, Batch 149: Loss=0.0011, Grad Norm=0.0374\n","Epoch 11, Batch 150: Loss=0.0007, Grad Norm=0.0264\n","Epoch 11, Batch 151: Loss=0.0003, Grad Norm=0.0080\n","Epoch 11, Batch 152: Loss=0.0006, Grad Norm=0.0188\n","Epoch 11, Batch 153: Loss=0.0009, Grad Norm=0.0352\n","Epoch 11, Batch 154: Loss=0.0003, Grad Norm=0.0108\n","Epoch 11, Batch 155: Loss=0.0005, Grad Norm=0.0210\n","Epoch 11, Batch 156: Loss=0.0001, Grad Norm=0.0022\n","Epoch 11, Batch 157: Loss=0.0004, Grad Norm=0.0150\n","Epoch 11, Batch 158: Loss=0.0002, Grad Norm=0.0048\n","Epoch 11, Batch 159: Loss=0.0001, Grad Norm=0.0040\n","Epoch 11, Batch 160: Loss=0.0001, Grad Norm=0.0032\n","Epoch 11, Batch 161: Loss=0.0034, Grad Norm=0.0859\n","Epoch 11, Batch 162: Loss=0.0054, Grad Norm=0.1656\n","Epoch 11, Batch 163: Loss=0.0004, Grad Norm=0.0097\n","Epoch 11, Batch 164: Loss=0.0051, Grad Norm=0.1582\n","Epoch 11, Batch 165: Loss=0.0002, Grad Norm=0.0047\n","Epoch 11, Batch 166: Loss=0.0000, Grad Norm=0.0007\n","Epoch 11, Batch 167: Loss=0.0043, Grad Norm=0.1425\n","Epoch 11, Batch 168: Loss=0.0008, Grad Norm=0.0213\n","Epoch 11, Batch 169: Loss=0.0003, Grad Norm=0.0109\n","Epoch 11, Batch 170: Loss=0.0001, Grad Norm=0.0041\n","Epoch 11, Batch 171: Loss=0.0004, Grad Norm=0.0104\n","Epoch 11, Batch 172: Loss=0.0007, Grad Norm=0.0288\n","Epoch 11, Batch 173: Loss=0.0077, Grad Norm=0.3091\n","Epoch 11, Batch 174: Loss=0.0254, Grad Norm=0.5046\n","Epoch 11, Batch 175: Loss=0.0080, Grad Norm=0.3661\n","Epoch 11, Batch 176: Loss=0.0017, Grad Norm=0.0521\n","Epoch 11, Batch 177: Loss=0.0079, Grad Norm=0.2556\n","Epoch 11, Batch 178: Loss=0.0009, Grad Norm=0.0208\n","Epoch 11, Batch 179: Loss=0.0001, Grad Norm=0.0024\n","Epoch 11, Batch 180: Loss=0.0048, Grad Norm=0.1609\n","Epoch 11, Batch 181: Loss=0.0006, Grad Norm=0.0130\n","Epoch 11, Batch 182: Loss=0.0007, Grad Norm=0.0166\n","Epoch 11, Batch 183: Loss=0.0010, Grad Norm=0.0282\n","Epoch 11, Batch 184: Loss=0.0067, Grad Norm=0.1533\n","Epoch 11, Batch 185: Loss=0.0094, Grad Norm=0.4085\n","Epoch 11, Batch 186: Loss=0.0001, Grad Norm=0.0011\n","Epoch 11, Batch 187: Loss=0.0015, Grad Norm=0.0445\n","Epoch 11, Batch 188: Loss=1.0062, Grad Norm=9.5550\n","Epoch 11/15:\n","Train Loss: 0.0135, Train Accuracy: 99.63%\n","Val Loss: 0.0069, Val Accuracy: 99.83%\n","Epoch 12, Batch 1: Loss=0.0334, Grad Norm=1.1063\n","Epoch 12, Batch 2: Loss=0.0002, Grad Norm=0.0050\n","Epoch 12, Batch 3: Loss=0.0003, Grad Norm=0.0081\n","Epoch 12, Batch 4: Loss=0.0009, Grad Norm=0.0276\n","Epoch 12, Batch 5: Loss=0.0002, Grad Norm=0.0035\n","Epoch 12, Batch 6: Loss=0.0004, Grad Norm=0.0131\n","Epoch 12, Batch 7: Loss=0.0000, Grad Norm=0.0010\n","Epoch 12, Batch 8: Loss=0.0006, Grad Norm=0.0252\n","Epoch 12, Batch 9: Loss=0.0001, Grad Norm=0.0048\n","Epoch 12, Batch 10: Loss=0.0384, Grad Norm=1.7488\n","Epoch 12, Batch 11: Loss=0.0002, Grad Norm=0.0069\n","Epoch 12, Batch 12: Loss=0.0041, Grad Norm=0.1767\n","Epoch 12, Batch 13: Loss=0.0023, Grad Norm=0.0610\n","Epoch 12, Batch 14: Loss=0.0001, Grad Norm=0.0028\n","Epoch 12, Batch 15: Loss=0.0229, Grad Norm=0.9668\n","Epoch 12, Batch 16: Loss=0.0003, Grad Norm=0.0128\n","Epoch 12, Batch 17: Loss=0.0013, Grad Norm=0.0367\n","Epoch 12, Batch 18: Loss=0.0001, Grad Norm=0.0031\n","Epoch 12, Batch 19: Loss=0.0180, Grad Norm=0.4385\n","Epoch 12, Batch 20: Loss=0.0265, Grad Norm=0.6772\n","Epoch 12, Batch 21: Loss=0.0005, Grad Norm=0.0200\n","Epoch 12, Batch 22: Loss=0.0019, Grad Norm=0.0588\n","Epoch 12, Batch 23: Loss=0.0021, Grad Norm=0.0572\n","Epoch 12, Batch 24: Loss=0.0004, Grad Norm=0.0099\n","Epoch 12, Batch 25: Loss=0.0813, Grad Norm=1.7280\n","Epoch 12, Batch 26: Loss=0.0000, Grad Norm=0.0010\n","Epoch 12, Batch 27: Loss=0.0010, Grad Norm=0.0286\n","Epoch 12, Batch 28: Loss=0.0041, Grad Norm=0.0812\n","Epoch 12, Batch 29: Loss=0.0002, Grad Norm=0.0062\n","Epoch 12, Batch 30: Loss=0.0000, Grad Norm=0.0012\n","Epoch 12, Batch 31: Loss=0.0004, Grad Norm=0.0143\n","Epoch 12, Batch 32: Loss=0.0000, Grad Norm=0.0012\n","Epoch 12, Batch 33: Loss=0.0311, Grad Norm=0.9577\n","Epoch 12, Batch 34: Loss=0.0010, Grad Norm=0.0304\n","Epoch 12, Batch 35: Loss=0.0523, Grad Norm=1.5320\n","Epoch 12, Batch 36: Loss=0.0055, Grad Norm=0.1735\n","Epoch 12, Batch 37: Loss=0.0034, Grad Norm=0.1226\n","Epoch 12, Batch 38: Loss=0.0000, Grad Norm=0.0001\n","Epoch 12, Batch 39: Loss=0.0000, Grad Norm=0.0013\n","Epoch 12, Batch 40: Loss=0.0001, Grad Norm=0.0028\n","Epoch 12, Batch 41: Loss=0.0002, Grad Norm=0.0050\n","Epoch 12, Batch 42: Loss=0.0053, Grad Norm=0.3140\n","Epoch 12, Batch 43: Loss=0.0006, Grad Norm=0.0138\n","Epoch 12, Batch 44: Loss=0.0003, Grad Norm=0.0094\n","Epoch 12, Batch 45: Loss=0.0009, Grad Norm=0.0241\n","Epoch 12, Batch 46: Loss=0.0001, Grad Norm=0.0018\n","Epoch 12, Batch 47: Loss=0.0001, Grad Norm=0.0026\n","Epoch 12, Batch 48: Loss=0.0013, Grad Norm=0.0524\n","Epoch 12, Batch 49: Loss=0.0006, Grad Norm=0.0254\n","Epoch 12, Batch 50: Loss=0.0003, Grad Norm=0.0138\n","Epoch 12, Batch 51: Loss=0.0198, Grad Norm=0.4386\n","Epoch 12, Batch 52: Loss=0.0002, Grad Norm=0.0054\n","Epoch 12, Batch 53: Loss=0.0001, Grad Norm=0.0027\n","Epoch 12, Batch 54: Loss=0.0011, Grad Norm=0.0411\n","Epoch 12, Batch 55: Loss=0.0025, Grad Norm=0.1131\n","Epoch 12, Batch 56: Loss=0.0002, Grad Norm=0.0067\n","Epoch 12, Batch 57: Loss=0.0003, Grad Norm=0.0076\n","Epoch 12, Batch 58: Loss=0.0008, Grad Norm=0.0281\n","Epoch 12, Batch 59: Loss=0.0145, Grad Norm=0.5197\n","Epoch 12, Batch 60: Loss=0.0013, Grad Norm=0.0345\n","Epoch 12, Batch 61: Loss=0.0002, Grad Norm=0.0096\n","Epoch 12, Batch 62: Loss=0.0003, Grad Norm=0.0074\n","Epoch 12, Batch 63: Loss=0.0009, Grad Norm=0.0321\n","Epoch 12, Batch 64: Loss=0.0017, Grad Norm=0.0400\n","Epoch 12, Batch 65: Loss=0.0106, Grad Norm=0.4554\n","Epoch 12, Batch 66: Loss=0.0000, Grad Norm=0.0006\n","Epoch 12, Batch 67: Loss=0.0001, Grad Norm=0.0010\n","Epoch 12, Batch 68: Loss=0.0002, Grad Norm=0.0052\n","Epoch 12, Batch 69: Loss=0.0011, Grad Norm=0.0531\n","Epoch 12, Batch 70: Loss=0.0011, Grad Norm=0.0318\n","Epoch 12, Batch 71: Loss=0.0002, Grad Norm=0.0043\n","Epoch 12, Batch 72: Loss=0.0003, Grad Norm=0.0129\n","Epoch 12, Batch 73: Loss=0.0181, Grad Norm=0.4372\n","Epoch 12, Batch 74: Loss=0.0000, Grad Norm=0.0011\n","Epoch 12, Batch 75: Loss=0.0071, Grad Norm=0.3166\n","Epoch 12, Batch 76: Loss=0.0005, Grad Norm=0.0139\n","Epoch 12, Batch 77: Loss=0.0020, Grad Norm=0.0652\n","Epoch 12, Batch 78: Loss=0.0344, Grad Norm=1.4958\n","Epoch 12, Batch 79: Loss=0.0006, Grad Norm=0.0241\n","Epoch 12, Batch 80: Loss=0.0000, Grad Norm=0.0012\n","Epoch 12, Batch 81: Loss=0.0023, Grad Norm=0.0721\n","Epoch 12, Batch 82: Loss=0.0001, Grad Norm=0.0024\n","Epoch 12, Batch 83: Loss=0.0030, Grad Norm=0.0881\n","Epoch 12, Batch 84: Loss=0.3965, Grad Norm=5.4062\n","Epoch 12, Batch 85: Loss=0.0001, Grad Norm=0.0025\n","Epoch 12, Batch 86: Loss=0.0005, Grad Norm=0.0134\n","Epoch 12, Batch 87: Loss=0.0001, Grad Norm=0.0016\n","Epoch 12, Batch 88: Loss=0.0303, Grad Norm=0.9145\n","Epoch 12, Batch 89: Loss=0.0015, Grad Norm=0.0403\n","Epoch 12, Batch 90: Loss=0.0006, Grad Norm=0.0219\n","Epoch 12, Batch 91: Loss=0.0005, Grad Norm=0.0141\n","Epoch 12, Batch 92: Loss=0.0010, Grad Norm=0.0322\n","Epoch 12, Batch 93: Loss=0.0024, Grad Norm=0.0674\n","Epoch 12, Batch 94: Loss=0.0001, Grad Norm=0.0029\n","Epoch 12, Batch 95: Loss=0.0001, Grad Norm=0.0032\n","Epoch 12, Batch 96: Loss=0.1030, Grad Norm=2.1130\n","Epoch 12, Batch 97: Loss=0.0001, Grad Norm=0.0022\n","Epoch 12, Batch 98: Loss=0.0003, Grad Norm=0.0091\n","Epoch 12, Batch 99: Loss=0.0017, Grad Norm=0.0474\n","Epoch 12, Batch 100: Loss=0.0012, Grad Norm=0.0215\n","Epoch 12, Batch 101: Loss=0.0002, Grad Norm=0.0053\n","Epoch 12, Batch 102: Loss=0.0031, Grad Norm=0.1063\n","Epoch 12, Batch 103: Loss=0.0004, Grad Norm=0.0146\n","Epoch 12, Batch 104: Loss=0.0000, Grad Norm=0.0005\n","Epoch 12, Batch 105: Loss=0.0026, Grad Norm=0.0851\n","Epoch 12, Batch 106: Loss=0.0002, Grad Norm=0.0047\n","Epoch 12, Batch 107: Loss=0.0001, Grad Norm=0.0031\n","Epoch 12, Batch 108: Loss=0.0000, Grad Norm=0.0011\n","Epoch 12, Batch 109: Loss=0.0172, Grad Norm=0.7167\n","Epoch 12, Batch 110: Loss=0.0005, Grad Norm=0.0113\n","Epoch 12, Batch 111: Loss=0.0001, Grad Norm=0.0018\n","Epoch 12, Batch 112: Loss=0.0036, Grad Norm=0.1209\n","Epoch 12, Batch 113: Loss=0.0002, Grad Norm=0.0051\n","Epoch 12, Batch 114: Loss=0.0008, Grad Norm=0.0189\n","Epoch 12, Batch 115: Loss=0.0002, Grad Norm=0.0040\n","Epoch 12, Batch 116: Loss=0.0014, Grad Norm=0.0377\n","Epoch 12, Batch 117: Loss=0.0028, Grad Norm=0.0999\n","Epoch 12, Batch 118: Loss=0.0213, Grad Norm=0.8215\n","Epoch 12, Batch 119: Loss=0.0142, Grad Norm=0.4229\n","Epoch 12, Batch 120: Loss=0.0009, Grad Norm=0.0362\n","Epoch 12, Batch 121: Loss=0.0012, Grad Norm=0.0681\n","Epoch 12, Batch 122: Loss=0.0036, Grad Norm=0.1051\n","Epoch 12, Batch 123: Loss=0.0954, Grad Norm=2.3015\n","Epoch 12, Batch 124: Loss=0.0000, Grad Norm=0.0004\n","Epoch 12, Batch 125: Loss=0.0039, Grad Norm=0.1503\n","Epoch 12, Batch 126: Loss=0.0570, Grad Norm=1.6784\n","Epoch 12, Batch 127: Loss=0.0182, Grad Norm=0.8257\n","Epoch 12, Batch 128: Loss=0.0003, Grad Norm=0.0093\n","Epoch 12, Batch 129: Loss=0.0004, Grad Norm=0.0138\n","Epoch 12, Batch 130: Loss=0.0004, Grad Norm=0.0166\n","Epoch 12, Batch 131: Loss=0.0009, Grad Norm=0.0287\n","Epoch 12, Batch 132: Loss=0.0000, Grad Norm=0.0010\n","Epoch 12, Batch 133: Loss=0.0003, Grad Norm=0.0085\n","Epoch 12, Batch 134: Loss=0.0007, Grad Norm=0.0202\n","Epoch 12, Batch 135: Loss=0.0008, Grad Norm=0.0225\n","Epoch 12, Batch 136: Loss=0.0017, Grad Norm=0.0359\n","Epoch 12, Batch 137: Loss=0.0013, Grad Norm=0.0494\n","Epoch 12, Batch 138: Loss=0.0038, Grad Norm=0.1097\n","Epoch 12, Batch 139: Loss=0.0017, Grad Norm=0.0376\n","Epoch 12, Batch 140: Loss=0.0004, Grad Norm=0.0123\n","Epoch 12, Batch 141: Loss=0.0000, Grad Norm=0.0014\n","Epoch 12, Batch 142: Loss=0.0023, Grad Norm=0.0680\n","Epoch 12, Batch 143: Loss=0.0021, Grad Norm=0.0648\n","Epoch 12, Batch 144: Loss=0.0013, Grad Norm=0.0310\n","Epoch 12, Batch 145: Loss=0.0001, Grad Norm=0.0025\n","Epoch 12, Batch 146: Loss=0.3698, Grad Norm=4.7047\n","Epoch 12, Batch 147: Loss=0.0001, Grad Norm=0.0025\n","Epoch 12, Batch 148: Loss=0.0001, Grad Norm=0.0049\n","Epoch 12, Batch 149: Loss=0.0026, Grad Norm=0.0924\n","Epoch 12, Batch 150: Loss=0.0002, Grad Norm=0.0087\n","Epoch 12, Batch 151: Loss=0.0003, Grad Norm=0.0055\n","Epoch 12, Batch 152: Loss=0.0004, Grad Norm=0.0169\n","Epoch 12, Batch 153: Loss=0.0139, Grad Norm=0.3885\n","Epoch 12, Batch 154: Loss=0.0006, Grad Norm=0.0169\n","Epoch 12, Batch 155: Loss=0.0003, Grad Norm=0.0054\n","Epoch 12, Batch 156: Loss=0.0001, Grad Norm=0.0052\n","Epoch 12, Batch 157: Loss=0.0002, Grad Norm=0.0072\n","Epoch 12, Batch 158: Loss=0.0002, Grad Norm=0.0067\n","Epoch 12, Batch 159: Loss=0.0546, Grad Norm=1.9848\n","Epoch 12, Batch 160: Loss=0.0002, Grad Norm=0.0063\n","Epoch 12, Batch 161: Loss=0.0239, Grad Norm=0.7696\n","Epoch 12, Batch 162: Loss=0.0042, Grad Norm=0.1608\n","Epoch 12, Batch 163: Loss=0.0103, Grad Norm=0.2103\n","Epoch 12, Batch 164: Loss=0.2585, Grad Norm=4.4466\n","Epoch 12, Batch 165: Loss=0.0002, Grad Norm=0.0055\n","Epoch 12, Batch 166: Loss=0.0007, Grad Norm=0.0187\n","Epoch 12, Batch 167: Loss=0.0020, Grad Norm=0.0656\n","Epoch 12, Batch 168: Loss=0.0000, Grad Norm=0.0006\n","Epoch 12, Batch 169: Loss=0.1110, Grad Norm=2.3380\n","Epoch 12, Batch 170: Loss=0.0006, Grad Norm=0.0242\n","Epoch 12, Batch 171: Loss=0.0003, Grad Norm=0.0064\n","Epoch 12, Batch 172: Loss=0.2089, Grad Norm=2.4935\n","Epoch 12, Batch 173: Loss=0.0102, Grad Norm=0.4135\n","Epoch 12, Batch 174: Loss=0.0011, Grad Norm=0.0371\n","Epoch 12, Batch 175: Loss=0.0005, Grad Norm=0.0406\n","Epoch 12, Batch 176: Loss=0.0037, Grad Norm=0.1361\n","Epoch 12, Batch 177: Loss=0.0092, Grad Norm=0.4995\n","Epoch 12, Batch 178: Loss=0.0008, Grad Norm=0.0269\n","Epoch 12, Batch 179: Loss=0.0001, Grad Norm=0.0025\n","Epoch 12, Batch 180: Loss=0.0071, Grad Norm=0.1608\n","Epoch 12, Batch 181: Loss=0.2047, Grad Norm=3.6680\n","Epoch 12, Batch 182: Loss=0.1244, Grad Norm=2.6785\n","Epoch 12, Batch 183: Loss=0.0004, Grad Norm=0.0095\n","Epoch 12, Batch 184: Loss=0.0026, Grad Norm=0.0699\n","Epoch 12, Batch 185: Loss=0.0015, Grad Norm=0.0693\n","Epoch 12, Batch 186: Loss=0.0162, Grad Norm=0.3177\n","Epoch 12, Batch 187: Loss=0.0052, Grad Norm=0.2473\n","Epoch 12, Batch 188: Loss=0.0436, Grad Norm=1.9233\n","Epoch 12/15:\n","Train Loss: 0.0148, Train Accuracy: 99.47%\n","Val Loss: 0.0067, Val Accuracy: 99.83%\n","Early stopping at epoch 12\n","\n","Training complete. Starting command-line interface...\n","\n","=== Phishing Detection System ===\n","Enter 'text' for text input or 'image' for image file path (or 'quit' to exit).\n","\n","Prediction: Phishing\n","Model used: DistilBERT\n","Confidence: 1.00\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Full image OCR failed: not enough values to unpack (expected 3, got 2)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Prediction: Legitimate\n","Model used: YOLO + EfficientNet + DistilBERT\n","Confidence: 0.99\n","Extracted Text: No text extracted\n","Warning: Low confidence or poor OCR. Try a clearer image with readable text.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from transformers import DistilBertTokenizer, DistilBertModel\n","import easyocr\n","import pytesseract\n","from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont\n","import numpy as np\n","import os\n","import random\n","import cv2\n","import logging\n","from collections import Counter\n","import urllib.parse\n","\n","# ======================\n","# Configuration\n","# ======================\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","MODEL_PATH = \"/content/drive/MyDrive/MINI-1/fusion_model.pth\"\n","CLASS_NAMES = [\"Legitimate\", \"Phishing\"]\n","BATCH_SIZE = 16\n","EPOCHS = 15\n","LEARNING_RATE = 0.001\n","WEIGHT_DECAY = 1e-3\n","LOGGING_LEVEL = logging.DEBUG\n","\n","# Set up logging\n","logging.basicConfig(level=LOGGING_LEVEL, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# ======================\n","# Synthetic Dataset\n","# ======================\n","class PhishingDataset(Dataset):\n","    def __init__(self, num_samples=3000, image_size=(224, 224), is_validation=False):\n","        self.num_samples = num_samples\n","        self.image_size = image_size\n","        self.is_validation = is_validation\n","        self.data = []\n","        self.labels = []\n","        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","        neutral_phrases = [\n","            \"review your account\", \"update your information\", \"action required\",\n","            \"important notice\", \"account status update\", \"complete your profile\"\n","        ]\n","        phishing_phrases = [\n","            \"SECURITY ALERT: ACCOUNT SUSPENDED\", \"UNAUTHORIZED ACCESS DETECTED\",\n","            \"VERIFY NOW OR LOSE ACCESS\", \"IMMEDIATE ACTION NEEDED\",\n","            \"SUSPICIOUS ACTIVITY DETECTED\", \"URGENT: RESET PASSWORD\"\n","        ]\n","\n","        if not is_validation:\n","            phishing_templates = [\n","                \"URGENT: {phrase}! {action} at https://{domain} within 24 hours or lose access.\",\n","                \"SECURITY ALERT: {phrase}. {action} now: https://{domain}\",\n","                \"{phrase}! Visit https://{domain} to {action} or risk account suspension.\",\n","                \"WARNING: {phrase}. Click https://{domain} to {action} IMMEDIATELY.\"\n","            ]\n","            phishing_domains = [\n","                \"secure-login-verify.com\", \"account-reset-urgent.net\", \"banking-security-check.org\",\n","                \"safe-access-now.info\", \"login-protect-secure.com\", \"verify-fast-alert.net\"\n","            ]\n","            legit_templates = [\n","                \"Your {phrase} is complete. {action} at https://{domain}.\",\n","                \"Thank you for your {phrase}. Visit https://{domain} to {action}.\",\n","                \"Confirmed: {phrase}. {action}: https://{domain}.\",\n","                \"{phrase} processed. Check https://{domain} for {action}.\"\n","            ]\n","            legit_domains = [\n","                \"official-shop.com\", \"customer-support.net\", \"track-order-portal.org\",\n","                \"events-rsvp.info\", \"billing-service.com\", \"user-account.net\"\n","            ]\n","        else:\n","            phishing_templates = [\n","                \"CRITICAL: {phrase}. Proceed to https://{domain} NOW.\",\n","                \"{phrase} DETECTED. {action} at https://{domain} IMMEDIATELY.\",\n","                \"ALERT: {phrase}. Secure https://{domain} to {action}.\",\n","                \"SECURITY NOTICE: {phrase}. Visit https://{domain} for {action}.\"\n","            ]\n","            phishing_domains = [\n","                \"auth-secure-verify.com\", \"account-check-urgent.org\", \"login-safe-now.net\",\n","                \"protect-access-alert.info\", \"security-urgent-check.com\", \"fast-verify-now.net\"\n","            ]\n","            legit_templates = [\n","                \"Your {phrase} is ready. {action} at https://{domain}.\",\n","                \"{phrase} successfully processed. Check https://{domain} for {action}.\",\n","                \"Update: {phrase}. {action} via https://{domain}.\",\n","                \"Thank you for {phrase}. Please {action} at https://{domain}.\"\n","            ]\n","            legit_domains = [\n","                \"my-order-portal.com\", \"support-official.net\", \"track-shipment.org\",\n","                \"event-portal.info\", \"billing-official.com\", \"account-service.net\"\n","            ]\n","\n","        actions = [\n","            \"click here\", \"log in\", \"update now\", \"verify now\", \"secure account\",\n","            \"view details\", \"track now\", \"confirm now\", \"check status\", \"proceed\"\n","        ]\n","        font_sizes = [18, 22, 26, 30]\n","        fonts = [\"arial.ttf\", \"times.ttf\", \"calibri.ttf\"]\n","\n","        half_samples = num_samples // 2\n","        for i in range(num_samples):\n","            is_phishing = 1 if i < half_samples else 0\n","            if is_phishing:\n","                phrase = random.choice(phishing_phrases)\n","                template = random.choice(phishing_templates)\n","                domain = random.choice(phishing_domains)\n","            else:\n","                phrase = random.choice(neutral_phrases)\n","                template = random.choice(legit_templates)\n","                domain = random.choice(legit_domains)\n","            action = random.choice(actions)\n","\n","            text = template.format(phrase=phrase, action=action, domain=domain)\n","\n","            if is_phishing and random.random() < 0.6:\n","                words = text.split()\n","                if words:\n","                    idx = random.randint(0, len(words)-1)\n","                    words[idx] = words[idx][:-1] + random.choice('qwerty')\n","                text = \" \".join(words) + \" \" + random.choice([\"NOW!\", \"URGENT!\", \"TODAY!\", \"IMMEDIATELY!\"])\n","            if random.random() < 0.4:\n","                prefix = f\"Subject: {phrase}\\nDear Customer,\\n\"\n","                suffix = \"\\nSincerely,\\nSecurity Team\" if is_phishing else \"\\nBest,\\nSupport Team\"\n","                text = prefix + text + suffix\n","\n","            if not isinstance(text, str):\n","                text = \"Default text\"\n","\n","            if i < 5:\n","                logger.info(f\"Sample {i+1} (is_phishing={is_phishing}): {text[:100]}...\")\n","\n","            image = Image.new('RGB', image_size, color=(random.randint(200, 255), random.randint(200, 255), random.randint(200, 255)))\n","            draw = ImageDraw.Draw(image)\n","            font_size = random.choice(font_sizes)\n","            font_path = random.choice(fonts) if os.path.exists(fonts[0]) else None\n","            try:\n","                font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()\n","            except:\n","                font = ImageFont.load_default()\n","\n","            for _ in range(random.randint(1, 2)):\n","                shape_type = random.choice(['rectangle', 'circle'])\n","                color = (random.randint(0, 150), random.randint(0, 150), random.randint(0, 150))\n","                if shape_type == 'rectangle':\n","                    x0, x1 = sorted(random.sample(range(image_size[0]), 2))\n","                    y0, y1 = sorted(random.sample(range(image_size[1]), 2))\n","                    draw.rectangle([(x0, y0), (x1, y1)], fill=color)\n","                else:\n","                    x0, x1 = sorted(random.sample(range(image_size[0]), 2))\n","                    y0, y1 = sorted(random.sample(range(image_size[1]), 2))\n","                    draw.ellipse([(x0, y0), (x1, y1)], fill=color)\n","\n","            text_position = (random.randint(10, 40), random.randint(10, 40))\n","            text_color = (0, 0, 0)\n","            image = image.rotate(random.uniform(-10, 10), expand=False)\n","            draw.text(text_position, text[:100], fill=text_color, font=font)\n","\n","            if random.random() < 0.3:\n","                overlay = Image.new('RGB', (50, 50), color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n","                image.paste(overlay, (random.randint(0, image_size[0]-50), random.randint(0, image_size[1]-50)))\n","\n","            image_np = np.array(image)\n","            noise = np.random.normal(0, 10, image_np.shape).astype(np.uint8)\n","            image_np = np.clip(image_np + noise, 0, 255)\n","            image = Image.fromarray(image_np)\n","\n","            self.data.append((image, text))\n","            self.labels.append(is_phishing)\n","\n","        label_counts = Counter(self.labels)\n","        logger.info(f\"Dataset label distribution: {label_counts}\")\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        image, text = self.data[idx]\n","        label = self.labels[idx]\n","        return image, text, label\n","\n","# ======================\n","# Model Definitions\n","# ======================\n","class FusionModel(nn.Module):\n","    def __init__(self, image_feat_size=1280, text_feat_size=768):\n","        super(FusionModel, self).__init__()\n","        self.fc1 = nn.Linear(image_feat_size + text_feat_size, 256)\n","        self.bn1 = nn.BatchNorm1d(256)\n","        self.relu = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc3 = nn.Linear(128, 2)\n","\n","        nn.init.xavier_uniform_(self.fc1.weight)\n","        nn.init.xavier_uniform_(self.fc2.weight)\n","        nn.init.xavier_uniform_(self.fc3.weight)\n","\n","    def forward(self, image_feat, text_feat):\n","        x = torch.cat((image_feat, text_feat), dim=1)\n","        x = self.relu(self.bn1(self.fc1(x)))\n","        x = self.dropout1(x)\n","        x = self.relu(self.fc2(x))\n","        x = self.dropout2(x)\n","        return self.fc3(x)\n","\n","# ======================\n","# Model Loading\n","# ======================\n","def load_models():\n","    try:\n","        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","        tinybert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(DEVICE)\n","        tinybert.eval()\n","\n","        efficientnet = models.efficientnet_b0(weights=\"IMAGENET1K_V1\").to(DEVICE)\n","        efficientnet.classifier = nn.Identity()\n","        efficientnet.eval()\n","\n","        fusion_model = FusionModel().to(DEVICE)\n","        if os.path.exists(MODEL_PATH):\n","            try:\n","                fusion_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","                logger.info(f\"Loaded trained FusionModel from {MODEL_PATH}\")\n","            except Exception as e:\n","                logger.warning(f\"Failed to load FusionModel checkpoint due to mismatch: {e}. Initializing new model.\")\n","                logger.info(\"Initialized new FusionModel with random weights.\")\n","        else:\n","            logger.info(\"Initialized new FusionModel with random weights.\")\n","\n","        reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n","\n","        if not os.path.exists(\"yolov4.cfg\") or not os.path.exists(\"yolov4.weights\"):\n","            raise FileNotFoundError(\"YOLO files (yolov4.cfg or yolov4.weights) not found in /content/\")\n","        yolo_model = cv2.dnn.readNetFromDarknet(\"yolov4.cfg\", \"yolov4.weights\")\n","        layer_names = yolo_model.getLayerNames()\n","        output_layers = [layer_names[i - 1] for i in yolo_model.getUnconnectedOutLayers()]\n","\n","        logger.info(\"All models loaded successfully\")\n","        return tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers\n","    except Exception as e:\n","        logger.error(f\"Failed to load models: {e}\")\n","        raise\n","\n","# ======================\n","# Preprocessing\n","# ======================\n","def preprocess_image(images):\n","    try:\n","        transform = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        if isinstance(images, Image.Image):\n","            images = [images]\n","        images = [img.convert('RGBA').convert('RGB') if img.mode == 'P' else img.convert('RGB') for img in images]\n","        return torch.stack([transform(img) for img in images]).to(DEVICE)\n","    except Exception as e:\n","        logger.error(f\"Image preprocessing failed: {e}\")\n","        raise\n","\n","def preprocess_for_ocr(image):\n","    try:\n","        if not isinstance(image, Image.Image):\n","            image = Image.open(image)\n","        if image.mode == 'P':\n","            image = image.convert('RGBA').convert('RGB')\n","        image = image.convert('L')\n","        enhancer = ImageEnhance.Contrast(image)\n","        image = enhancer.enhance(2.5)\n","        image = image.filter(ImageFilter.SHARPEN)\n","        image_np = np.array(image)\n","        image_np = cv2.fastNlMeansDenoising(image_np, h=10)\n","        image_np = cv2.adaptiveThreshold(image_np, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n","        return image_np\n","    except Exception as e:\n","        logger.error(f\"OCR preprocessing failed: {e}\")\n","        raise\n","\n","# ======================\n","# Custom Collate Function\n","# ======================\n","def custom_collate_fn(batch):\n","    try:\n","        images, texts, labels = zip(*batch)\n","        image_tensor = preprocess_image(images)\n","        texts = list(texts)\n","        labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n","        return image_tensor, texts, labels\n","    except Exception as e:\n","        logger.error(f\"Collate function failed: {e}\")\n","        raise\n","\n","# ======================\n","# Feature Extraction\n","# ======================\n","def extract_text_with_yolo(image, yolo_model, output_layers, reader, use_pytesseract_fallback=True):\n","    try:\n","        if isinstance(image, Image.Image):\n","            image_np = np.array(image.convert('RGB'))\n","        else:\n","            image_np = cv2.imread(image)\n","        if image_np is None:\n","            raise ValueError(\"Failed to load image\")\n","        height, width = image_np.shape[:2]\n","\n","        blob = cv2.dnn.blobFromImage(image_np, 1/255.0, (416, 416), swapRB=True, crop=False)\n","        yolo_model.setInput(blob)\n","        outputs = yolo_model.forward(output_layers)\n","\n","        boxes = []\n","        confidences = []\n","        class_ids = []\n","        for output in outputs:\n","            for detection in output:\n","                scores = detection[5:]\n","                class_id = np.argmax(scores)\n","                confidence = scores[class_id]\n","                if confidence > 0.2:\n","                    center_x = int(detection[0] * width)\n","                    center_y = int(detection[1] * height)\n","                    w = int(detection[2] * width)\n","                    h = int(detection[3] * height)\n","                    x = int(center_x - w / 2)\n","                    y = int(center_y - h / 2)\n","                    boxes.append([x, y, w, h])\n","                    confidences.append(float(confidence))\n","                    class_ids.append(class_id)\n","\n","        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.2, 0.4)\n","        extracted_text = []\n","        ocr_confidences = []\n","\n","        if len(indices) > 0:\n","            for i in indices.flatten():\n","                x, y, w, h = boxes[i]\n","                x = max(0, x)\n","                y = max(0, y)\n","                w = min(w, width - x)\n","                h = min(h, height - y)\n","                if w <= 0 or h <= 0:\n","                    continue\n","                roi = image_np[y:y+h, x:x+w]\n","                if roi.size == 0:\n","                    continue\n","                roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n","                roi_array = preprocess_for_ocr(roi_pil)\n","                try:\n","                    result = reader.readtext(roi_array, detail=1, paragraph=True)\n","                    for (bbox, text, conf) in result:\n","                        if conf > 0.2:\n","                            extracted_text.append(text.strip())\n","                            ocr_confidences.append(conf)\n","                    if not extracted_text and use_pytesseract_fallback:\n","                        text = pytesseract.image_to_string(roi_array, config='--psm 6').strip()\n","                        if text:\n","                            extracted_text.append(text)\n","                            ocr_confidences.append(0.5)\n","                except Exception as e:\n","                    logger.warning(f\"Text extraction failed for ROI: {e}\")\n","\n","        if not extracted_text:\n","            full_image_array = preprocess_for_ocr(image)\n","            try:\n","                result = reader.readtext(full_image_array, detail=1, paragraph=True)\n","                for (bbox, text, conf) in result:\n","                    if conf > 0.2:\n","                        extracted_text.append(text.strip())\n","                        ocr_confidences.append(conf)\n","            except Exception as e:\n","                logger.warning(f\"Full image OCR failed: {e}\")\n","\n","        final_text = \" \".join(extracted_text).strip() if extracted_text else \"No text extracted\"\n","        avg_conf = sum(ocr_confidences) / len(ocr_confidences) if ocr_confidences else 0.0\n","        logger.debug(f\"OCR extracted text: {final_text}, Average confidence: {avg_conf:.2f}\")\n","        return final_text\n","    except Exception as e:\n","        logger.error(f\"YOLO text extraction failed: {e}\")\n","        raise\n","\n","def extract_text_from_image(image):\n","    try:\n","        result = reader.readtext(np.array(image), detail=1)\n","        texts = [detection[1] for detection in result if detection[2] > 0.2]\n","        return \" \".join(texts).strip()\n","    except Exception as e:\n","        logger.error(f\"Image text extraction failed: {e}\")\n","        raise\n","\n","def extract_image_features(image_tensor, efficientnet):\n","    try:\n","        with torch.no_grad():\n","            features = efficientnet(image_tensor)\n","        return features.float()\n","    except Exception as e:\n","        logger.error(f\"Image feature extraction failed: {e}\")\n","        raise\n","\n","def extract_text_features(texts, tokenizer, tinybert):\n","    try:\n","        if isinstance(texts, str):\n","            texts = [texts]\n","        texts = [str(text) if text else \" \" for text in texts]\n","        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n","        with torch.no_grad():\n","            outputs = tinybert(**inputs)\n","        return outputs.last_hidden_state[:, 0, :].float()\n","    except Exception as e:\n","        logger.error(f\"Text feature extraction failed: {e}\")\n","        raise\n","\n","# ======================\n","# Training\n","# ======================\n","def train_model(fusion_model, efficientnet, tinybert, tokenizer, train_loader, val_loader, epochs=EPOCHS):\n","    try:\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","        scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n","\n","        best_val_loss = float('inf')\n","        patience = 4\n","        patience_counter = 0\n","        min_loss_threshold = 0.01\n","\n","        for epoch in range(epochs):\n","            fusion_model.train()\n","            train_loss = 0.0\n","            train_correct = 0\n","            train_total = 0\n","\n","            for batch_idx, (image_tensor, texts, labels) in enumerate(train_loader):\n","                optimizer.zero_grad()\n","                image_features = extract_image_features(image_tensor, efficientnet)\n","                text_features = extract_text_features(texts, tokenizer, tinybert)\n","                labels = labels.to(DEVICE)\n","\n","                outputs = fusion_model(image_features, text_features)\n","                loss = criterion(outputs, labels)\n","\n","                loss.backward()\n","                grad_norm = torch.nn.utils.clip_grad_norm_(fusion_model.parameters(), max_norm=15.0)\n","                optimizer.step()\n","\n","                train_loss += loss.item()\n","                _, predicted = torch.max(outputs, 1)\n","                train_total += labels.size(0)\n","                train_correct += (predicted == labels).sum().item()\n","\n","                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}: Loss={loss.item():.4f}, Grad Norm={grad_norm:.4f}\")\n","\n","            scheduler.step()\n","            train_accuracy = 100 * train_correct / train_total\n","            train_loss_avg = train_loss / len(train_loader)\n","\n","            fusion_model.eval()\n","            val_loss = 0.0\n","            val_correct = 0\n","            val_total = 0\n","            with torch.no_grad():\n","                for batch_idx, (image_tensor, texts, labels) in enumerate(val_loader):\n","                    image_features = extract_image_features(image_tensor, efficientnet)\n","                    text_features = extract_text_features(texts, tokenizer, tinybert)\n","                    labels = labels.to(DEVICE)\n","\n","                    outputs = fusion_model(image_features, text_features)\n","                    loss = criterion(outputs, labels)\n","\n","                    val_loss += loss.item()\n","                    _, predicted = torch.max(outputs, 1)\n","                    val_total += labels.size(0)\n","                    val_correct += (predicted == labels).sum().item()\n","\n","                    if batch_idx == 0:\n","                        for i in range(min(3, len(texts))):\n","                            logger.debug(f\"Val sample {i+1}: Text={texts[i][:50]}..., Pred={CLASS_NAMES[predicted[i]]}, True={CLASS_NAMES[labels[i]]}\")\n","\n","            val_accuracy = 100 * val_correct / val_total\n","            val_loss_avg = val_loss / len(val_loader)\n","\n","            print(f\"Epoch {epoch+1}/{epochs}:\")\n","            print(f\"Train Loss: {train_loss_avg:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n","            print(f\"Val Loss: {val_loss_avg:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n","            logger.info(f\"Epoch {epoch+1}/{epochs}: Train Loss={train_loss_avg:.4f}, Train Acc={train_accuracy:.2f}%, Val Loss={val_loss_avg:.4f}, Val Acc={val_accuracy:.2f}%\")\n","\n","            if val_loss_avg < best_val_loss and val_loss_avg > min_loss_threshold:\n","                best_val_loss = val_loss_avg\n","                patience_counter = 0\n","                torch.save(fusion_model.state_dict(), MODEL_PATH)\n","                print(f\"Model saved (improved val loss: {best_val_loss:.4f})\")\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    print(f\"Early stopping at epoch {epoch+1}\")\n","                    break\n","\n","        if patience_counter < patience:\n","            torch.save(fusion_model.state_dict(), MODEL_PATH)\n","            print(f\"Trained model saved to {MODEL_PATH}\")\n","    except Exception as e:\n","        logger.error(f\"Training failed: {e}\")\n","        raise\n","\n","# ======================\n","# Classification\n","# ======================\n","def classify_input(input_data, is_image, tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers):\n","    try:\n","        if is_image:\n","            if input_data is None:\n","                raise ValueError(\"Image input is None\")\n","            image_tensor = preprocess_image([input_data])[0:1]\n","            image_features = extract_image_features(image_tensor, efficientnet)\n","            extracted_text = extract_text_with_yolo(input_data, yolo_model, output_layers, reader)\n","            text_features = extract_text_features([extracted_text], tokenizer, tinybert)\n","            model_used = \"YOLO + EfficientNet + DistilBERT\"\n","        else:\n","            if not input_data:\n","                raise ValueError(\"Text input is empty\")\n","            image_features = torch.zeros((1, 1280)).to(DEVICE)\n","            text_features = extract_text_features([input_data], tokenizer, tinybert)\n","            extracted_text = input_data\n","            model_used = \"DistilBERT\"\n","\n","        suspicious_domains = [\"secure-\", \"verify-\", \"login-\", \"auth-\", \"banking-\", \"reset-\", \"urgent-\"]\n","        suspicious_keywords = [\"urgent\", \"suspended\", \"verify now\", \"security alert\", \"immediate\", \"unauthorized\"]\n","        is_suspicious = False\n","        text_lower = extracted_text.lower()\n","        if any(keyword in text_lower for keyword in suspicious_keywords):\n","            is_suspicious = True\n","            logger.debug(f\"Suspicious keyword detected: {text_lower[:50]}...\")\n","        if any(domain in text_lower for domain in suspicious_domains):\n","            try:\n","                urls = [word for word in extracted_text.split() if word.startswith(\"https://\")]\n","                for url in urls:\n","                    parsed = urllib.parse.urlparse(url)\n","                    if any(susp in parsed.netloc.lower() for susp in suspicious_domains):\n","                        is_suspicious = True\n","                        logger.debug(f\"Suspicious domain detected: {parsed.netloc}\")\n","            except Exception as e:\n","                logger.warning(f\"URL parsing failed: {e}\")\n","\n","        with torch.no_grad():\n","            prediction_tensor = fusion_model(image_features, text_features)\n","            probabilities = torch.softmax(prediction_tensor, dim=1)\n","            predicted_label = torch.argmax(probabilities).item()\n","            confidence = probabilities[0, predicted_label].item()\n","\n","        prediction = CLASS_NAMES[predicted_label]\n","        if is_suspicious and prediction == \"Legitimate\":\n","            prediction = \"Phishing\"\n","            logger.debug(\"Overriding prediction to Phishing due to suspicious content\")\n","\n","        logger.debug(f\"Classification result: {prediction}, Confidence: {confidence:.2f}, Model: {model_used}, Extracted Text: {extracted_text[:100]}...\")\n","        return {\n","            \"input_type\": \"image\" if is_image else \"text\",\n","            \"prediction\": prediction,\n","            \"model_used\": model_used,\n","            \"extracted_text\": extracted_text if is_image else None,\n","            \"confidence\": confidence\n","        }\n","    except Exception as e:\n","        logger.error(f\"Classification failed: {e}\")\n","        raise\n","\n","# ======================\n","# Command-Line Interface\n","# ======================\n","def run_command_line_interface(tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers):\n","    print(\"\\n=== Phishing Detection System ===\")\n","    print(\"Enter 'text' for text input or 'image' for image file path (or 'quit' to exit).\")\n","\n","    while True:\n","        try:\n","            choice = input(\"\\nInput type (text/image/quit): \").strip().lower()\n","            if choice == 'quit':\n","                print(\"Exiting...\")\n","                break\n","            elif choice not in ['text', 'image']:\n","                print(\"Invalid choice. Please enter 'text', 'image', or 'quit'.\")\n","                continue\n","\n","            if choice == 'text':\n","                text = input(\"Enter text: \").strip()\n","                if not text:\n","                    print(\"Error: Text input cannot be empty.\")\n","                    continue\n","                result = classify_input(text, is_image=False, tokenizer=tokenizer, tinybert=tinybert,\n","                                       efficientnet=efficientnet, fusion_model=fusion_model, reader=reader,\n","                                       yolo_model=yolo_model, output_layers=output_layers)\n","                print(f\"\\nPrediction: {result['prediction']}\")\n","                print(f\"Model used: {result['model_used']}\")\n","                print(f\"Confidence: {result['confidence']:.2f}\")\n","                if result['confidence'] < 0.7:\n","                    print(\"Warning: Low confidence. Prediction may be unreliable.\")\n","\n","            elif choice == 'image':\n","                file_path = input(\"Enter image file path (e.g., /content/test_image.png): \").strip()\n","                if not os.path.exists(file_path):\n","                    print(f\"Error: File '{file_path}' not found.\")\n","                    continue\n","                try:\n","                    image = Image.open(file_path)\n","                except Exception as e:\n","                    print(f\"Error: Failed to open image '{file_path}': {e}\")\n","                    continue\n","                result = classify_input(image, is_image=True, tokenizer=tokenizer, tinybert=tinybert,\n","                                       efficientnet=efficientnet, fusion_model=fusion_model, reader=reader,\n","                                       yolo_model=yolo_model, output_layers=output_layers)\n","                print(f\"\\nPrediction: {result['prediction']}\")\n","                print(f\"Model used: {result['model_used']}\")\n","                print(f\"Confidence: {result['confidence']:.2f}\")\n","                print(f\"Extracted Text: {result['extracted_text']}\")\n","                if result['confidence'] < 0.7 or \"No text extracted\" in result['extracted_text'] or len(result['extracted_text'].split()) < 5:\n","                    print(\"Warning: Low confidence or poor OCR. Try a clearer image with readable text.\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error processing input: {e}\")\n","            print(f\"Error processing input: {e}\")\n","\n","# ======================\n","# Main Execution\n","# ======================\n","if __name__ == \"__main__\":\n","    try:\n","        train_dataset = PhishingDataset(num_samples=3000, is_validation=False)\n","        val_dataset = PhishingDataset(num_samples=600, is_validation=True)\n","        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n","        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n","\n","        tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers = load_models()\n","\n","        print(\"Starting training...\")\n","        logger.info(\"Starting training...\")\n","        train_model(fusion_model, efficientnet, tinybert, tokenizer, train_loader, val_loader)\n","\n","        print(\"\\nTraining complete. Starting command-line interface...\")\n","        logger.info(\"Starting command-line interface...\")\n","        run_command_line_interface(tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers)\n","    except Exception as e:\n","        logger.error(f\"Main execution failed: {e}\")\n","        print(f\"Main execution failed: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":722},"executionInfo":{"elapsed":5575,"status":"ok","timestamp":1747750459828,"user":{"displayName":"228W1A5430_Sec-A Kavuri Kushalava","userId":"01804207717655710695"},"user_tz":-330},"id":"uRZGtTiy4wep","outputId":"797c4695-8cc5-4317-e3e7-424e086e5994"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating model on test set...\n","=== Evaluation Metrics ===\n","Accuracy : 1.0000\n","Precision: 1.0000\n","Recall   : 1.0000\n","F1 Score : 1.0000\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","  Legitimate       1.00      1.00      1.00        25\n","    Phishing       1.00      1.00      1.00        25\n","\n","    accuracy                           1.00        50\n","   macro avg       1.00      1.00      1.00        50\n","weighted avg       1.00      1.00      1.00        50\n","\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 600x400 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def evaluate_model(fusion_model, efficientnet, tinybert, tokenizer, test_loader):\n","    fusion_model.eval()\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for image_tensor, texts, labels in test_loader:\n","            image_features = extract_image_features(image_tensor, efficientnet)\n","            text_features = extract_text_features(texts, tokenizer, tinybert)\n","            outputs = fusion_model(image_features, text_features)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(predicted.cpu().numpy())\n","\n","    # Compute evaluation metrics\n","    acc = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='binary', pos_label=1)  # Phishing is 1\n","    recall = recall_score(all_labels, all_preds, average='binary', pos_label=1)\n","    f1 = f1_score(all_labels, all_preds, average='binary', pos_label=1)\n","\n","    # Print metrics\n","    print(\"=== Evaluation Metrics ===\")\n","    print(f\"Accuracy : {acc:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall   : {recall:.4f}\")\n","    print(f\"F1 Score : {f1:.4f}\")\n","    print(\"\\nClassification Report:\\n\")\n","    print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n","\n","    # Visualize Confusion Matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(6, 4))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n","    plt.title(\"Confusion Matrix\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.show()\n","\n","# Calling the evaluation function\n","if __name__ == \"__main__\":\n","    # Create test dataset and loader\n","    test_dataset = PhishingDataset(num_samples=50)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n","\n","    # Load models (assuming they are already loaded or available from previous context)\n","    tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers = load_models()\n","\n","    # Evaluate the model\n","    print(\"Evaluating model on test set...\")\n","    evaluate_model(fusion_model, efficientnet, tinybert, tokenizer, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"Hjo7j5xgL_F0","outputId":"0fdfd28b-ed94-4139-cee8-71f80357ac98"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating model on test set...\n","=== Evaluation Metrics ===\n","Overall Accuracy: 1.0000 (50/50 correct predictions)\n","Per-Class Accuracy:\n","- Phishing (Class 1): 1.0000 (31/31 samples)\n","- Legitimate (Class 0): 1.0000 (19/19 samples)\n","Precision: 1.0000\n","Recall: 1.0000\n","F1-Score: 1.0000\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","  Legitimate       1.00      1.00      1.00        19\n","    Phishing       1.00      1.00      1.00        31\n","\n","    accuracy                           1.00        50\n","   macro avg       1.00      1.00      1.00        50\n","weighted avg       1.00      1.00      1.00        50\n","\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 600x400 with 2 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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\n","text/plain":["<Figure size 600x400 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Real-Time Performance:\n","Requests Per Second (RPS): 20.6\n","Latency: 48.6 ms per input\n","\n","Computational Cost:\n","FLOPs: 0.00 GFLOPs per inference\n","\n","Robustness:\n","Image Quality (Gaussian noise, σ = 0.1): 1.0000 accuracy\n","Text Variability (Short snippets, <20 characters): 0.7200 accuracy\n","Occlusion Handling (50% occlusion): 1.0000 accuracy\n","\n","Component-Wise Performance:\n","TinyBERT (Text Analysis):\n","- Accuracy: 0.3800 (19/50)\n","- Precision: 0.0000\n","- Recall: 0.0000\n","- Latency: 18.6 ms\n","EfficientNet (Image Analysis):\n","- Accuracy: 0.3800 (19/50)\n","- Precision: 0.0000\n","- Recall: 0.0000\n","- Latency: 43.4 ms\n","Fusion Model:\n","- Accuracy: 1.0000 (50/50)\n","- Precision: 1.0000\n","- Recall: 1.0000\n","- Latency: 48.6 ms\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch.profiler import profile, record_function, ProfilerActivity\n","import time\n","from PIL import Image\n","\n","def evaluate_model(fusion_model, efficientnet, tinybert, tokenizer, test_loader, num_samples=50, folds=5):\n","    # Initialize lists for cross-validation\n","    all_labels = []\n","    all_preds = []\n","    all_probs = []\n","    tinybert_preds = []\n","    efficientnet_preds = []\n","\n","    # Define linear layers for component-wise predictions and move them to the correct device\n","    tinybert_linear = torch.nn.Linear(128, 2).to(DEVICE)\n","    efficientnet_linear = torch.nn.Linear(1280, 2).to(DEVICE)\n","\n","    # 5-fold cross-validation simulation\n","    for fold in range(folds):\n","        fusion_model.eval()\n","        fold_labels = []\n","        fold_preds = []\n","        fold_probs = []\n","        fold_tinybert_preds = []\n","        fold_efficientnet_preds = []\n","\n","        with torch.no_grad():\n","            for image_tensor, texts, labels in test_loader:\n","                # Fusion Model predictions\n","                image_features = extract_image_features(image_tensor, efficientnet)\n","                text_features = extract_text_features(texts, tokenizer, tinybert)\n","                outputs = fusion_model(image_features, text_features)\n","                probs = outputs[:, 1]  # Probability of Phishing (Class 1)\n","                _, predicted = torch.max(outputs, 1)\n","\n","                # TinyBERT-only predictions (text only)\n","                text_outputs = torch.softmax(tinybert_linear(text_features), dim=1)\n","                _, text_predicted = torch.max(text_outputs, 1)\n","\n","                # EfficientNet-only predictions (image only)\n","                image_outputs = torch.softmax(efficientnet_linear(image_features), dim=1)\n","                _, image_predicted = torch.max(image_outputs, 1)\n","\n","                fold_labels.extend(labels.cpu().numpy())\n","                fold_preds.extend(predicted.cpu().numpy())\n","                fold_probs.extend(probs.cpu().numpy())\n","                fold_tinybert_preds.extend(text_predicted.cpu().numpy())\n","                fold_efficientnet_preds.extend(image_predicted.cpu().numpy())\n","\n","        all_labels.extend(fold_labels)\n","        all_preds.extend(fold_preds)\n","        all_probs.extend(fold_probs)\n","        tinybert_preds.extend(fold_tinybert_preds)\n","        efficientnet_preds.extend(fold_efficientnet_preds)\n","\n","    # Truncate to match the test set size\n","    all_labels = all_labels[:num_samples]\n","    all_preds = all_preds[:num_samples]\n","    all_probs = all_probs[:num_samples]\n","    tinybert_preds = tinybert_preds[:num_samples]\n","    efficientnet_preds = efficientnet_preds[:num_samples]\n","\n","    # Compute overall and per-class accuracy\n","    acc = accuracy_score(all_labels, all_preds)\n","    phishing_acc = accuracy_score([label for label, pred in zip(all_labels, all_preds) if label == 1],\n","                                 [pred for label, pred in zip(all_labels, all_preds) if label == 1])\n","    legitimate_acc = accuracy_score([label for label, pred in zip(all_labels, all_preds) if label == 0],\n","                                   [pred for label, pred in zip(all_labels, all_preds) if label == 0])\n","    phishing_support = sum(1 for label in all_labels if label == 1)\n","    legitimate_support = sum(1 for label in all_labels if label == 0)\n","\n","    # Compute precision, recall, F1-score\n","    precision = precision_score(all_labels, all_preds, average='binary', pos_label=1)\n","    recall = recall_score(all_labels, all_preds, average='binary', pos_label=1)\n","    f1 = f1_score(all_labels, all_preds, average='binary', pos_label=1)\n","\n","    # Print metrics\n","    print(\"=== Evaluation Metrics ===\")\n","    print(f\"Overall Accuracy: {acc:.4f} ({int(acc * num_samples)}/{num_samples} correct predictions)\")\n","    print(f\"Per-Class Accuracy:\")\n","    print(f\"- Phishing (Class 1): {phishing_acc:.4f} ({int(phishing_acc * phishing_support)}/{phishing_support} samples)\")\n","    print(f\"- Legitimate (Class 0): {legitimate_acc:.4f} ({int(legitimate_acc * legitimate_support)}/{legitimate_support} samples)\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1-Score: {f1:.4f}\")\n","    print(\"\\nClassification Report:\\n\")\n","    print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(6, 4))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n","    plt.title(\"Confusion Matrix\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.show()\n","\n","    # ROC Curve and AUC-ROC\n","    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n","    roc_auc = auc(fpr, tpr)\n","    plt.figure(figsize=(6, 4))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","    # Real-Time Performance\n","    start_time = time.time()\n","    for _ in range(1000):  # Simulate 1,000 inputs\n","        with torch.no_grad():\n","            image_features = extract_image_features(next(iter(test_loader))[0][:1], efficientnet)\n","            text_features = extract_text_features(next(iter(test_loader))[1][:1], tokenizer, tinybert)\n","            fusion_model(image_features, text_features)\n","    end_time = time.time()\n","    latency = (end_time - start_time) / 1000 * 1000  # ms per input\n","    rps = 1000 / (end_time - start_time)  # Requests per second\n","    print(f\"\\nReal-Time Performance:\")\n","    print(f\"Requests Per Second (RPS): {rps:.1f}\")\n","    print(f\"Latency: {latency:.1f} ms per input\")\n","\n","    # Computational Cost\n","    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n","        with record_function(\"model_inference\"):\n","            image_features = extract_image_features(next(iter(test_loader))[0][:1], efficientnet)\n","            text_features = extract_text_features(next(iter(test_loader))[1][:1], tokenizer, tinybert)\n","            fusion_model(image_features, text_features)\n","    flops = prof.key_averages().total_average().self_cpu_time_total / 1e9  # Convert to GFLOPs (approximation)\n","    print(f\"\\nComputational Cost:\")\n","    print(f\"FLOPs: {flops:.2f} GFLOPs per inference\")\n","\n","    # Robustness: Image Quality (Add Gaussian noise)\n","    noisy_labels = []\n","    noisy_preds = []\n","    with torch.no_grad():\n","        for image_tensor, texts, labels in test_loader:\n","            # Add Gaussian noise to images\n","            noisy_images = image_tensor + torch.randn_like(image_tensor) * 0.1  # σ = 0.1\n","            noisy_images = torch.clamp(noisy_images, 0, 1)\n","            image_features = extract_image_features(noisy_images, efficientnet)\n","            text_features = extract_text_features(texts, tokenizer, tinybert)\n","            outputs = fusion_model(image_features, text_features)\n","            _, predicted = torch.max(outputs, 1)\n","            noisy_labels.extend(labels.cpu().numpy())\n","            noisy_preds.extend(predicted.cpu().numpy())\n","    noisy_acc = accuracy_score(noisy_labels, noisy_preds)\n","    print(f\"\\nRobustness:\")\n","    print(f\"Image Quality (Gaussian noise, σ = 0.1): {noisy_acc:.4f} accuracy\")\n","\n","    # Robustness: Text Variability (Simulate by shortening text)\n","    short_text_labels = []\n","    short_text_preds = []\n","    with torch.no_grad():\n","        for image_tensor, texts, labels in test_loader:\n","            # Shorten texts to simulate variability\n","            short_texts = [text[:20] if len(text) > 20 else text for text in texts]\n","            image_features = extract_image_features(image_tensor, efficientnet)\n","            text_features = extract_text_features(short_texts, tokenizer, tinybert)\n","            outputs = fusion_model(image_features, text_features)\n","            _, predicted = torch.max(outputs, 1)\n","            short_text_labels.extend(labels.cpu().numpy())\n","            short_text_preds.extend(predicted.cpu().numpy())\n","    short_text_acc = accuracy_score(short_text_labels, short_text_preds)\n","    print(f\"Text Variability (Short snippets, <20 characters): {short_text_acc:.4f} accuracy\")\n","\n","    # Robustness: Occlusion (Simulate by masking part of the image)\n","    occluded_labels = []\n","    occluded_preds = []\n","    with torch.no_grad():\n","        for image_tensor, texts, labels in test_loader:\n","            # Occlude 50% of the image (bottom half)\n","            occluded_images = image_tensor.clone()\n","            occluded_images[:, :, 112:, :] = 0  # Mask bottom half (224x224 image)\n","            image_features = extract_image_features(occluded_images, efficientnet)\n","            text_features = extract_text_features(texts, tokenizer, tinybert)\n","            outputs = fusion_model(image_features, text_features)\n","            _, predicted = torch.max(outputs, 1)\n","            occluded_labels.extend(labels.cpu().numpy())\n","            occluded_preds.extend(predicted.cpu().numpy())\n","    occluded_acc = accuracy_score(occluded_labels, occluded_preds)\n","    print(f\"Occlusion Handling (50% occlusion): {occluded_acc:.4f} accuracy\")\n","\n","    # Component-Wise Performance\n","    tinybert_acc = accuracy_score(all_labels, tinybert_preds)\n","    tinybert_precision = precision_score(all_labels, tinybert_preds, average='binary', pos_label=1)\n","    tinybert_recall = recall_score(all_labels, tinybert_preds, average='binary', pos_label=1)\n","    efficientnet_acc = accuracy_score(all_labels, efficientnet_preds)\n","    efficientnet_precision = precision_score(all_labels, efficientnet_preds, average='binary', pos_label=1)\n","    efficientnet_recall = recall_score(all_labels, efficientnet_preds, average='binary', pos_label=1)\n","\n","    print(\"\\nComponent-Wise Performance:\")\n","    print(\"TinyBERT (Text Analysis):\")\n","    print(f\"- Accuracy: {tinybert_acc:.4f} ({int(tinybert_acc * num_samples)}/{num_samples})\")\n","    print(f\"- Precision: {tinybert_precision:.4f}\")\n","    print(f\"- Recall: {tinybert_recall:.4f}\")\n","    start_time = time.time()\n","    extract_text_features(next(iter(test_loader))[1][:1], tokenizer, tinybert)\n","    tinybert_latency = (time.time() - start_time) * 1000\n","    print(f\"- Latency: {tinybert_latency:.1f} ms\")\n","\n","    print(\"EfficientNet (Image Analysis):\")\n","    print(f\"- Accuracy: {efficientnet_acc:.4f} ({int(efficientnet_acc * num_samples)}/{num_samples})\")\n","    print(f\"- Precision: {efficientnet_precision:.4f}\")\n","    print(f\"- Recall: {efficientnet_recall:.4f}\")\n","    start_time = time.time()\n","    extract_image_features(next(iter(test_loader))[0][:1], efficientnet)\n","    efficientnet_latency = (time.time() - start_time) * 1000\n","    print(f\"- Latency: {efficientnet_latency:.1f} ms\")\n","\n","    print(\"Fusion Model:\")\n","    print(f\"- Accuracy: {acc:.4f} ({int(acc * num_samples)}/{num_samples})\")\n","    print(f\"- Precision: {precision:.4f}\")\n","    print(f\"- Recall: {recall:.4f}\")\n","    print(f\"- Latency: {latency:.1f} ms\")\n","\n","# Calling the evaluation function\n","if __name__ == \"__main__\":\n","    # Create test dataset and loader\n","    test_dataset = PhishingDataset(num_samples=50)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n","\n","    # Load models (assuming they are already loaded or available from previous context)\n","    tokenizer, tinybert, efficientnet, fusion_model, reader, yolo_model, output_layers = load_models()\n","\n","    # Evaluate the model\n","    print(\"Evaluating model on test set...\")\n","    evaluate_model(fusion_model, efficientnet, tinybert, tokenizer, test_loader, num_samples=50, folds=5)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1JFvEfHZG_AlD_Jsh-8Yve3P7mqrmaDxO","authorship_tag":"ABX9TyOWRJRvyTVG5a9tVAZl71n4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9b840de26f5e464fa69b597255b86e59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f58549b29cc44dea100c4294026bd94","IPY_MODEL_29c2a1a01bce4f408c9c0c88eb764e4d","IPY_MODEL_99683a517e9a41418b25b90819c25dbc"],"layout":"IPY_MODEL_ec6aab008bdb4bd09000d08559a78032"}},"7f58549b29cc44dea100c4294026bd94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_691e60d3dafa4dff9ba71daa79134cb1","placeholder":"​","style":"IPY_MODEL_4b7284d37ed24bc6bc23fc214ba41cda","value":"vocab.txt: 100%"}},"29c2a1a01bce4f408c9c0c88eb764e4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ff044207a7c4beebf5af0f77332b631","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20090a040e7a4c96ae9d87141c45a894","value":231508}},"99683a517e9a41418b25b90819c25dbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b79e9ad2051f4b339525c19b7238d459","placeholder":"​","style":"IPY_MODEL_1baaf89a71164d4b96ce8b89dfeabb46","value":" 232k/232k [00:00&lt;00:00, 5.99MB/s]"}},"ec6aab008bdb4bd09000d08559a78032":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"691e60d3dafa4dff9ba71daa79134cb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b7284d37ed24bc6bc23fc214ba41cda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ff044207a7c4beebf5af0f77332b631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20090a040e7a4c96ae9d87141c45a894":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b79e9ad2051f4b339525c19b7238d459":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1baaf89a71164d4b96ce8b89dfeabb46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"546a7dfbc7e64cfda02e858fb0865e4a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0517e07ef51f48e4a9ac0184aab2dafc","IPY_MODEL_69056b79986e42feb507c94c8a55f5b4","IPY_MODEL_57639ff084a6410fa1790832dbc1401b"],"layout":"IPY_MODEL_20a5fe78f14c4f3cab11aa8cee454243"}},"0517e07ef51f48e4a9ac0184aab2dafc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d18764f74194161a499ddec686172ee","placeholder":"​","style":"IPY_MODEL_1017e21334104e06aa40a9566aea54f3","value":"config.json: 100%"}},"69056b79986e42feb507c94c8a55f5b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a72f4caf842403598d4f44e5dcd7f50","max":285,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e04e8d1fc62c43ddb618e373e15f35c4","value":285}},"57639ff084a6410fa1790832dbc1401b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_596caa65e0c54f68bdd59baccb4ad569","placeholder":"​","style":"IPY_MODEL_9cd5c0038eb64e1db4686c09c62392b1","value":" 285/285 [00:00&lt;00:00, 6.11kB/s]"}},"20a5fe78f14c4f3cab11aa8cee454243":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d18764f74194161a499ddec686172ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1017e21334104e06aa40a9566aea54f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a72f4caf842403598d4f44e5dcd7f50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e04e8d1fc62c43ddb618e373e15f35c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"596caa65e0c54f68bdd59baccb4ad569":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cd5c0038eb64e1db4686c09c62392b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e2800c21bc643f5b0ce463b588f5070":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6e85c3af69e47c88b31acd0caf335f4","IPY_MODEL_09e847be8b584d3e86ccaa6ede0d4d98","IPY_MODEL_ffc37ee799bd4960910fbad27b9efe70"],"layout":"IPY_MODEL_22b6871d64974f4a82b7e33150fbb8c8"}},"e6e85c3af69e47c88b31acd0caf335f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fc09e6b16364524a4c677b5d447c952","placeholder":"​","style":"IPY_MODEL_36ef8b2d12d949b78f78b7347feba687","value":"pytorch_model.bin: 100%"}},"09e847be8b584d3e86ccaa6ede0d4d98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a93eaf869bf34bccbedaefecbbc3eeb0","max":17756393,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0168e04412934ec48ea8b09585595217","value":17756393}},"ffc37ee799bd4960910fbad27b9efe70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb62124bf77748c2b83f12a072297eb4","placeholder":"​","style":"IPY_MODEL_35c20836861d43d18398c36d19d90395","value":" 17.8M/17.8M [00:00&lt;00:00, 62.8MB/s]"}},"22b6871d64974f4a82b7e33150fbb8c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fc09e6b16364524a4c677b5d447c952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36ef8b2d12d949b78f78b7347feba687":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a93eaf869bf34bccbedaefecbbc3eeb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0168e04412934ec48ea8b09585595217":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb62124bf77748c2b83f12a072297eb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35c20836861d43d18398c36d19d90395":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"233529b35e074309b20609573db52d52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c694e93e317f46579073554bccc68731","IPY_MODEL_727866fd570b45149dc55ac49ca24b1a","IPY_MODEL_104c8a3075934673b9bdb896ea08106c"],"layout":"IPY_MODEL_8ae860562e59485bac16778360bbba43"}},"c694e93e317f46579073554bccc68731":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35a3d0c27af94a07a3b0d5602e0a5e0f","placeholder":"​","style":"IPY_MODEL_e3b0f48d374145ddadb3d10eff0650bf","value":"model.safetensors: 100%"}},"727866fd570b45149dc55ac49ca24b1a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b8de581ad7346e89a7a49897e051971","max":17743328,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6fe309560d74cdca49abd5e4d37b7bb","value":17743328}},"104c8a3075934673b9bdb896ea08106c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d176ab33f1c648929f40162a32b053c1","placeholder":"​","style":"IPY_MODEL_d447543f1120445ebf67904c52ad5858","value":" 17.7M/17.7M [00:00&lt;00:00, 95.9MB/s]"}},"8ae860562e59485bac16778360bbba43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35a3d0c27af94a07a3b0d5602e0a5e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3b0f48d374145ddadb3d10eff0650bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b8de581ad7346e89a7a49897e051971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6fe309560d74cdca49abd5e4d37b7bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d176ab33f1c648929f40162a32b053c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d447543f1120445ebf67904c52ad5858":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}