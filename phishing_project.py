# -*- coding: utf-8 -*-
"""Phishing project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1czvWdl6QrDiYAxJd920zjYMEU__33kqV
"""

pip install transformers torch torchvision pandas scikit-learn matplotlib tqdm opencv-python flask

!pip install tldextract

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
import pandas as pd
import re
from sklearn.preprocessing import LabelEncoder

# ✅ Load datasets
file_paths = {
    "dataset_phishing": "/content/drive/MyDrive/MINI-1/URL/dataset_phishing.csv",
    "Phishing_Legitimate_full": "/content/drive/MyDrive/MINI-1/URL/Phishing_Legitimate_full.csv",
    "PhiUSIIL_Phishing_URL_Dataset": "/content/drive/MyDrive/MINI-1/URL/PhiUSIIL_Phishing_URL_Dataset.csv"
}

dfs = {name: pd.read_csv(path) for name, path in file_paths.items()}

# ✅ Reset index for each DataFrame to ensure continuous indexing
for name, df in dfs.items():
    df.reset_index(drop=True, inplace=True)

# ✅ Clean URLs and encode labels
def clean_text(text):
    return re.sub(r"https?://|www\.", "", text)  # Remove protocol & www

for name, df in dfs.items():
    text_col = "url" if "url" in df.columns else "URL" if "URL" in df.columns else "text"
    if text_col in df.columns:
        df["cleaned_text"] = df[text_col].astype(str).apply(clean_text)

    label_col = "status" if "status" in df.columns else "CLASS_LABEL" if "CLASS_LABEL" in df.columns else "label"
    if label_col in df.columns:
        df["label"] = LabelEncoder().fit_transform(df[label_col])

# ✅ Tokenization with TinyBERT
tokenizer = BertTokenizer.from_pretrained("prajjwal1/bert-tiny")
dfs["dataset_phishing"]["tokenized"] = dfs["dataset_phishing"]["cleaned_text"].apply(
    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128, truncation=True)
)

# ✅ Validate dataset size before splitting
print(f"Dataset size: {len(dfs['dataset_phishing'])}")

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    dfs["dataset_phishing"]["tokenized"].tolist(), dfs["dataset_phishing"]["label"], test_size=0.2, random_state=42
)

print(f"Training size: {len(X_train)}")
print(f"Test size: {len(X_test)}")

# ✅ Define a PyTorch Dataset for loading data
class PhishingTextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            str(self.texts[idx]),  # Ensure text is a string
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        # Access labels using .iloc to ensure proper indexing after train-test split
        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        return input_ids, attention_mask, label

# ✅ Prepare datasets
train_dataset = PhishingTextDataset(X_train, y_train, tokenizer)
test_dataset = PhishingTextDataset(X_test, y_test, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# ✅ Load TinyBERT Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tinybert_model = BertForSequenceClassification.from_pretrained("prajjwal1/bert-tiny", num_labels=2).to(device)

# ✅ Define optimizer & loss function
optimizer = optim.AdamW(tinybert_model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

# ✅ Training Loop
epochs = 5
tinybert_model.train()

# ✅ Check data loading process
for batch in train_loader:
    print(batch)  # Check if the batch has valid data
    break  # Just check for the first batch

# ✅ Training the model
for epoch in range(epochs):
    total_loss = 0
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        optimizer.zero_grad()
        outputs = tinybert_model(input_ids, attention_mask=attention_mask)

        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}")

print("✅ TinyBERT Training Complete!")

# Save the trained model
torch.save(tinybert_model.state_dict(), "tinybert_model.pth")

# Load the trained model
tinybert_model.load_state_dict(torch.load("tinybert_model.pth"))

# Set the model to evaluation mode
tinybert_model.eval()

correct = 0
total = 0

# Disable gradient computation for inference (to speed up and reduce memory usage)
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        # Forward pass
        outputs = tinybert_model(input_ids, attention_mask=attention_mask)

        # Get predictions by selecting the class with the highest score
        predictions = torch.argmax(outputs.logits, dim=1)

        # Compare predictions with true labels
        correct += (predictions == labels).sum().item()
        total += labels.size(0)

# Calculate accuracy
accuracy = correct / total
print(f"✅ TinyBERT Test Accuracy: {accuracy * 100:.2f}%")

from sklearn.metrics import classification_report

# Model evaluation
tinybert_model.eval()
predictions = []
true_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = tinybert_model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

# Generate the classification report
report = classification_report(true_labels, predictions, target_names=["Legitimate", "Phishing"])
print(report)

def predict_phishing(text, threshold=0.2):
    # Tokenize the input text
    encoding = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )

    # Move input to the correct device
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    # Perform inference
    with torch.no_grad():
        output = tinybert_model(input_ids, attention_mask=attention_mask)

        # Get logits and apply softmax to get probabilities
        logits = output.logits
        probabilities = torch.nn.functional.softmax(logits, dim=1)

        # Print out probabilities for debugging
        print(f"Probabilities: {probabilities}")

        # Custom threshold to decide on phishing prediction
        phishing_prob = probabilities[0][1].item()

        # Adjust decision threshold for phishing predictions
        if phishing_prob >= threshold:
            return "Phishing"
        else:
            return "Legitimate"

# Save the trained model
torch.save(tinybert_model.state_dict(), "tinybert_model.pth")

# Load the trained model
tinybert_model.load_state_dict(torch.load("tinybert_model.pth"))

# Test inference with a lower threshold
sample_text = "Subject: Urgent: Confirm Your Software Account Access Message:Hi Mr. Raju,We have detected unusual activity in your software development account, and for your security, we require you to confirm your identity to prevent any unauthorized access.Please click the link below to log in and verify your account:www.fake-website.com .Failure to verify your identity within 24 hours may result in a temporary suspension of your account.Best regards,The Deloitte Support Team"
result = predict_phishing(sample_text, threshold=0.1)
print(f"Prediction: {result}")

"""**TINYBERT MODEL COMPLETED**"""

print(len(dfs["dataset_phishing"]))
print(len(dfs["Phishing_Legitimate_full"]))
print(len(dfs["PhiUSIIL_Phishing_URL_Dataset"]))

for name, df in dfs.items():
    print(name, df["label"].value_counts())